--- 
title: "ImpactTB/BAA: Standard Operating Procedures for Data Analysis"
author: "Colorado State University Coding Team"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "A comprehensive guide to wet lab data collection, sample processing, and computational tool creation for robust and efficient data analysis and dissemination."
---

# Overview

Here, we have built a comprehensive guide to wet lab data collection, sample processing, and computational tool creation for robust and efficient data analysis and dissemination.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Introduction

## About the project: Immune Mechanisms of Protection against Mycobacterium tuberculosis (IMPAc-TB)

The objective of the IMPAc-TB program is to get a thorough understanding of the immune responses necessary to avoid initial infection with *Mycobacterium tuberculosis (Mtb)*, formation of latent infection, and progression to active TB illness. To achieve these goals, the National Institute of Allergy and Infectious Diseases awarded substantial funding and established multidisciplinary research teams that will analyze immune responses against *Mtb* in animal models (mice, guinea pigs, and non-human primates) and humans, as well as immune responses elicited by promising vaccine candidates. The contract awards establish and give up to seven years of assistance for IMPAc-TB Centers to explain the immune responses required for *Mtb* infection protection. 

The seven centers that are part of the study are (in alphabetical order):

1. Colorado State University
2. Harvard T.H. Chan School of Public Health
3. Seattle Children Hospital



Colorado State University Team and role of each member:
Dr. Marcela Henao-Tamayo: Principal Investigator
Dr. Brendan Podell: Principal Investigator
Dr. Andres Obregon-Henao: Research Scientist-III
Dr. Taru S. Dutt: Research Scientist-I










<!--chapter:end:01-intro.Rmd-->

# Initial mouse characteristics

Here is a review of existing methods.

<!--chapter:end:02-initial_mouse_characteristics.Rmd-->

# Mouse Weights

### Overview
Extreme weight loss and loss of muscle mass, also known as cachexia, typically presents along side chronic inflammatory illnesses like Tuberculosis disease @ [baazim2022interplay]. We now recognize that cachexia is part of a systemic response to inflammation, and has been linked to upregulation of pro-inflammatory cytokines such as TNF, IL-6, and IFNg in humans @ [baazim2022interplay]. Additionally, studies support the role of key immune cell populations such as CD8+ T-cells that, when depleted, counteract muscle and fat deterioration  @[baazim2019cd8],  and suggest that CD8+ T-cells may metabolically reprogram adipose tissue.

In recognition of cachexia related illnesses and diseases, we tracked the progression of weight loss over the course of this study, as is done with many TB-mouse studies @ [smith2022host], @ [segueni2016controlled]. These data is also useful when correlating to CFU count as well as expression of cytokines and other biological markers @ [smith2022host]. Here, mice are weighed in grams weekly to monitor clinical status as TB patients frequently display weight loss as clinical symptom associated with disease progression. 

The following contains information about how the data was collected, organized, and curated for analysis in RStudio.

### Parameters
Weights are recorded in an excel worksheet. 

Column titles are as follows: who_collected	date_collected	sex	dob	notch_id	mouse_number	weight	unit	cage_number	group	notes

Groups included are: bcg, saline, bcg+id93, saline+id93, saline+noMtb

The notes column contains information regarding clinical observations. 

good reference: https://elifesciences.org/articles/74419#s4

```{r}
library(readxl)
library(tidyverse)
```

### Read in data

Data is stored in one excel sheet, each week is one sheet named as the date -> return vector for each sheet name 

```{r include=FALSE}
weight_data <- read_xlsx("DATA/body_weights_measurement.xlsx")
excel_sheets("DATA/body_weights_measurement.xlsx") #lists names of each sheet

multiplesheets <- function(fname) {
   
  # getting info about all excel sheets
  sheets <- readxl::excel_sheets(fname)
  tibble <- lapply(sheets, function(x) readxl::read_excel(fname, sheet = x))
  data_frame <- lapply(tibble, as.data.frame)
    
  # assigning names to data frames
  names(data_frame) <- sheets
    
  # print data frame
  print(data_frame)
}
  
# specifying the path name
path <- "DATA/body_weights_measurement.xlsx"
multiplesheets(path)
```

### Can also use rio to read in the data, more streamlined
```{r include=FALSE}
library(rio)
# specifying the path name
path <- "DATA/body_weights_measurement.xlsx"
  
# reading data from all sheets
data <- import_list(path)
  
# print data
print (data)
```


### Clean data
```{r}
dataset <- data$before_vaccination %>%
 select("sex", "mouse_number", "weight", "cage_number", "group")

```


```{r}

# combining columns mouse_number and cage_number

dataset$mouse_id <- paste(dataset$mouse_number, "-", dataset$cage_number)

```


### Body weight over time graph and statistics
```{r}

```

### Weight loss over time graph and statistics 
```{r}

```

### Weight vs CFU
```{r}

```

### Weight vs ELISA results 
```{r}

```

### Weight vs lesion burden
```{r}

```


<!--chapter:end:03-mouse_weights.Rmd-->

# Colony forming units to determine bacterial counts 

## Data description

The data are collected in a spreadsheet with multiple sheets. The first sheet
(named "[x]") is used to record some metadata for the experiment, while the 
following sheets are used to record CFUs counts from the plates used for samples
from each organ, with one sheet per organ. For example, if you plated data
from both the lung and spleen, there would be three sheets in the file: one 
with the metadata, one with the plate counts for the lung, and one with the
plate counts for the spleen. 

The metadata sheet is used to record information about the overall process of
plating the data. Values from this sheet will be used in calculating the bacterial
load in the original sample based on the CFU counts. This spreadsheet includes
the following columns: 

- `organ`: Include one row for each organ that was plated in the experiment. 
You should name the organ all in lowercase (e.g., "lung", "spleen"). You 
should use the same name to also name the sheet that records data for that organ
for example, if you have rows in the metadata sheet for "lung" and "spleen", 
then you should have two other sheets in the file, one sheet named "lung" and 
one named "spleen", which you'll use to store the plate counts for each of those
organs.
- `prop_resuspended`: In this column, give the proportion of that organ that 
was plated. For example, if you plated half the lung, then in the "lung" row
of this spread sheet, you should put 0.5 in the `prop_resuspended` column. 
- `total_resuspended_uL`: This column contains an original volume of tissue homogenate. For example, raw lung tissue is homogenized in 500 uL of PBS in a tube containing metal beads. 
- `og_aliquot_uL`: 100 uL of th total_resuspended slurry would be considered an original aliquot and is used to peform serial dilutions.
- `dilution_factor`: Amount of the original stock solution that is present in the total solution, after dilution(s)
- `plated_uL`: Amount of suspension + diluent plated on section of solid agar 

## Read in data

```{r message = FALSE, warning = FALSE}
library(readxl)
library(dplyr)
library(purrr)
library(tidyr)
library(stringr)

#Replace w/ path to CFU sheet
path <- c("DATA/Copy of baa_cfu_sheet.xlsx")

sheet_names <- excel_sheets(path)
sheet_names <- sheet_names[!sheet_names %in% c("metadata")]

merged_data <- list()

for(i in 1:length(sheet_names)){
  
  data <- read_excel(path, sheet = sheet_names[i]) %>% 
    mutate(organ = paste0(sheet_names[i]))
  
  data <- data %>% 
    #mutate(missing_col = NA) %>% 
    mutate_if(is.double, as.numeric) %>% 
    mutate_if(is.numeric, as.character) %>% 
    pivot_longer(starts_with("dil_"), names_to = "dilution",
                 values_to = "CFUs") %>% 
    mutate(dilution = str_extract(dilution, "[0-9]+"),
           dilution = as.numeric(dilution))
    
  
  merged_data[[i]] <- data
  
  
}
  
all_data <- bind_rows(merged_data, .id = "column_label") %>% 
    select(-column_label)
  
```

## Exploratory analysis and quality checks

## Exploratory analysis

**Dimensions of input data:**

Based on the input data, data were collected for the following organ or 
organs: 

The following number of mice were included for each: 

The following number of replicates were recorded at each count date for 
each experimental group: 

The following number of dilutions and dilution level were recorded for 
each organ: 

**People who plated and collected the data. Date or dates of counting:**

Based on the input data, the plates included in these data were counted by 
the following person or persons: 
Based on the input data, the plates included in these data were counted on 
the following date or dates: 

```{r}
all_data %>%
  select(organ, who_plated, who_counted, count_date) %>%
  distinct()
```

**Distribution of CFUs at each dilution:**

WE NEED TO ADD SAMPLE CFU PLOTS

Here's a plot that shows how many plates were too numerous to count at each 
dilution level: 

Here is a plot that shows how the CFU counts were distributed by dilution
level in the data: 

## Identify a good dilution for each sample
```{r}
# Make all_data into tidy data and filter for CFUs between 10-75
  
tidy_cfu_data <- all_data %>%
  mutate(dilution = str_extract(dilution, "[0-9]+"),
         dilution = as.numeric(dilution)) %>%
  filter(CFUs >= 10 & CFUs <= 75) %>%
  mutate(CFUs = as.numeric(CFUs))
```

## Calculate CFUs from best dilution/Estimate bacterial load for each sample based on good dilution
```{r}
# Calculating CFU/ml for every qualifying replicate between 10-75 CFUs. Column binding by organ name to the metadata sheet via inner_join().
meta <- read_excel(path, sheet = "metadata")

tidy_cfu_meta_joined <- inner_join(tidy_cfu_data, meta) %>%
  group_by(groups) %>% 
  mutate(CFUs_per_ml = (CFUs * (dilution_factor^2) * (total_resuspension_mL/volume_plated_ul) * 10)) %>%
  select(organ, count_date, who_plated, who_counted, groups,  mouse, dilution,  CFUs, CFUs_per_ml) %>%
  ungroup()

tidy_cfu_meta_joined
```

## Create initial report information for these data
```{r}

```


## Sample ANOVA
```{r}
cfu_stats <- tidy_cfu_meta_joined %>% 
  group_by(organ) %>%
  nest() %>%
  mutate(aov_result = map(data, ~aov(CFUs_per_ml ~ groups, data = .x)),
         tukey_result = map(aov_result, TukeyHSD),
         tidy_tukey = map(tukey_result, broom::tidy)) %>%
  unnest(tidy_tukey, .drop = TRUE) %>%
  separate(contrast, into = c("contrast1", "contrast2"), sep = "-") %>%
  select(-data, -aov_result, -tukey_result, -term, -null.value)# %>%
  # filter(adj.p.value <= 0.05)

cfu_stats
```

## Save processed data to database
## Example one

## Example two





<!--chapter:end:04-cfus.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Enzyme-linked immunosorbest assay (ELISA)

ELISA is a standard molecular biology assay for detecting and quantifying a variety of compounds, including peptides, proteins, and antibodies in a sample. The sample could be serum, plasma, or bronchoalveolar lavage fluid (BALF).

#### **Importance of ELISA**

An antigen-specific reaction in the host results in the production of antibodies, which are proteins found in the blood. In the event of an infectious disease, it aids in the detection of antibodies in the body.ELISA is distinguishable from other antibody-assays in that it produces quantifiable findings and separates non-specific from specific interactions by serial binding to solid surfaces, which is often a polystyrene multiwell plate.

In IMPAc-TB project, it is crucial to evaluate the if the vaccine is eliciting humoral immunity and generating antibodies against vaccine antigen. ELISA will be used to determine the presence of Immunoglobulin (Ig) IgG, IgA, and IgM in the serum different time points post-vaccination. 

#### **Principle of ELISA**

ELISA is based on the principle of antigen-antibody interaction. An antigen must be immobilized on a solid surface and then complexed with an enzyme-linked antibody in an ELISA. The conjugated enzyme's activity is evaluated by incubating it with a substrate to yield a quantifiable result, which enables detection. There are four basic steps of ELISA:

**1. Coating multiwell plate with antigen/antibody**: This step depends on what we want to detect the sample. If we need to evaluate the the presence of antibody, the plate will be coated with the antigen, and vice versa. To coat the plate, a fixed concentration of antigen (protein) is added to a 96 well high-binding plate (charged plate). Plate is incubated over night with the antigen at 4 degree celsius (as proteins are temperature sensitive) so that antigens are completely bound to the well. 

**2. Blocking**: It is possible that not each and every site of the well is coated with the targeted antigen, and there could be uncovered areas. It is important to block those empty spaces so that primary antibody (which we will add to the next step) binds to these spaces and give us false positive results. For this, microplate well surface-binding sites are blocked with an unrelated protein or other substance.Most common blocking agents are bovine serum albumin, skim milk, and casein. One of the best blocking agents is to use the serum from the organism in which your secondary (detection antibody) is raised. For example, if the secondary antibody is raised in goat, then we can use goat serum as a blocking agent. 

**3. Probing**: Probing is the step where we add sample containing antibodies that we want to detect. This will be the primary antibody. If the antibodies against the antigen (which we have coated) are present in the sample, it will bind to the antigen with high affinity. 

**4. Washing**: After the incubation of sample containing primary antibody, the wells are washed so that any unbound antibody is washed away. Washing solution contains phosphate buffer saline + 0.05% tween-20 (a mild detergent). 0.05% tween-20 washes away all the non-specific interactions as those are not strong, but keeps all the specific interaction as those are strong and cannot be detached with mild detergent. 

**5. Detection**: To detect the presence of antibody-antigen complex, a secondary antibody labelled with an enzyme (usually horseradish peroxidase) is added to the wells, incubated and washed. 

**6. Signal Measurement**: Finally to detect "if" and "how much" of the antibody is present, a chromogenic substrate (like 3,3',5,5'-Tetramethylbenzidine) is added to the wells, which can be cleaved the the enzyme that is tagged to the secondary antibody. The color compund is formed after the addition of the substrate, which is directly proportional to the amount of antibody present in the sample. The plate is read on a plate reader, where color is converted to numbers. 

```{r pressure, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("DATA/elisa.png")
```

## Read in data from excel file
```{r}
library(readxl)
library(tidyverse)
library(minpack.lm)
library(broom)
library(purrr)
```

```{r}
elisa_raw_data <- read_excel("DATA/elisa_s1_07-25-20.xlsx", sheet = "S1", col_names = FALSE,  range = "B2:M9")


head(elisa_raw_data)
```

### Tidy the data

```{r}

# Convert all columns to numeric

elisa_raw_data_numeric <- elisa_raw_data %>% 
  mutate_if(is.character, as.numeric)

# pivot longer the data

elisa_raw_data_tidy <- pivot_longer(data = elisa_raw_data_numeric, cols = "...1":"...12", names_to = "well_id", values_to = "od_450nm")

# remove "..." from the first column

elisa_raw_data_tidy$well_id <- str_replace(elisa_raw_data_tidy$well_id, "...", "")

# Add new column to the data_frame

elisa_raw_data_tidy_new <- elisa_raw_data_tidy %>%
  mutate(name = rep(LETTERS[1:8], each = 12))

elisa_raw_data_tidy_new <- elisa_raw_data_tidy_new %>%
  mutate(well_id = paste0(name, well_id)) %>%
  select(-name)

head(elisa_raw_data_tidy_new)

```
### Read in second data set

```{r}
elisa_label_data <- read_excel("DATA/elisa_s1_07-25-20.xlsx", sheet = "S1", col_names = FALSE,  range = "Q2:AB9")

head(elisa_label_data)
```

```{r}
# pivot longer the data

elisa_label_data_tidy <- pivot_longer(data = elisa_label_data, cols = "...1":"...12", names_to = "well_id", values_to = "information")

# remove "..." from the first column

elisa_label_data_tidy$well_id <- str_replace(elisa_label_data_tidy$well_id, "...", "")

# Add new column to the data_frame

elisa_label_data_tidy_new <- elisa_label_data_tidy %>%
  mutate(name = rep(LETTERS[1:8], each = 12))

elisa_label_data_tidy_new <- elisa_label_data_tidy_new %>%
  mutate(well_id = paste0(name, well_id)) %>%
  select(-name)

head(elisa_label_data_tidy_new)
```

## #Join the OD table with the information table

```{r}

elisa_data = elisa_raw_data_tidy_new %>% inner_join(elisa_label_data_tidy_new, by="well_id")

head(elisa_data)
```

### Separate the information table into sample ID and dilution columns

```{r}
tidy_elisa_data <- separate(elisa_data, col = "information", into = c("sample_id", "dilution"), sep = "\\(")

head(tidy_elisa_data)
```
```{r}
tidy_elisa_data <- tidy_elisa_data %>%
  mutate(dilution = str_extract(dilution, "(/)[0-9]+"),
         dilution = str_replace(dilution, "/", ""),
         dilution = as.numeric(dilution))

tidy_elisa_data <- tidy_elisa_data %>%
  select(well_id, sample_id, dilution, od_450nm)

head(tidy_elisa_data)
```

## ELISA data analysis optimization

ELISA data can be analyzed in different ways based on how the data is acquired. There are a a few examples of the type of ELISA data:

**1. With standard curve:** ELISA can be used to determine the concentrations of the antigen and antibody. This type of ELISA data usually have a standard curve with
different concentrations of the known analyte and the concentration in the sample is determined by extrapolating the unknown values in the curve. This type of assay is straightforward, easy to interpret and are more robust.

**2. Without standard curve:** Usually vaccine studies involve investigating the presence of high-affinity (and novel) antibodies against the vaccine antigens. 
Therefore, plotting a standard curve is not feasible as there is no previous information available for antibody concentration or type of antibody. Also, because antibody response to a vaccine will differ depending on the individual, 
it is not practical to generate a calibration curve from which absolute concentrations can be extrapolated.
For this type of ELISA, quantification of the antibody titers is performed using serial dilutions of the test samples, and analysis can be performed using the  following three methods:

1. Fitting sigmoid model
2. Endpoint titer method
3: Absorbance summation method

Let's have a look at these methods, how we can apply these methods in our data, and R-based packages that we can utilize to perform tis analysis.

### **1. Fitting sigmoid model:** 

In this model, we will perform a 8-10 point serial dilution of our sample and plot a 4 parameter sigmoidal curve. Example data:

```{r}
elisa_example_data <- read_excel("DATA/example_elisa_data.xlsx")

# separate 1/

elisa_example_data <- separate(elisa_example_data, col = "dilution", into = c("numerator", "denomenator"), sep = "\\/")

elisa_example_data <- elisa_example_data %>% 
  mutate_if(is.character, as.numeric)

elisa_example_data$dilution <- elisa_example_data$numerator / elisa_example_data$denomenator  

elisa_example_data <- elisa_example_data %>%
  mutate(log_dilution = log(dilution, base = 3))

head(elisa_example_data)

mod_1 <- nlsLM(absorbance ~ ((a-d)/(1+(log_dilution/c)^b)) + d,
data = elisa_example_data, 
start = list (a = 4, d= 0, c = -5, b = 1))

# a = maximum absorbance
# d = minimum absobance
# c = point of maximum growith (dilition where the curve is straight line)
# b = slope at c

mod_1

summary(mod_1)

```
#### Fitted model curve
```{r}
tidy_params <- mod_1 %>% tidy()

a <- tidy_params$estimate[tidy_params$term == "a"]
b <- tidy_params$estimate[tidy_params$term == "b"]
c <- tidy_params$estimate[tidy_params$term == "c"]
d <- tidy_params$estimate[tidy_params$term == "d"]

elisa_example_data <- elisa_example_data %>%
  mutate(fitted = predict(mod_1))

elisa_example_data <- elisa_example_data %>%
  mutate(fitted = predict(mod_1))

elisa_example_data %>%
  ggplot(aes(x = log_dilution, y = absorbance)) +
  geom_point() +
  geom_line(aes(y=fitted), color = "blue")
```

### 2. Endpoint titer method

The endpoint titer approach chooses an absorbance value just above the background noise (or the lower asymptotic level). **The highest dilution with an absorbance greater than this predetermined value is the endpoint titer.** This method is based on the assumption that a sample with a higher protein concentration will require a higher dilution factor to achieve an absorbance just above the level of background noise.

```{r}
endpoint_titer <- c * (((a - d) / (0.2 - d)) - 1) ^ (1 / b)

summary(endpoint_titer)

endpoint_titer

```

### Absorbance summation method

In this model of data analysis, we sum all the absorbance values from each sample to obtain one value. This value is termed as absorption summation (AS). Using the above data, the AS will be calculated as below:

```{r}
AS = 0.04 + 0.04 + 0.05 + 0.05 + 0.06 + 0.1 + 0.22 + 0.51 + 1.1 + 2.34 + 3.73 + 4.0

AS

```

## Use the the data analysis models in our data 

The presented data is from a mouse study. In this data, presence of IgG antibody has been evaluated against receptor binding domain (RBD) of SARS-CoV-2 virus in two different groups of mice. We need to elucidate which group has higher concentration of the antibodies. 

### read in the data

```{r}
elisa_data <- read_excel("DATA/elisa_data_serial_dilution.xlsx")
```

### make the tidy data

```{r}

elisa_data <- pivot_longer(data = elisa_data, cols = "Mouse_1":"Mouse_5", names_to = "mouse_id", values_to = "absorbance")

elisa_data

# separate dilution column and convert it to log2

elisa_data <- separate(elisa_data, col = "Dilution", into = c("numerator", "denomenator"), sep = "\\/")

elisa_data <- elisa_data %>% 
  transform(numerator = as.numeric(numerator),
            denomenator = as.numeric(denomenator))

elisa_data <- elisa_data %>%
  mutate(dilution = elisa_data$numerator / elisa_data$denomenator) 

elisa_data <- elisa_data %>%
  mutate(log_dilution = log2(dilution))

elisa_data

```


```{r}
# converting data into dataframe

elisa_data_df <- elisa_data %>% 
  group_by(Groups, mouse_id) %>% 
  summarize(log_dilution = log_dilution, absorbance = absorbance)

elisa_data_df

elisa_data_nested <- elisa_data %>%
  group_by(Groups, mouse_id) %>%
  nest()

elisa_data_nested

```


```{r}
# plot the curves to evaluate the a, d, c, and b

elisa_data %>%
  ggplot(aes(x = log_dilution, y = absorbance)) +
  geom_point() +
  geom_line() + 
  facet_wrap(Groups ~ mouse_id)
```

Based on the curve, the values are: 

a = 4, 
d = 0
c = 2
b = 1

## Creating a function for fitting model 
```{r}
# Creating a function for fitted model

fitted_model_elisa <- function(df_elisa, start_a, start_d, start_c, start_b) {
  mod_1 <- nlsLM(absorbance ~ ((a-d)/(1+(log_dilution/c)^b)) + d,
data = df_elisa,
start = list(a = start_a, d = start_d, c = start_c, b = start_b))
  return(mod_1)
}
```

### Fitting the model into the data

```{r}
fitted_model_elisa(elisa_data_nested$data[[1]], start_a = 4, start_d = 0, start_c = -8, start_b = 1)
```

### Apply the fitted model function to the whole dataframe

```{r}

elisa_fitted_data <- elisa_data_nested %>%
  mutate(fitted_data = purrr::map(data, ~ fitted_model_elisa(.x, start_a = 4, start_d = 0, start_c = -8, start_b = 1)))

elisa_fitted_data
```

### Take out the summary of the data

```{r}
elisa_fitted_data_summary <- elisa_fitted_data %>%
  mutate(fitted_data_summary = purrr::map(fitted_data, broom::glance))

unnested_elisa_fitted_data <- elisa_fitted_data_summary %>%
  unnest(elisa_fitted_data_summary) %>%
  ungroup() %>%
  dplyr::select(Groups, mouse_id, fitted_data)

unnested_elisa_fitted_data$fitted_data[[1]]

# Another way

elisa_fitted_data_summary %>% 
  unnest(fitted_data_summary)
```







### Creating a function for endpoint titer model and fitting the model into the data


<!--chapter:end:05-elisa.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

