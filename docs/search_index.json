[["index.html", "ImpactTB/BAA: Standard Operating Procedures for Data Analysis Chapter 1 Overview 1.1 About the project 1.2 About this book", " ImpactTB/BAA: Standard Operating Procedures for Data Analysis Colorado State University Coding Team 2022-12-06 Chapter 1 Overview 1.1 About the project The objective of the Immune Mechanisms of Protection against Mycobacterium tuberculosis (IMPAc-TB) program is to get a thorough understanding of the immune responses necessary to avoid initial infection with Mycobacterium tuberculosis (Mtb), formation of latent infection, and progression to active TB illness. To achieve these goals, the National Institute of Allergy and Infectious Diseases awarded substantial funding and established multidisciplinary research teams that will analyze immune responses against Mtb in animal models (mice, guinea pigs, and non-human primates) and humans, as well as immune responses elicited by promising vaccine candidates. The contract awards establish and give up to seven years of assistance for IMPAc-TB Centers to explain the immune responses required for Mtb infection protection. The seven centers that are part of the study are (in alphabetical order): Colorado State University Harvard T.H. Chan School of Public Health Seattle Children Hospital [more] Colorado State University Team and role of each member: Dr. Marcela Henao-Tamayo: Principal Investigator Dr. Brendan Podell: Principal Investigator Dr. Andres Obregon-Henao: Research Scientist-III Dr. Taru S. Dutt: Research Scientist-I [more] 1.2 About this book The aim of this book is to provide data protocols and data collection templates for key types of data that are collected over the course of this project. By using standard templates to record data, as well as starting from defined pipelines to process and analyze the data, we aim to standardize the collection and processing of data across this project. Here, we have built a comprehensive guide to wet lab data collection, sample processing, and computational tool creation for robust and efficient data analysis and dissemination. "],["experimental-metadata.html", "Chapter 2 Experimental metadata", " Chapter 2 Experimental metadata Metadata for an experiment: species start_date end_date experimental_groups "],["animal-initial-conditions-and-weekly-weights.html", "Chapter 3 Animal initial conditions and weekly weights", " Chapter 3 Animal initial conditions and weekly weights 3.0.1 Overview We use the template in this section to record information about each animal used in the experiment. This includes the species, sex, and experimental group. It also includes some information to identify the animal, which in the case of mice includes a code describing the pattern of notches put in the mouse’s ear and the cage that the animal is assigned to at the beginning of the experiment. These are all values that can be determined at the start of the experiment, when the mice are first assigned to groups. This template is also used to record some data over the course of the experiment. This includes adverse events and cases where an animal is moved from one cage to another during the experiment. In addition, in our experiments, we are measuring the mice every week to record their weight over the course of the experiment. This weight measuring begins before the first vaccination and continues through until the last mouse is sacrificed. We have used ear notches to identify each mouse, and between the ear notch and the mouse’s cage number, we can uniquely track each mouse in the study. There are a few reasons that we are measuring these mouse weights. The first is to help us manage the mice, particularly in terms of animal welfare. If there are mice that are losing a lot of weight, that can be an indication that they may need to be euthanized. For example, some animal care standards consider that an adult animal that has lost 20% or more of its weight compared to its baseline weight is indicating a clear sign of morbidity or suffering. A second reason is that the weight measure might provide a record of each mouse’s general health over the course of the study. In the study, mice are weighed in grams weekly to monitor clinical status, as one potential sign of tuberculosis infection and severity is weight loss. In humans, tuberculosis patients frequently display weight loss as a clinical symptom associated with disease progression. In particular, extreme weight loss and loss of muscle mass, also known as cachexia, can present as a result of chronic inflammatory illnesses like tuberculosis (Baazim, Antonio-Herrera, and Bergthaler 2022). This cachexia is part of a systemic response to inflammation, and in humans has been linked to upregulation of pro-inflammatory cytokines including tumor necrosis factor, interleukin-6, and interferon-gamma (Baazim, Antonio-Herrera, and Bergthaler 2022). Additionally, studies support a role in cachexia of key immune cell populations such as cytotoxic T-cells which, when depleted, counteract muscle and fat deterioration (Baazim et al. 2019), suggest that thsi type of T-cells may metabolically reprogram adipose tissue. Given these relationships between weight loss, diseases, and immune processes, it is possible that mouse weight might provide a regularly measurable insight into the severity of disease in each animal. While many of data points are collected to measure the final disease state of each animal, fewer are available before the animal is sacrificed. We are hoping that mouse weights will provide one measure that, while it may not perfectly capture disease severity, may provide some information throughout the experiment that is correlated to disease severity at regular time intervals. Other studies that use a mouse model of tuberculosis have collected mouse weights, as well (Smith et al. 2022; Segueni et al. 2016). We plan to investigate these data to visualize the trajectory of weight gain / loss in each mouse both before and after they are challenged with tuberculosis. We also plan to test whether each mouse’s weight change after challenge is correlated with other metrics of the severity of disease and immune response. We will do this by testing the correlation between the percent change in weight between challenge and sacrifice with CFUs at sacrifice as well as expression of cytokines and other biological markers (Smith et al. 2022). 3.0.2 Template description Both the animals’ initial conditions and their weekly measures (adverse events, cage changes, and weights) should be recorded in an excel worksheet. You can download a copy of the template here. The worksheet is divided into sheets. The first sheet is recorded at the first time point when the mice are measured and is used to record information about the mice that will remain unchanged over the course of the study, like species and sex. Here is what the first sheet of the template looks like: The second and later sheets are used to record the weight at each measured timepoint. The second sheet will record the weights on the first date they are measured, so it should be recorded at the same time as the first sheet—with initial mouse information—is completed. Here is what the first sheet of the template looks like: As you continue to measure at new timepoints, you should add a sheet at each timepoint, with each new sheet following the format of the second sheet in the template. The second and later sheets should be labeled with the date when those weights were measured (e.g., “5.26.22” for weights measured on May 26, 2022). When you download the template, it will have example values filled out in blue. Use these to get an idea for how to record your own data. When you are ready to record your own data, delete these example values and replace them with data collected from your own experiment. Column titles are as follows. First, in the first sheet, you will record: notch_id: Record the ear notch pattern in the mouse. Make sure that you record consistently across all timepoints, so that each mouse can be tracked across dates. If you are doing single notches, for example, this might be “0” for no notches, “1R” for one notch in the right ear, “1L” for one notch in the left ear, and “1R1L” for one notch in each ear. starting_cage_number: Record the number of the cage that the mouse is put into at the start of the experiment. In combination with the mouse’s notch_id, this will provide a unique identifier for each mouse at the start of the experiment. dob: Record the date the mouse was born. species: Record the species of the mouse (e.g., “C57BL/6” for C57 black 6 mice or “CBA” for CBA mice). sex: Record as “m” for male or “f” for female group: Provide the experimental group of the mouse. Be sure that you use the same abbreviation or notation across each timepoint. Examples of group designations might be: bcg, saline, bcg+id93, saline+id93, saline+noMtb For the second and later sheets, you will record: who_collected: Record the first name of the person who actually handled the mouse from the scale. date_collected: Record the date using quotation marks, with the month, then day, then year. For example, “May 31, 2022.” weight: Record as a number, without a unit in this column. The next column will be used for the units. unit: Provide the units that were used to take the weight (e.g., “g” for grams). Be consistent across all animals and timepoints in the abbreviation that you use (e.g., always use “g” for grams, not “g” sometimes and “grams” sometimes) existing_cage_number: Provide the cage number that the mouse is in when you start weighing at that time point. If the mouse is moved to another cage on this day, you will specify that in the next column. If the animal was moved from one cage to another between the last weighing and the date of the timepoint you are measuring, put in this column the cage number that the animal was in the last time it was weighed. new_cage_number: If the animal is moved to a new cage on the date of the timepoint you are measuring, then use this column to record the number of the cage you move it too. Similarly, if the animal moved cages between the last measured timepoint and this one, use this column to record the cage it was moved to. Otherwise, if the animal stays in the same cage that it was at the last measured time point, leave this column empty. group: Provide the experimental group of the mouse. Be sure that you use the same abbreviation or notation across each timepoint. Examples of group designations might be: bcg, saline, bcg+id93, saline+id93, saline+noMtb notes: Record information regarding clinical observations (e.g., “back is balding,” “barbering,” “excessive grooming,” “euthanized”). 3.0.3 Processing collected data Once data are collected, the file can be run through an R workflow. This workflow will convert the data into a format that is easier to work with for data analysis and visualization. It will also produce a report on the data in the spreadsheet, and ultimately it will also write relevant results in a format that can be used to populate a global database for all experiments in the project. The next section provides the details of the pipeline. It aims to explain the code that processes the data and generates visualizations. You do not need to run this code step-by-step, but instead can access a script with the full code here. To use this reporting template, you need to download it to your computer and save it in the file directory where you saved the data you collected with the data collection template. You can then open RStudio and navigate so that you are working within this directory. You should also make sure that you have installed a few required packages on R on the computer you are using to run the report. These packages are: tidyverse, purrr, lubridate, readxl, knitr, and ggbeeswarm. Within RStudio, open the report template file. There is one spot where you will need to change the code in the template file, so it will read in the data from the version of the template that you saved, which you may have renamed. In the YAML of the report template file, change the file path beside “data:” so that it is the file name of your data file. Once you’ve made this change, you can use the “Knit” button in RStudio to create a report from the data file and the report template file. The report includes the following elements: Summary table of animals at the start of the experiment Time series plots of animal weights over the experiment, grouped by experimental group Boxplots of the distribution of animal weights within each experimental group at the last available time point Plot of measured weight, identified by the person who was handling the animal, to help determine if there are consistent differences by handler Table of all the animals in the experiment at the last measured time point, ordered by their weight change since the previous measurement. This table is meant to help in identifying animals that may need to be euthanized for animal welfare reasons. You can download an example of a report created using this template by clicking [here]. When you knit to create the report, it will create a Word file in the same file directory where you put your data file and report template. It will also create and output a version of the data that has been processed (in the case of the weights data, this mainly involves tracking mice as they change cages, to link all weights that are from a single animal). This output fill will be named “…” and, like the report file, will be saved in the same file directory as the data file and the report template. 3.0.4 Details of processing script This section goes through the code within the report template. It explains each part of the code in detail. You do not need to understand these details to use the report template. However, if you have questions about how the data are being processed, or how the outputs are created, all those details are available in this section. As a note, there are two places in the following code where there’s a small change compared to the report template. In the report, you incorporate the path to the data file using the data: section in the YAML at the top of the document. In the following code, we’ve instead used the path of some example data within this book’s file directory, so the code will run for this chapter as well. First, the workflow loads some additional R libraries. You may need to install these on your local R session if you do not already have them installed. library(readxl) library(tidyverse) library(ggbeeswarm) These packages bring in some useful functions that are not available in the base installation of R. They are all open source. To cite any of them, you can use the citation function. For example, to get the information you would need to cite the readxl package, in R you can run: citation(&quot;readxl&quot;) ## ## To cite package &#39;readxl&#39; in publications use: ## ## Hadley Wickham and Jennifer Bryan (2019). readxl: Read Excel Files. R ## package version 1.3.1. https://CRAN.R-project.org/package=readxl ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {readxl: Read Excel Files}, ## author = {Hadley Wickham and Jennifer Bryan}, ## year = {2019}, ## note = {R package version 1.3.1}, ## url = {https://CRAN.R-project.org/package=readxl}, ## } Next, the code in the report template creates a few custom functions to help process the data from the data collection template. The first of these functions checks the data collection template to identify all the timepoints that were collected and then reads each in, ultimately joining data from all time points into one large dataset. The data collection template requires you to use a new sheet in the spreadsheet for each weight collection time point, with a first sheet that records initial information about the animals. If you only take weights at three time points, there would only be three time point sheets in the final file. Conversely, if you collect weight data at twenty time points, there would be twenty sheets in the final file. The first function, called ``, reads the data file, checks to find all the weight recording sheets, whether it’s three or twenty, and then reads the data in from all the sheets and binds them together into a single dataframe. ## Function to read in mouse weights. This takes a filepath to an Excel sheet ## that follows the template of the animal weight collection template. It ## identifies all the sheets in that file and reads in all the ones that ## measure weekly weights. It returns one large dataframe with all of the ## measured weights. read_mouse_weights &lt;- function(filepath) { # getting info about all excel sheets mouse_weights_sheets &lt;- readxl::excel_sheets(filepath)[-1] # First sheet is initial data, not mouse weights mouse_weights &lt;- purrr::map(mouse_weights_sheets, ~ readxl::read_excel(filepath, sheet = .x, col_types = c(&quot;text&quot;, # who_collected &quot;text&quot;, # date_collected &quot;text&quot;, # notch_id &quot;numeric&quot;, # weight &quot;text&quot;, # unit &quot;text&quot;, # existing_cage_number &quot;text&quot;, # new_cage_number &quot;text&quot;, # group &quot;text&quot; # notes ))) %&gt;% dplyr::bind_rows() %&gt;% mutate(date_collected = lubridate::mdy(date_collected)) return(mouse_weights) } The remaining functions are all functions to help track a mouse over the experiment even if it changes cages. In processing this data, the key challenge is to track a single mouse over the experiment. The mice are identified by a pattern of notches in their ears. However, there are a limited number of notches that can be distinguished, so the notch information does not distinctly identify every mouse in the study, just every mouse within a certain cage. By knowing both an ear notch ID and a cage number, you can distinctly identify each mouse in the study. However, mice are moved from one cage to another in some cases during a study. If mice within a cage are fighting, or if they are showing signs of excessive grooming, these can be reasons to move a mouse to a new cage once the experiment has started. The cage moves need to be resolved when processing the data so that each mouse can be tracked even as they move. In the data collection template, we have created a design that aims to include information about cage moves, but to do so in a way that is as simple as possible for the person who is recording the data. The weights are recorded for each time point in a separate sheet of the data collection template. On the sheet for a time point, there are also columns to give the mouse’s cage at the start of that data collection time point, as well as the cage the mouse was moved to, if it was moved. The report template code then uses this information to create a unique ID for each mouse (one that is constant across the experiment), and then attach it to the mouse’s measurements even as the mouse is moved from one cage to another. The following two functions both help with this process: # Function to get the next cage number based on the # existing cage number and notch ID. If the mouse does not # switch cages again, the output is a vector of length 0. # This takes the dataframe and existing identifiers (notch id and # existing cage number) as inputs. It returns the next cage # that the mouse was moved to. If the mouse has not moved # from the existing case, the output has length 0. get_next_cage &lt;- function(existing_cage_number, notch_id, df = our_mouse_weights){ next_cage &lt;- df %&gt;% filter(.data$existing_cage_number == {{existing_cage_number}} &amp; .data$notch_id == {{notch_id}} &amp; !is.na(.data$new_cage_number)) %&gt;% pull(new_cage_number) return(next_cage) } # Function to get the full list of cages for each individual # mouse, over the course of all data collected to date. This # inputs the starting identifiers of the mouse (starting cage ID # and notch ID). It then works through any cage changes to create # a list for that mouse of all cages it was put in over the # course of the experiment. get_mouse_cages &lt;- function(mouse_starting_cage, mouse_notch_id, df = our_mouse_weights){ mouse_cage_list &lt;- mouse_starting_cage i &lt;- 1 while(TRUE){ next_cage &lt;- get_next_cage(existing_cage_number = mouse_cage_list[i], notch_id = mouse_notch_id, df = df) if(length(next_cage) == 0) { break } i &lt;- i + 1 mouse_cage_list[i] &lt;- next_cage } return(mouse_cage_list) } Next, the report template code gets to the workflow itself, where it uses both these custom functions and other R code to process the data and then to provide summaries and visualizations of the data. The first step in the workflow is to read in the data from the spreadsheet. As long as the data are collected following the template that was described earlier, this code should be able to read it in correctly and create a master dataset with the data from all sheets of the spreadsheet. This step of the pipeline uses one of the custom functions that was defined at the start of the report template code: # Read in the mouse weights from the Excel template. This creates one large # dataframe with the weights from all the timepoints. our_mouse_weights &lt;- read_mouse_weights(filepath = &quot;DATA/body_weights_measurement.xlsx&quot;) Next, the code runs through a number of steps to create a unique ID for each mouse and then apply that ID to each time point, even if a mouse changes cages. # Add a unique mouse ID for the first time point. This will become each mouse&#39;s # unique ID across all measured timepoints. our_mouse_weights &lt;- our_mouse_weights %&gt;% mutate(mouse_id = 1:n(), mouse_id = ifelse(date_collected == first(date_collected), mouse_id, NA)) # Create a dataframe that lists all mice at the first time point, # as well as a list of all the cages they have been in over the # experiment mice_cage_lists &lt;- our_mouse_weights %&gt;% filter(date_collected == first(date_collected)) %&gt;% select(notch_id, existing_cage_number, mouse_id) %&gt;% mutate(cage_list = map2(.x = existing_cage_number, .y = notch_id, .f = ~ get_mouse_cages(.x, .y, df = our_mouse_weights))) # Add a column with the latest cage to the weight dataframe our_mouse_weights$latest_cage &lt;- NA # Loop through all the individual mice, based on mice with a # measurement at the first time point. Add the unique ID for # each mouse, which will apply throughout the experiment. Also # add the most recent cage ID, so the mouse can be identified # by lab members based on it&#39;s current location for(i in 1:nrow(mice_cage_lists)){ this_notch_id &lt;- mice_cage_lists[i, ]$notch_id this_cage_list &lt;- mice_cage_lists[i, ]$cage_list[[1]] this_unique_id &lt;- mice_cage_lists[i, ]$mouse_id latest_cage &lt;- this_cage_list[length(this_cage_list)] our_mouse_weights$mouse_id[our_mouse_weights$notch_id == this_notch_id &amp; our_mouse_weights$existing_cage_number %in% this_cage_list] &lt;- this_unique_id our_mouse_weights$latest_cage[our_mouse_weights$notch_id == this_notch_id &amp; our_mouse_weights$existing_cage_number %in% this_cage_list] &lt;- latest_cage } # Add a label for each mouse based on its notch_id and latest cage our_mouse_weights &lt;- our_mouse_weights %&gt;% mutate(mouse_label = paste(&quot;Cage:&quot;, latest_cage, &quot;Notch:&quot;, notch_id)) Ultimately, this creates both a unique ID for each mouse (in a column of the dataframe called mouse_id), as well as creates a unique label that can be used in plots and tables (given in the mouse_label column). The unique ID is set at the beginning of the study for each mouse and remains the same throughout the study. The label, on the other hand, is based on the mouse’s ear notch pattern and the most recent cage it was recorded to be in. We made this choice for a labeling identifier, because it will help the researchers to quickly identify a mouse in the study based on it’s current, rather than starting, cage. The next part of the code reads in the initial data that were recorded for each animal in the experiment. The code then pulls in information from the processed weights dataset to match these initial data with each animals unique ID. Ultimately, these starting data are incorporated into the large dataset of mouse weights, creating a single large dataset to work with (our_mouse_weights) that includes all the information that was recorded in the data collection template. # Read in the data from the original file with the initial animal # characteristics mouse_initial &lt;- readxl::read_excel(&quot;DATA/body_weights_measurement.xlsx&quot;, sheet = 1, col_types = c(&quot;text&quot;, # notch_id &quot;text&quot;, # starting_cage_number &quot;text&quot;, # dob &quot;text&quot;, # species &quot;text&quot;, # sex &quot;text&quot; # group )) %&gt;% mutate(dob = lubridate::mdy(dob), sex = forcats::as_factor(sex)) # Figure out the starting cage for each mouse, so they can be incorporated # with the initial data so we can get the mouse ID that was added for the # starting time point mouse_ids &lt;- our_mouse_weights %&gt;% filter(date_collected == first(date_collected)) %&gt;% select(notch_id, existing_cage_number, mouse_id) %&gt;% rename(starting_cage_number = existing_cage_number) # Merge in the mouse IDs with the dataframe of initial mouse characteristics mouse_initial &lt;- mouse_initial %&gt;% left_join(mouse_ids, by = c(&quot;notch_id&quot;, &quot;starting_cage_number&quot;)) # Join the initial data with the weekly weights data into one large dataset our_mouse_weights &lt;- our_mouse_weights %&gt;% left_join(mouse_initial, by = c(&quot;mouse_id&quot;, &quot;notch_id&quot;, &quot;group&quot;)) At this point, the first few rows of this large dataset look like this: our_mouse_weights %&gt;% slice(1:5) ## # A tibble: 5 × 16 ## who_collected date_collected notch_id weight unit existing_cage_number ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Taru 2022-05-26 0 18.4 g 22003 ## 2 Taru 2022-05-26 1R 17.2 g 22003 ## 3 Taru 2022-05-26 1L 17 g 22003 ## 4 Taru 2022-05-26 1R1L 18.8 g 22003 ## 5 Taru 2022-05-26 0 18.4 g 22004 ## # … with 10 more variables: new_cage_number &lt;chr&gt;, group &lt;chr&gt;, notes &lt;chr&gt;, ## # mouse_id &lt;int&gt;, latest_cage &lt;chr&gt;, mouse_label &lt;chr&gt;, ## # starting_cage_number &lt;chr&gt;, dob &lt;date&gt;, species &lt;chr&gt;, sex &lt;fct&gt; The rest of the code in the report template will create summaries and graphs of the data. First, there is some code that provides summaries of the research animals at the start of the experiment. It uses the mouse_initial dataset (which pulled in data from the first sheet of the data collection template). It uses a summarize call to summarize details from this sheet of data, including the species of the animal, the total number of animals, how many were males versus females, and which experimental groups were included. It uses some additional code to format the data so the resulting table will be clearer, and then uses the kable function to output the results as a nicely formatted table. # Create a table that summarizes the animals at the start of the experiment mouse_initial %&gt;% summarize(Species = paste(unique(species), collapse = &quot;, &quot;), `Total animals` = n(), `Sex distribution` = paste0(&quot;male: &quot;, sum(sex == &quot;m&quot;), &quot;, female: &quot;, sum(sex == &quot;f&quot;)), `Experimental groups` = paste(unique(group), collapse = &quot;, &quot;), `N. of starting cages` = length(unique(starting_cage_number))) %&gt;% mutate_all(as.character) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = paste0(name, &quot;:&quot;)) %&gt;% knitr::kable(col.names = c(&quot;&quot;, &quot;&quot;), caption = &quot;Summary of experimental animals at the start of the experiment&quot;, align = c(&quot;r&quot;, &quot;l&quot;)) Table 3.1: Summary of experimental animals at the start of the experiment Species: C57BL/6 Total animals: 140 Sex distribution: male: 60, female: 80 Experimental groups: bcg, bcg+id93, saline, saline+id93, saline+noMtb N. of starting cages: 34 The next piece of code creates a time series of mouse weights over time. The points for each mouse are connected to create a line, so it’s easy to see both variation across mice at a single time point and variation in a single mouse over the study. The lines are colored to distinguish male from female mouse (and there is a clear difference in average weights in the two groups). The plot is faceted so that the time series for mice in each experimental group are shown in different small “facets” of the plot, but with the same axis ranges used on each small plot to help comparisons across plots. # Create a plot of mouse weights over time our_mouse_weights %&gt;% ggplot(aes(x = date_collected, y = weight, group = mouse_id, color = sex)) + geom_line() + facet_wrap(~ group) + ggtitle(&quot;Animal weights over time by experiment group&quot;) + labs(x = &quot;Date collected&quot;, y = &quot;Weight (g)&quot;) Next, the code creates boxplots that focus on differences in weights at the latest available timepoint. One boxplot is created for each experimental group, and the points for individual mice are shown behind the boxplot, to provide a better idea of the pattern of variation in individual mice. These points are colored based on sex, to help explore patterns by sex. # Plot animal weight boxplots for the latest time point our_mouse_weights %&gt;% filter(date_collected == last(date_collected)) %&gt;% ggplot(aes(x = group, y = weight)) + geom_beeswarm(aes(color = sex)) + geom_boxplot(fill = NA, color = &quot;dodgerblue&quot;) + ggtitle(&quot;Animal weights at last collection by experimental group&quot;) + labs(x = &quot;Experimental group&quot;, y = &quot;Weight (g)&quot;) The next piece of code shows how mouse weights vary by the person who was handling the animals at a certain time point. Different handlers may have small differences in how they handle and weight the mice. If there are noticable differences in the measured weights, this is something that could be corrected through statisical modeling in later analysis, so we included it as a potential check. # Plot animal weights by animal handler our_mouse_weights %&gt;% ggplot(aes(x = date_collected, y = weight, color = who_collected)) + geom_point() + ggtitle(&quot;Animal weights by animal handler&quot;) + labs(x = &quot;Date collected&quot;, y = &quot;Weight (g)&quot;, color = &quot;Person who\\nhandled the\\nanimal&quot;) The next piece of code creates a table with each of the animals that was still tracked at the last time point (if animals were sacrificed prior to the last recorded time point, they would not be included here). This table focuses on the weight change since the previous measured time point. It is ordered by the change in weight, from the largest decrease to the largest increase. It is meant as an aide in identifying mice that are showing signs of suffering and may need to be considered for being euthanized. The animals are labeled in this table by their most recent cage location, so it will be easier to find them if necessary. # Create table of animal weight changes since previous time point our_mouse_weights %&gt;% select(date_collected, weight, group, mouse_label, sex) %&gt;% group_by(mouse_label) %&gt;% mutate(weight_change = (weight - lag(weight)) / lag(weight)) %&gt;% ungroup() %&gt;% filter(date_collected == last(date_collected)) %&gt;% mutate(formatted_weight_change = paste0(formatC(weight_change * 100, digits = 1, format = &quot;f&quot;), &quot;%&quot;)) %&gt;% arrange(weight_change) %&gt;% select(mouse_label, group, sex, weight, formatted_weight_change) %&gt;% knitr::kable(col.names = c(&quot;Mouse&quot;, &quot;Experimental group&quot;, &quot;Sex&quot;, &quot;Weight (g)&quot;, &quot;Weight change since last measure&quot;), caption = &quot;Individual data on weight changes in mice between current measurement and previous measurement.&quot;) Table 3.2: Individual data on weight changes in mice between current measurement and previous measurement. Mouse Experimental group Sex Weight (g) Weight change since last measure Cage: 22021 Notch: 1R bcg m 25.20 -10.4% Cage: 22017 Notch: 1R saline f 23.13 -4.1% Cage: 22015 Notch: 1L bcg f 23.18 -3.1% Cage: 22476 Notch: 1R1L saline+noMtb f 20.54 -2.8% Cage: 22014 Notch: 1L saline+id93 f 21.47 -2.8% Cage: 22014 Notch: 0 saline+id93 f 21.40 -2.7% Cage: 22006 Notch: 1R1L bcg+id93 f 21.02 -2.4% Cage: 22015 Notch: 0 bcg f 20.06 -1.0% Cage: 22004 Notch: 1R bcg f 21.42 -1.0% Cage: 22006 Notch: 0 bcg+id93 f 18.67 -0.7% Cage: 22476 Notch: 0 saline+noMtb f 19.42 -0.6% Cage: 22016 Notch: 1R1L bcg+id93 f 23.13 -0.5% Cage: 22012 Notch: 1R saline+id93 f 22.44 -0.4% Cage: 22015 Notch: 2L bcg f 20.56 -0.3% Cage: 22013B Notch: 1L saline+id93 f 20.46 -0.2% Cage: 22009A Notch: 0 saline f 22.50 -0.1% Cage: 22029 Notch: 0 saline+id93 m 29.12 0.3% Cage: 22017 Notch: 1L saline f 21.72 0.6% Cage: 22022 Notch: 1R bcg+id93 m 25.90 0.6% Cage: 22016 Notch: 2L bcg+id93 f 22.37 0.7% Cage: 22004 Notch: 1R1L bcg f 20.91 0.8% Cage: 22477 Notch: 1L saline+noMtb m 30.92 1.0% Cage: 22013A Notch: 0 saline+id93 f 20.04 1.1% Cage: 22007A Notch: 0 bcg+id93 f 20.42 1.1% Cage: 22474 Notch: 0 saline+noMtb f 21.93 1.2% Cage: 22476 Notch: 1R saline+noMtb f 23.14 1.3% Cage: 22476 Notch: 1L saline+noMtb f 19.78 1.4% Cage: 22477 Notch: 1R saline+noMtb m 30.15 1.4% Cage: 22004 Notch: 0 bcg f 22.20 1.5% Cage: 22012A Notch: 1L saline+id93 f 21.10 1.5% Cage: 22025 Notch: 0 saline m 29.54 1.6% Cage: 22006 Notch: 1R bcg+id93 f 18.72 1.6% Cage: 22005 Notch: 1R bcg f 24.25 1.8% Cage: 22011 Notch: 0 saline f 23.02 1.8% Cage: 22028 Notch: 1L saline+id93 m 28.20 1.9% Cage: 22474 Notch: 1R saline+noMtb f 25.57 2.0% Cage: 22011 Notch: 1R1L saline f 21.04 2.1% Cage: 22020 Notch: 1R1L bcg m 31.00 2.1% Cage: 22022 Notch: 1R1L bcg+id93 m 28.65 2.2% Cage: 22474 Notch: 1R1L saline+noMtb f 22.42 2.2% Cage: 22018 Notch: 1L saline+id93 f 19.16 2.4% Cage: 22029 Notch: 1R saline+id93 m 29.50 2.4% Cage: 22478 Notch: 1R saline+noMtb m 28.21 2.4% Cage: 22475 Notch: 1R saline+noMtb f 22.35 2.5% Cage: 22029 Notch: 1L saline+id93 m 28.63 2.6% Cage: 22030 Notch: 0 saline+id93 m 29.36 2.6% Cage: 22019 Notch: 1R bcg m 27.07 2.7% Cage: 22008 Notch: 0 bcg+id93 f 19.80 2.8% Cage: 22478 Notch: 0 saline+noMtb m 28.53 2.8% Cage: 22019 Notch: 0 bcg m 29.57 2.8% Cage: 22013C Notch: 1R1L saline+id93 f 21.28 2.8% Cage: 22479 Notch: 1L saline+noMtb m 28.91 2.8% Cage: 22017 Notch: 1R1L saline f 23.10 2.8% Cage: 22020 Notch: 1L bcg m 28.18 2.9% Cage: 22021 Notch: 0 bcg m 28.95 3.1% Cage: 22028 Notch: 1R saline+id93 m 29.92 3.1% Cage: 22003 Notch: 1R bcg f 20.23 3.1% Cage: 22004 Notch: 1L bcg f 24.01 3.2% Cage: 22007 Notch: 1R1L bcg+id93 f 21.08 3.2% Cage: 22017 Notch: 0 saline f 22.25 3.2% Cage: 22016 Notch: 1L bcg+id93 f 22.85 3.2% Cage: 22029 Notch: 1R1L saline+id93 m 29.09 3.2% Cage: 22010 Notch: 1R1L saline f 23.24 3.2% Cage: 22030 Notch: 1R1L saline+id93 m 27.80 3.3% Cage: 22007 Notch: 1R bcg+id93 f 22.24 3.3% Cage: 22011 Notch: 1L saline f 23.49 3.3% Cage: 22025 Notch: 1R saline m 26.55 3.3% Cage: 22027 Notch: 1R1L saline m 26.30 3.5% Cage: 22024 Notch: 1R bcg+id93 m 31.37 3.5% Cage: 22479 Notch: 1R1L saline+noMtb m 30.93 3.5% Cage: 22020 Notch: 1R bcg m 25.86 3.6% Cage: 22005 Notch: 1L bcg f 22.01 3.6% Cage: 22024 Notch: 0 bcg+id93 m 30.35 3.7% Cage: 22026 Notch: 1R saline m 29.40 3.7% Cage: 22477 Notch: 0 saline+noMtb m 30.41 3.8% Cage: 22030 Notch: 1R saline+id93 m 33.13 3.8% Cage: 22018 Notch: 2R saline+id93 f 19.94 3.8% Cage: 22007 Notch: 1L bcg+id93 f 18.48 3.8% Cage: 22023 Notch: 0 bcg+id93 m 25.46 3.8% Cage: 22021 Notch: 1L bcg m 28.83 3.9% Cage: 22027 Notch: 0 saline m 25.85 4.0% Cage: 22024 Notch: 1L bcg+id93 m 26.37 4.1% Cage: 22012 Notch: 1R1L saline+id93 f 22.80 4.2% Cage: 22028 Notch: 0 saline+id93 m 27.79 4.2% Cage: 22030 Notch: 1L saline+id93 m 29.23 4.2% Cage: 22475 Notch: 1L saline+noMtb f 22.92 4.3% Cage: 22020 Notch: 0 bcg m 29.65 4.3% Cage: 22017 Notch: 2L saline f 24.53 4.3% Cage: 22477 Notch: 1R1L saline+noMtb m 31.00 4.3% Cage: 22009 Notch: 1R1L saline f 20.18 4.5% Cage: 22014 Notch: 1R saline+id93 f 21.25 4.5% Cage: 22014 Notch: 1R1L saline+id93 f 22.30 4.5% Cage: 22479 Notch: 0 saline+noMtb m 29.65 4.5% Cage: 22478 Notch: 1R1L saline+noMtb m 31.71 4.5% Cage: 22018 Notch: 0 saline+id93 f 22.65 4.5% Cage: 22026 Notch: 0 saline m 29.50 4.6% Cage: 22022 Notch: 0 bcg+id93 m 27.30 4.9% Cage: 22019 Notch: 1R1L bcg m 28.30 4.9% Cage: 22008 Notch: 1L bcg+id93 f 21.52 5.1% Cage: 22479 Notch: 1R saline+noMtb m 30.92 5.1% Cage: 22003 Notch: 0 bcg f 23.14 5.2% Cage: 22005 Notch: 0 bcg f 23.52 5.2% Cage: 22026 Notch: 1R1L saline m 29.80 5.2% Cage: 22478 Notch: 1L saline+noMtb m 30.34 5.2% Cage: 22026 Notch: 1L saline m 29.28 5.2% Cage: 22009 Notch: 1R saline f 21.85 5.3% Cage: 22024 Notch: 1R1L bcg+id93 m 31.11 5.4% Cage: 22475 Notch: 0 saline+noMtb f 22.00 5.5% Cage: 22006 Notch: 1L bcg+id93 f 23.42 5.5% Cage: 22023 Notch: 1R bcg+id93 m 29.27 5.7% Cage: 22016 Notch: 0 bcg+id93 f 22.94 5.7% Cage: 22025 Notch: 1L saline m 30.71 5.8% Cage: 22015 Notch: 1R1L bcg f 21.34 5.9% Cage: 22010 Notch: 1L saline f 21.75 5.9% Cage: 22008 Notch: 1R1L bcg+id93 f 22.07 6.1% Cage: 22025 Notch: 1R1L saline m 27.45 6.1% Cage: 22018 Notch: 1R1L saline+id93 f 20.15 6.3% Cage: 22023 Notch: 1L bcg+id93 m 29.21 6.3% Cage: 22012 Notch: 0 saline+id93 f 22.71 6.4% Cage: 22028 Notch: 1R1L saline+id93 m 30.22 6.6% Cage: 22475 Notch: 1R1L saline+noMtb f 21.76 6.6% Cage: 22022 Notch: 1L bcg+id93 m 30.77 6.9% Cage: 22027 Notch: 1R saline m 29.12 7.0% Cage: 22008 Notch: 1R bcg+id93 f 20.25 7.1% Cage: 22010 Notch: 1R saline f 22.74 7.3% Cage: 22019 Notch: 1L bcg m 33.57 7.4% Cage: 22018 Notch: 1R saline+id93 f 21.63 7.4% Cage: 22005 Notch: 1R1L bcg f 26.56 7.6% Cage: 22480 Notch: 1L saline f 26.30 7.7% Cage: 22011 Notch: 1R saline f 19.84 7.8% Cage: 22023 Notch: 1R1L bcg+id93 m 26.36 8.0% Cage: 22016 Notch: 1R bcg+id93 f 22.47 8.1% Cage: 22003 Notch: 1R1L bcg f 22.66 8.2% Cage: 22474 Notch: 1L saline+noMtb f 22.50 8.2% Cage: 22010 Notch: 0 saline f 24.44 8.2% Cage: 22027 Notch: 1L saline m 27.50 8.5% Cage: 22015 Notch: 1R bcg f 22.96 10.4% Cage: 22013 Notch: 1R saline+id93 f 25.57 11.1% Cage: 22003 Notch: 1L bcg f 23.45 11.3% Cage: 22021 Notch: 1R1L bcg m 28.89 19.5% As a last step, the code in the template writes a CSV file with the processed data. This file will be an input into a script that will format the data to add to a database where we are collecting and integrating data from all the CSU experiments, and ultimately from there into project-wide storage. # Write out processed data into a CSV file write_csv(our_mouse_weights, &quot;example_mouse_output.csv&quot;) References "],["colony-forming-units-to-determine-bacterial-counts.html", "Chapter 4 Colony forming units to determine bacterial counts 4.1 Data description 4.2 Read in data 4.3 Example one 4.4 Exploratory analysis and quality checks 4.5 Exploratory analysis 4.6 Identify a good dilution for each sample 4.7 Calculate CFUs from best dilution/Estimate bacterial load for each sample based on good dilution 4.8 Create initial report information for these data 4.9 Sample ANOVA 4.10 Save processed data to database 4.11 Example two", " Chapter 4 Colony forming units to determine bacterial counts 4.1 Data description The data are collected in a spreadsheet with multiple sheets. The first sheet (named “metadata”) is used to record some metadata for the experiment, while the following sheets are used to record CFUs counts from the plates used for samples from each organ, with one sheet per organ. For example, if you plated data from both the lung and spleen, there would be three sheets in the file: one with the metadata, one with the plate counts for the lung, and one with the plate counts for the spleen. The metadata sheet is used to record information about the overall process of plating the data. Values from this sheet will be used in calculating the bacterial load in the original sample based on the CFU counts. This spreadsheet includes the following columns: organ: Include one row for each organ that was plated in the experiment. You should name the organ all in lowercase (e.g., “lung,” “spleen”). You should use the same name to also name the sheet that records data for that organ for example, if you have rows in the metadata sheet for “lung” and “spleen,” then you should have two other sheets in the file, one sheet named “lung” and one named “spleen,” which you’ll use to store the plate counts for each of those organs. prop_resuspended: In this column, give the proportion of that organ that was plated. For example, if you plated half the lung, then in the “lung” row of this spread sheet, you should put 0.5 in the prop_resuspended column. total_resuspended_uL: This column contains an original volume of tissue homogenate. For example, raw lung tissue is homogenized in 500 uL of PBS in a tube containing metal beads. og_aliquot_uL: 100 uL of th total_resuspended slurry would be considered an original aliquot and is used to peform serial dilutions. dilution_factor: Amount of the original stock solution that is present in the total solution, after dilution(s) plated_uL: Amount of suspension + diluent plated on section of solid agar 4.2 Read in data library(readxl) library(dplyr) library(purrr) library(tidyr) library(stringr) library(tidyverse) library(gridExtra) library(ggplot2) library(ggpubr) #Replace w/ path to CFU sheet path &lt;- c(&quot;DATA/Copy of baa_cfu_sheet.xlsx&quot;) sheet_names &lt;- excel_sheets(path) sheet_names &lt;- sheet_names[!sheet_names %in% c(&quot;metadata&quot;)] merged_data &lt;- list() for(i in 1:length(sheet_names)){ data &lt;- read_excel(path, sheet = sheet_names[i]) %&gt;% mutate(organ = paste0(sheet_names[i])) data &lt;- data %&gt;% #mutate(missing_col = NA) %&gt;% mutate_if(is.double, as.numeric) %&gt;% mutate_if(is.numeric, as.character) %&gt;% pivot_longer(starts_with(&quot;dil_&quot;), names_to = &quot;dilution&quot;, values_to = &quot;CFUs&quot;) %&gt;% mutate(dilution = str_extract(dilution, &quot;[0-9]+&quot;), dilution = as.numeric(dilution)) merged_data[[i]] &lt;- data } all_data &lt;- bind_rows(merged_data, .id = &quot;column_label&quot;) %&gt;% select(-column_label) head(merged_data) ## [[1]] ## # A tibble: 342 × 8 ## count_date who_plated who_counted groups mouse organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &quot;\\&quot;February 21 2022… BK BK group… A lung 0 TNTC ## 2 &quot;\\&quot;February 21 2022… BK BK group… A lung 1 TNTC ## 3 &quot;\\&quot;February 21 2022… BK BK group… A lung 2 TNTC ## 4 &quot;\\&quot;February 21 2022… BK BK group… A lung 3 53 ## 5 &quot;\\&quot;February 21 2022… BK BK group… A lung 4 9 ## 6 &quot;\\&quot;February 21 2022… BK BK group… A lung 5 4 ## 7 &quot;\\&quot;February 21 2022… BK BK group… A lung 6 2 ## 8 &quot;\\&quot;February 21 2022… BK BK group… A lung 7 1 ## 9 &quot;\\&quot;February 21 2022… BK BK group… A lung 8 0 ## 10 &quot;\\&quot;February 21 2022… BK BK group… B lung 0 TNTC ## # … with 332 more rows ## ## [[2]] ## # A tibble: 112 × 8 ## count_date who_plated who_counted groups mouse organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 0 TNTC ## 2 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 1 TNTC ## 3 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 2 53 ## 4 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 3 9 ## 5 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 4 4 ## 6 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 5 2 ## 7 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 6 1 ## 8 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 7 0 ## 9 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 B sple… 0 TNTC ## 10 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 B sple… 1 TNTC ## # … with 102 more rows head(all_data) ## # A tibble: 6 × 8 ## count_date who_plated who_counted groups mouse organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 0 TNTC ## 2 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 1 TNTC ## 3 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 2 TNTC ## 4 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 3 53 ## 5 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 4 9 ## 6 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 5 4 4.3 Example one 4.4 Exploratory analysis and quality checks 4.5 Exploratory analysis Dimensions of input data: Based on the input data, data were collected for the following organ or organs: The following number of mice were included for each: The following number of replicates were recorded at each count date for each experimental group: The following number of dilutions and dilution level were recorded for each organ: People who plated and collected the data. Date or dates of counting: Based on the input data, the plates included in these data were counted by the following person or persons: Based on the input data, the plates included in these data were counted on the following date or dates: all_data %&gt;% select(organ, who_plated, who_counted, count_date) %&gt;% distinct() ## # A tibble: 3 × 4 ## organ who_plated who_counted count_date ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 lung BK BK &quot;\\&quot;February 21 2022\\&quot;&quot; ## 2 lung BK BK &quot;\\&quot;April 18 2022\\&quot;&quot; ## 3 spleen JR JR &quot;\\&quot;April 25 2022\\&quot;&quot; head(all_data) ## # A tibble: 6 × 8 ## count_date who_plated who_counted groups mouse organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 0 TNTC ## 2 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 1 TNTC ## 3 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 2 TNTC ## 4 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 3 53 ## 5 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 4 9 ## 6 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 5 4 Distribution of CFUs at each dilution: Here’s a plot that shows how many plates were too numerous to count at each dilution level: Here is a plot that shows how the CFU counts were distributed by dilution level in the data: 4.6 Identify a good dilution for each sample # Make all_data into tidy data and filter for CFUs between 10-75 tidy_cfu_data &lt;- all_data %&gt;% mutate(dilution = str_extract(dilution, &quot;[0-9]+&quot;), dilution = as.numeric(dilution)) %&gt;% filter((CFUs &gt;= 5 &amp; CFUs &lt;= 95) | groups == &quot;control&quot;) %&gt;% mutate(CFUs = as.numeric(CFUs)) head(tidy_cfu_data) ## # A tibble: 6 × 8 ## count_date who_plated who_counted groups mouse organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 3 53 ## 2 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 4 9 ## 3 &quot;\\&quot;February 21 2022\\… BK BK group… C lung 5 8 ## 4 &quot;\\&quot;February 21 2022\\… BK BK group… D lung 3 53 ## 5 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 2 92 ## 6 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 4 7 4.7 Calculate CFUs from best dilution/Estimate bacterial load for each sample based on good dilution # Calculating CFU/ml for every qualifying replicate between 10-75 CFUs. Column binding by organ name to the metadata sheet via inner_join(). meta &lt;- read_excel(path, sheet = &quot;metadata&quot;) tidy_cfu_meta_joined &lt;- inner_join(meta, tidy_cfu_data) %&gt;% group_by(groups) %&gt;% mutate(CFUs_per_ml = (CFUs * (dilution_factor^dilution) * (total_resuspension_mL/volume_plated_ul) * 1000)) %&gt;% select(organ, count_date, who_plated, who_counted, groups, mouse, dilution, CFUs, CFUs_per_ml) %&gt;% ungroup() ## Joining, by = &quot;organ&quot; head(tidy_cfu_meta_joined) ## # A tibble: 6 × 9 ## organ count_date who_plated who_counted groups mouse dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lung &quot;\\&quot;February 21 2022\\… BK BK group… A 3 53 ## 2 lung &quot;\\&quot;February 21 2022\\… BK BK group… A 4 9 ## 3 lung &quot;\\&quot;February 21 2022\\… BK BK group… C 5 8 ## 4 lung &quot;\\&quot;February 21 2022\\… BK BK group… D 3 53 ## 5 lung &quot;\\&quot;February 21 2022\\… BK BK group… A 2 92 ## 6 lung &quot;\\&quot;February 21 2022\\… BK BK group… A 4 7 ## # … with 1 more variable: CFUs_per_ml &lt;dbl&gt; 4.8 Create initial report information for these data tidy_lung_cfu_plot &lt;- tidy_cfu_meta_joined %&gt;% filter(organ == &quot;lung&quot;) %&gt;% mutate(group = fct_relevel(groups, &quot;group_1&quot;, &quot;group_2&quot;, &quot;group_3&quot;, &quot;group_4&quot;)) %&gt;% ggplot(aes(x = groups, y = log10(CFUs_per_ml), fill = groups))+ stat_boxplot( aes(x = groups, y = log10(CFUs_per_ml)), geom=&#39;errorbar&#39;, linetype=1, width=0.5)+ geom_boxplot(aes(group = groups), fill = NA, show.legend = FALSE, color = &quot;lightgrey&quot;)+ geom_point(show.legend = FALSE)+ labs(title = paste0(&quot;CFUs in early infected mouse lung&quot;), x = &quot;Group&quot;, y = &quot;log10(CFU/mL)&quot;, color = &quot;Group&quot;)+ guides(shape = &quot;none&quot;)+ theme_minimal()+ stat_compare_means(label = &quot;p.signif&quot;, method = &quot;t.test&quot;, ref.group = &quot;group_1&quot;) + scale_y_continuous(expand = c(0, 0), limits = c(0, 8)) tidy_lung_cfu_plot 4.9 Sample ANOVA cfu_stats &lt;- tidy_cfu_meta_joined %&gt;% group_by(organ) %&gt;% nest() %&gt;% mutate(aov_result = map(data, ~aov(CFUs_per_ml ~ groups, data = .x)), tukey_result = map(aov_result, TukeyHSD), tidy_tukey = map(tukey_result, broom::tidy)) %&gt;% unnest(tidy_tukey, .drop = TRUE) %&gt;% separate(contrast, into = c(&quot;contrast1&quot;, &quot;contrast2&quot;), sep = &quot;-&quot;) %&gt;% select(-data, -aov_result, -tukey_result, -term, -null.value)# %&gt;% ## Warning: The `.drop` argument of `unnest()` is deprecated as of tidyr 1.0.0. ## All list-columns are now preserved. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. # filter(adj.p.value &lt;= 0.05) cfu_stats ## # A tibble: 9 × 7 ## # Groups: organ [2] ## organ contrast1 contrast2 estimate conf.low conf.high adj.p.value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lung group_2 group_1 -60953. -138742. 16836. 0.171 ## 2 lung group_3 group_1 -63903. -135699. 7893. 0.0963 ## 3 lung group_4 group_1 -26214. -102416. 49987. 0.793 ## 4 lung group_3 group_2 -2950. -69900. 64000. 0.999 ## 5 lung group_4 group_2 34739. -36915. 106393. 0.569 ## 6 lung group_4 group_3 37689. -27410. 102787. 0.417 ## 7 spleen group_2 group_1 -6565 -13529. 399. 0.0656 ## 8 spleen group_3 group_1 -7310 -13341. -1279. 0.0178 ## 9 spleen group_3 group_2 -745. -6776. 5286. 0.943 4.10 Save processed data to database 4.11 Example two "],["enzyme-linked-immunosorbest-assay-elisa.html", "Chapter 5 Enzyme-linked immunosorbest assay (ELISA) 5.1 Importance of ELISA 5.2 ELISA data analysis 5.3 1. Curve fitting model: 5.4 2. Endpoint titer method 5.5 Apply the fitting sigmoid model and endpoint titer function in our dataset 5.6 Create function of Fitted model and endpoint titer, where the output of the fitted model data will be the input of the endpoint titer 5.7 ELISA data processing", " Chapter 5 Enzyme-linked immunosorbest assay (ELISA) ELISA is a standard molecular biology assay for detecting and quantifying a variety of compounds, including peptides, proteins, and antibodies in a sample. The sample could be serum, plasma, or bronchoalveolar lavage fluid (BALF). 5.1 Importance of ELISA An antigen-specific reaction in the host results in the production of antibodies, which are proteins found in the blood. In the event of an infectious disease, it aids in the detection of antibodies in the body. ELISA is distinguishable from other antibody-assays in that it produces quantifiable findings and separates non-specific from specific interactions by serial binding to solid surfaces, which is often a polystyrene multi-well plate. In IMPAc-TB project, it is crucial to evaluate the if the vaccine is eliciting humoral immunity and generating antibodies against vaccine antigen. ELISA will be used to determine the presence of Immunoglobulin (Ig) IgG, IgA, and IgM in the serum different time points post-vaccination. 5.1.1 Principle of ELISA ELISA is based on the principle of antigen-antibody interaction. An antigen must be immobilized on a solid surface and then complexed with an enzyme-linked antibody in an ELISA. The conjugated enzyme’s activity is evaluated by incubating it with a substrate to yield a quantifiable result, which enables detection. There are four basic steps of ELISA: 1. Coating multiwell plate with antigen/antibody: This step depends on what we want to detect the sample. If we need to evaluate the the presence of antibody, the plate will be coated with the antigen, and vice versa. To coat the plate, a fixed concentration of antigen (protein) is added to a 96 well high-binding plate (charged plate). Plate is incubated over night with the antigen at 4 degree celsius (as proteins are temperature sensitive) so that antigens are completely bound to the well. 2. Blocking: It is possible that not each and every site of the well is coated with the targeted antigen, and there could be uncovered areas. It is important to block those empty spaces so that primary antibody (which we will add to the next step) binds to these spaces and give us false positive results. For this, microplate well surface-binding sites are blocked with an unrelated protein or other substance.Most common blocking agents are bovine serum albumin, skim milk, and casein. One of the best blocking agents is to use the serum from the organism in which your secondary (detection antibody) is raised. For example, if the secondary antibody is raised in goat, then we can use goat serum as a blocking agent. 3. Probing: Probing is the step where we add sample containing antibodies that we want to detect. This will be the primary antibody. If the antibodies against the antigen (which we have coated) are present in the sample, it will bind to the antigen with high affinity. 4. Washing: After the incubation of sample containing primary antibody, the wells are washed so that any unbound antibody is washed away. Washing solution contains phosphate buffer saline + 0.05% tween-20 (a mild detergent). 0.05% tween-20 washes away all the non-specific interactions as those are not strong, but keeps all the specific interaction as those are strong and cannot be detached with mild detergent. 5. Detection: To detect the presence of antibody-antigen complex, a secondary antibody labelled with an enzyme (usually horseradish peroxidase) is added to the wells, incubated and washed. 6. Signal Measurement: Finally to detect “if” and “how much” of the antibody is present, a chromogenic substrate (like 3,3’,5,5’-Tetramethylbenzidine) is added to the wells, which can be cleaved the the enzyme that is tagged to the secondary antibody. The color compund is formed after the addition of the substrate, which is directly proportional to the amount of antibody present in the sample. The plate is read on a plate reader, where color is converted to numbers. Figure 5.1: A caption 5.1.2 Loading libraries library(readxl) library(tidyverse) library(minpack.lm) library(broom) library(purrr) library(ggbeeswarm) 5.2 ELISA data analysis Analysis of ELISA data is the most important part of the ELISA experiment. ELISA data can be analyzed in different ways based on how the data is acquired. There are a a few examples of the type of ELISA data : 1. With standard curve: ELISA can be used to determine the concentrations of the antigen and antibody. This type of ELISA data usually have a standard curve with different concentrations of the known analyte and the concentration in the sample is determined by extrapolating the unknown values in the curve. This type of assay is straightforward, easy to interpret and are more robust. 2. Without standard curve: Usually vaccine studies involve investigating the presence of high-affinity (and novel) antibodies against the vaccine antigens. Therefore, plotting a standard curve is not feasible as there is no previous information available for antibody concentration or type of antibody. Also, because antibody response to a vaccine will differ depending on the individual, it is not practical to generate a calibration curve from which absolute concentrations can be extrapolated. For this type of ELISA, quantification of the antibody titers is performed using serial dilutions of the test samples, and analysis can be performed using the following three methods (Hartman et al. 2018): Fitting sigmoid model Endpoint titer method 3: Absorbance summation method Let’s have a look at these methods, how we can apply these methods in our data, and R-based packages that we can utilize to perform this analysis. 5.3 1. Curve fitting model: The curve in ELISA data represents a plot of known concentrations versus their corresponding signal responses. The typical range of these calibration curves is one to two orders of magnitude on the response axis and two or more orders of magnitude on the concentration axis. The real curve of each assay could be easily identified if an infinite number of concentration dilutions with an infinite number of repetitions could be tested. The correct curve must be approximated from a relatively small number of noisy points, though, because there are a finite number of dilutions that may be performed. To estimate the dose-response relationship between standard dilutions, a method of interpolating between standards is required because there cannot be a standard at every concentration. This process is typically performed using a mathematical function or regression to approximate the true shape of the curve. A curve model is the name given to this approximating function, which commonly uses two or more parameters to describe a family of curves, and are then adjusted in order to find the curve from the family of curves that best fits the assay data. Three qualities should be included in a good curve fitting model. 1. The true curve’s shape must be accurately approximated by the curve model. If the curve model does not accomplish this, there is no way to adjust for this component of the total error that results from a lack of fit. 2. In order to get concentration estimates with minimal inaccuracy, a decent curve model must be able to average away as much of the random variation as is practical. 3. A successful curve model must be capable of accurately predicting concentration values for points between the anchor points of the standard dilutions. 5.3.1 How do we perform curve fitting model There are two major steps in performing curve fitting model for non-linear data like ELISA: 1. Finding the initial starting estimates of the parameters 2. locating the optimal solution in a region of the initial estimates We have presented an example below where we have performed a 8-10 point serial dilution of our sample and fitted a 4 parameter curve model. 5.3.2 An example of the curve fitting model 5.3.2.1 Read in the data This information comes from the 2018 study conducted by Hartman et al. Hartman et al. analyzed the ELISA data in their study utilizing fitted sigmoid analysis, end point titer, and absorbance summation. We utilized this information to determine whether our formulas and calculations provide the same outcomes and values as theirs. elisa_example_data &lt;- read_excel(&quot;DATA/example_elisa_data.xlsx&quot;) 5.3.2.2 Tidying the data We next performed tidying the data and make it in a format so that we can plot a sigmoid curve with that. # Divide dilution column into two seoparate columns elisa_example_data &lt;- separate(elisa_example_data, col = &quot;dilution&quot;, into = c(&quot;numerator&quot;, &quot;denominator&quot;), sep = &quot;\\\\/&quot;) # Convert the tabke from character to numeric elisa_example_data &lt;- elisa_example_data %&gt;% mutate_if(is.character, as.numeric) elisa_example_data$dilution &lt;- elisa_example_data$numerator/elisa_example_data$denominator elisa_example_data &lt;- elisa_example_data %&gt;% mutate(log_dilution = log(dilution, base = 3)) head(elisa_example_data) ## # A tibble: 6 × 5 ## numerator denominator absorbance dilution log_dilution ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 30 4 0.0333 -3.10 ## 2 1 90 3.73 0.0111 -4.10 ## 3 1 270 2.34 0.00370 -5.10 ## 4 1 810 1.1 0.00123 -6.10 ## 5 1 2430 0.51 0.000412 -7.10 ## 6 1 7290 0.22 0.000137 -8.10 5.3.2.3 Create function for curve fitting model We next created the curve fitting model function by using nlsLM function from “minpack.lm” package. The purpose of nlslm is to minimize the sum square of the vector returned by the function fn, by a modification of the Levenberg-Marquardt algorithm. In the early 1960s, the Levenberg-Marquardt algorithm was developed to address nonlinear least squares problems. Through a series of well-chosen updates to model parameter values, Levenberg-Marquardt algorithm lower the sum of the squares of the errors between the model function and the data points. mod_1 &lt;- nlsLM(absorbance ~ ((a-d)/(1+(log_dilution/c)^b)) + d, data = elisa_example_data, start = list (a = 4, d= 0, c = -5, b = 1)) # a = maximum absorbance # d = minimum absobance # c = point of maximum growth # b = slope at c mod_1 ## Nonlinear regression model ## model: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## data: elisa_example_data ## a d c b ## 4.12406 0.04532 -5.31056 7.62972 ## residual sum-of-squares: 0.02221 ## ## Number of iterations to convergence: 9 ## Achieved convergence tolerance: 1.49e-08 summary(mod_1) ## ## Formula: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## a 4.12406 0.05820 70.860 1.75e-12 *** ## d 0.04532 0.02268 1.998 0.0808 . ## c -5.31056 0.03933 -135.037 1.01e-14 *** ## b 7.62972 0.35854 21.280 2.50e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05269 on 8 degrees of freedom ## ## Number of iterations to convergence: 9 ## Achieved convergence tolerance: 1.49e-08 5.3.2.4 Apply the function to the data tidy_params &lt;- mod_1 %&gt;% tidy() a &lt;- tidy_params$estimate[tidy_params$term == &quot;a&quot;] b &lt;- tidy_params$estimate[tidy_params$term == &quot;b&quot;] c &lt;- tidy_params$estimate[tidy_params$term == &quot;c&quot;] d &lt;- tidy_params$estimate[tidy_params$term == &quot;d&quot;] elisa_example_data &lt;- elisa_example_data %&gt;% mutate(fitted = predict(mod_1)) elisa_example_data &lt;- elisa_example_data %&gt;% mutate(fitted = predict(mod_1)) 5.3.2.5 Plot the sigmoid curve with fitted sigmoid model elisa_example_data %&gt;% ggplot(aes(x = log_dilution, y = absorbance)) + geom_point() + geom_line(aes(y=fitted), color = &quot;dodgerblue&quot;) 5.4 2. Endpoint titer method The endpoint titer approach chooses an absorbance value just above the background noise (or the lower asymptotic level). The highest dilution with an absorbance greater than this predetermined value is the endpoint titer. This method is based on the assumption that a sample with a higher protein concentration will require a higher dilution factor to achieve an absorbance just above the level of background noise. 5.4.1 Create an endpoint titer function and apply it to the output of the fitted sigmoid model values. endpoint_titer &lt;- c * (((a - d) / (0.2 - d)) - 1) ^ (1 / b) summary(endpoint_titer) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -8.113 -8.113 -8.113 -8.113 -8.113 -8.113 endpoint_titer ## [1] -8.113285 5.4.2 Other methods to analyze ELISA data 5.4.2.1 Absorption summation 5.4.2.2 Area under the curve In this model of data analysis, we sum all the absorbance values from each sample to obtain one value. This value is termed as absorption summation (AS). Using the above data, the AS will be calculated as below: AS = 0.04 + 0.04 + 0.05 + 0.05 + 0.06 + 0.1 + 0.22 + 0.51 + 1.1 + 2.34 + 3.73 + 4.0 AS ## [1] 12.24 5.5 Apply the fitting sigmoid model and endpoint titer function in our dataset The presented data is from a mouse study. In this data, presence of IgG antibody has been evaluated against receptor binding domain (RBD) of SARS-CoV-2 virus in two different groups of mice. We need to elucidate which group has higher concentration of the antibodies. 5.5.0.1 Read in the data elisa_data &lt;- read_excel(&quot;DATA/elisa_data_serial_dilution.xlsx&quot;) 5.5.0.2 Tidy the data elisa_data &lt;- pivot_longer(data = elisa_data, cols = &quot;Mouse_1&quot;:&quot;Mouse_5&quot;, names_to = &quot;mouse_id&quot;, values_to = &quot;absorbance&quot;) head(elisa_data) ## # A tibble: 6 × 4 ## Groups Dilution mouse_id absorbance ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Group 1 1/50 Mouse_1 4.1 ## 2 Group 1 1/50 Mouse_2 3.9 ## 3 Group 1 1/50 Mouse_3 4.3 ## 4 Group 1 1/50 Mouse_4 4.2 ## 5 Group 1 1/50 Mouse_5 4 ## 6 Group 1 1/100 Mouse_1 3.9 # separate dilution column and convert it to log2 elisa_data &lt;- separate(elisa_data, col = &quot;Dilution&quot;, into = c(&quot;numerator&quot;, &quot;denomenator&quot;), sep = &quot;\\\\/&quot;) elisa_data &lt;- elisa_data %&gt;% transform(numerator = as.numeric(numerator), denomenator = as.numeric(denomenator)) elisa_data &lt;- elisa_data %&gt;% mutate(dilution = elisa_data$numerator/elisa_data$denomenator) elisa_data &lt;- elisa_data %&gt;% mutate(log_dilution = log2(dilution)) head(elisa_data) ## Groups numerator denomenator mouse_id absorbance dilution log_dilution ## 1 Group 1 1 50 Mouse_1 4.1 0.02 -5.643856 ## 2 Group 1 1 50 Mouse_2 3.9 0.02 -5.643856 ## 3 Group 1 1 50 Mouse_3 4.3 0.02 -5.643856 ## 4 Group 1 1 50 Mouse_4 4.2 0.02 -5.643856 ## 5 Group 1 1 50 Mouse_5 4.0 0.02 -5.643856 ## 6 Group 1 1 100 Mouse_1 3.9 0.01 -6.643856 5.5.0.2.1 converting data into dataframe elisa_data_df &lt;- elisa_data %&gt;% group_by(Groups, mouse_id) %&gt;% summarize(log_dilution = log_dilution, absorbance = absorbance) ## `summarise()` has grouped output by &#39;Groups&#39;, &#39;mouse_id&#39;. You can override using ## the `.groups` argument. elisa_data_nested &lt;- elisa_data %&gt;% group_by(Groups, mouse_id) %&gt;% nest() head(elisa_data_nested) ## # A tibble: 6 × 3 ## # Groups: Groups, mouse_id [6] ## Groups mouse_id data ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 Group 1 Mouse_1 &lt;tibble [10 × 5]&gt; ## 2 Group 1 Mouse_2 &lt;tibble [10 × 5]&gt; ## 3 Group 1 Mouse_3 &lt;tibble [10 × 5]&gt; ## 4 Group 1 Mouse_4 &lt;tibble [10 × 5]&gt; ## 5 Group 1 Mouse_5 &lt;tibble [10 × 5]&gt; ## 6 Group 2 Mouse_1 &lt;tibble [10 × 5]&gt; 5.5.0.2.2 plot the curves to evaluate the a, d, c, and b elisa_data %&gt;% ggplot(aes(x = log_dilution, y = absorbance)) + geom_point() + geom_line() + facet_wrap(Groups ~ mouse_id) Based on the curve, the values are: a = 4, d = 0 c = 2 b = 1 5.5.1 Creating a function for fitting model fitted_model_elisa &lt;- function(df_elisa, start_a, start_d, start_c, start_b) { mod_1 &lt;- nlsLM(absorbance ~ ((a-d)/(1+(log_dilution/c)^b)) + d, data = df_elisa, start = list(a = start_a, d = start_d, c = start_c, b = start_b)) return(mod_1) } 5.5.1.1 Fitting the model into the dataset fitted_model_elisa(elisa_data_nested$data[[1]], start_a = 4, start_d = 0, start_c = -8, start_b = 1) ## Nonlinear regression model ## model: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## data: df_elisa ## a d c b ## 4.3070 -0.6009 -10.2577 5.2893 ## residual sum-of-squares: 0.1199 ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 1.49e-08 5.5.1.2 Apply the fitted model function to the whole dataframe elisa_fitted_data &lt;- elisa_data_nested %&gt;% mutate(fitted_data = purrr::map(data, ~ fitted_model_elisa(.x,start_a = 4, start_d = 0, start_c = -8, start_b = 1))) head(elisa_fitted_data) ## # A tibble: 6 × 4 ## # Groups: Groups, mouse_id [6] ## Groups mouse_id data fitted_data ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 Group 1 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 2 Group 1 Mouse_2 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 3 Group 1 Mouse_3 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 4 Group 1 Mouse_4 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 5 Group 1 Mouse_5 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 6 Group 2 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; 5.5.1.3 Take out the summary of the data elisa_fitted_data_summary &lt;- elisa_fitted_data %&gt;% mutate(elisa_fitted_data_summary = purrr::map(fitted_data, broom::glance)) unnested &lt;- elisa_fitted_data_summary %&gt;% unnest(elisa_fitted_data_summary) %&gt;% ungroup() %&gt;% dplyr::select(Groups, mouse_id, fitted_data) unnested$fitted_data[[1]] ## Nonlinear regression model ## model: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## data: df_elisa ## a d c b ## 4.3070 -0.6009 -10.2577 5.2893 ## residual sum-of-squares: 0.1199 ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 1.49e-08 5.6 Create function of Fitted model and endpoint titer, where the output of the fitted model data will be the input of the endpoint titer # Fitted model function fitted_model_elisa &lt;- function(df_elisa, start_a, start_d, start_c, start_b) { mod_1 &lt;- nlsLM(absorbance ~ ((a-d)/(1+(log_dilution/c)^b)) + d, data = df_elisa, start = list(a = start_a, d = start_d, c = start_c, b = start_b)) return(mod_1) } # Endpoint titer function endpoint_titer_elisa &lt;- function(fitted_data, back_value) { tidy_fitted &lt;- broom::tidy(fitted_data) est_a &lt;- tidy_fitted$estimate[tidy_fitted$term == &quot;a&quot;] est_b &lt;- tidy_fitted$estimate[tidy_fitted$term == &quot;b&quot;] est_c &lt;- tidy_fitted$estimate[tidy_fitted$term == &quot;c&quot;] est_d &lt;- tidy_fitted$estimate[tidy_fitted$term == &quot;d&quot;] endpoint_titer &lt;- est_c * (((est_a - est_d) / (back_value - est_d)) - 1) ^ (1 / est_b) return(endpoint_titer) } 5.6.0.1 Apply the fitted model fuction into the nested data and use the output of the fitted data as the input for endpoint titer value evaluation 5.6.0.1.1 Run fitted model on the data elisa_data_with_fit_model &lt;- elisa_data_nested %&gt;% mutate(fitted_data = purrr::map(data, ~ fitted_model_elisa(.x, start_a = 4, start_d = 0, start_c = -8, start_b = 1))) head(elisa_data_with_fit_model) ## # A tibble: 6 × 4 ## # Groups: Groups, mouse_id [6] ## Groups mouse_id data fitted_data ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 Group 1 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 2 Group 1 Mouse_2 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 3 Group 1 Mouse_3 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 4 Group 1 Mouse_4 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 5 Group 1 Mouse_5 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 6 Group 2 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; 5.6.0.1.2 Taking output of the fitted model function and into endpoint titer function elisa_data_with_endpoint_titer &lt;- elisa_data_with_fit_model %&gt;% mutate(endpoint_data = purrr::map(fitted_data, ~ endpoint_titer_elisa(.x, back_value = 0.2))) 5.6.0.2 Plot the endpoint titer data for the two groups elisa_data_with_endpoint_titer$endpoint_data= as.numeric(elisa_data_with_endpoint_titer$endpoint_data) elisa_data_with_endpoint_titer %&gt;% ggplot(aes(x = Groups, y = endpoint_data, color = Groups)) + geom_beeswarm(cex = 3) 5.6.0.3 Perform statistical analysis on the data elisa_data_stats &lt;- t.test(endpoint_data ~ Groups, data = elisa_data_with_endpoint_titer) elisa_data_stats %&gt;% tidy() ## # A tibble: 1 × 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.800 -14.0 -13.2 -18.8 0.00000268 5.63 -0.906 -0.695 ## # … with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; 5.6.0.4 Statistical data analysis for more than two groups 5.7 ELISA data processing We read ELISA plate in a 96 well plate using a plate reader. The plate reader generates the data in form of number in an excel sheet. We have created this pipeline/worksheet to bring out the information from the excl sheet to a tidy format in which the above created fitted model and endpoint titer functions can be applied. 5.7.0.1 Read in the first dataset Below is the example ELISA data that has came straight out of the plate reader. This data is arranged in a 96-well plate format and contains Optical Density (OD) values. elisa_raw_data &lt;- read_excel(&quot;DATA/elisa_s1_07-25-20.xlsx&quot;, sheet = &quot;S1&quot;, col_names = FALSE, range = &quot;B2:M9&quot;) ## New names: ## * `` -&gt; ...1 ## * `` -&gt; ...2 ## * `` -&gt; ...3 ## * `` -&gt; ...4 ## * `` -&gt; ...5 ## * ... head(elisa_raw_data) ## # A tibble: 6 × 12 ## ...1 ...2 ...3 ...4 ...5 ...6 ...7 ...8 ...9 ...10 ...11 ...12 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.199999999… 0.05 0.069 6.3E… 0.061 0.122 0.16… 0.145 0.135 6.80… 0.053 0.05 ## 2 7.900000000… 0.098 0.069 6.80… 0.115 0.202 5.89… 0.134 0.069 0.106 0.05 0.075 ## 3 8.899999999… 0.133 0.119 OVRF… 3.87 2.32 OVRF… 3.85 2.12 OVRF… 3.21 1.02 ## 4 OVRFLW 3.46 1.16 OVRF… 3.80 2.36 OVRF… 3.70 1.49 OVRF… 3.68 1.63 ## 5 3.815999999… 1.82 0.446 3.89… 3.42 1.13 OVRF… 2.33 0.608 OVRF… 3.41 1.10 ## 6 OVRFLW 3.69 1.43 OVRF… 3.66 1.27 3.839 1.74 0.444 2.49… 0.637 0.704 5.7.0.2 Tidy dataset 1 It is important to clean the data and arrange it in a format on which we can apply formulas and functions. # Convert all columns to numeric elisa_raw_data_numeric &lt;- elisa_raw_data %&gt;% mutate_if(is.character, as.numeric) ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion # pivot longer the data elisa_raw_data_tidy &lt;- pivot_longer(data = elisa_raw_data_numeric, cols = &quot;...1&quot;:&quot;...12&quot;, names_to = &quot;well_id&quot;, values_to = &quot;od_450nm&quot;) # remove &quot;...&quot; from the first column elisa_raw_data_tidy$well_id &lt;- str_replace(elisa_raw_data_tidy$well_id, &quot;...&quot;, &quot;&quot;) # Add new column to the data_frame elisa_raw_data_tidy_new &lt;- elisa_raw_data_tidy %&gt;% mutate(name = rep(LETTERS[1:8], each = 12)) elisa_raw_data_tidy_new &lt;- elisa_raw_data_tidy_new %&gt;% mutate(well_id = paste0(name, well_id)) %&gt;% select(-name) head(elisa_raw_data_tidy_new) ## # A tibble: 6 × 2 ## well_id od_450nm ## &lt;chr&gt; &lt;dbl&gt; ## 1 A1 0.052 ## 2 A2 0.05 ## 3 A3 0.069 ## 4 A4 0.063 ## 5 A5 0.061 ## 6 A6 0.122 5.7.0.3 Read in the second data set The second dataset contains the information such as groups, mouse id, and dilutions for the respective wells of the 96 well plate for the dataset-1. elisa_label_data &lt;- read_excel(&quot;DATA/elisa_s1_07-25-20.xlsx&quot;, sheet = &quot;S1&quot;, col_names = FALSE, range = &quot;Q2:AB9&quot;) ## New names: ## * `` -&gt; ...1 ## * `` -&gt; ...2 ## * `` -&gt; ...3 ## * `` -&gt; ...4 ## * `` -&gt; ...5 ## * ... head(elisa_label_data) ## # A tibble: 6 × 12 ## ...1 ...2 ...3 ...4 ...5 ...6 ...7 ...8 ...9 ...10 ...11 ...12 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 blank secon… naïv… 1A-1… 1A-1… 1A-1… 1A-2… 1A-2… 1A-2… 1A-3… 1A-3… 1A-3… ## 2 1A-4 (1/250 1A-4 … 1A-4… 1B-1… 1B-1… 1B-1… 1B-2… 1B-2… 1B-2… 1B-3… 1B-3… 1B-3… ## 3 1B-4 (1/250 1B-4 … 1B-4… 2A-1… 2A-1… 2A-1… 2A-2… 2A-2… 2A-2… 2A-3… 2A-3… 2A-3… ## 4 2B-1 (1/250 2B-1 … 2B-1… 2B-2… 2B-2… 2B-2… 2B-3… 2B-3… 2B-3… 2B-4… 2B-4… 2B-4… ## 5 3A-1 (1/250 3A-1 … 3A-1… 3A-2… 3A-2… 3A-2… 3A-3… 3A-3… 3A-3… 3A-4… 3A-4… 3A-4… ## 6 3B-1 (1/250 3B-1 … 3B-1… 3B-2… 3B-2… 3B-2… 3B-3… 3B-3… 3B-3… 3B-4… 3B-4… 3B-4… 5.7.0.4 Tidy dataset-2 # pivot longer the data elisa_label_data_tidy &lt;- pivot_longer(data = elisa_label_data, cols = &quot;...1&quot;:&quot;...12&quot;, names_to = &quot;well_id&quot;, values_to = &quot;information&quot;) # remove &quot;...&quot; from the first column elisa_label_data_tidy$well_id &lt;- str_replace(elisa_label_data_tidy$well_id, &quot;...&quot;, &quot;&quot;) # Add new column to the data_frame elisa_label_data_tidy_new &lt;- elisa_label_data_tidy %&gt;% mutate(name = rep(LETTERS[1:8], each = 12)) elisa_label_data_tidy_new &lt;- elisa_label_data_tidy_new %&gt;% mutate(well_id = paste0(name, well_id)) %&gt;% select(-name) head(elisa_label_data_tidy_new) ## # A tibble: 6 × 2 ## well_id information ## &lt;chr&gt; &lt;chr&gt; ## 1 A1 blank ## 2 A2 secondary ## 3 A3 naïve (1/250) ## 4 A4 1A-1 (1/250 ## 5 A5 1A-1 (1/1250 ## 6 A6 1A-1 (1/6250 5.7.0.5 Merge dataset-1 (with OD information) with dataset-2 (with respective data information) To create a complete full dataset with Groups, mouse-id, dilutions, and OD, we merged the dataset-1 and dataset-2 together. We also cleaned the data set so that mouse-ID and dilution columns are separate and have their own columns. #Merge the two datasets elisa_data = elisa_raw_data_tidy_new %&gt;% inner_join(elisa_label_data_tidy_new, by=&quot;well_id&quot;) head(elisa_data) ## # A tibble: 6 × 3 ## well_id od_450nm information ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 A1 0.052 blank ## 2 A2 0.05 secondary ## 3 A3 0.069 naïve (1/250) ## 4 A4 0.063 1A-1 (1/250 ## 5 A5 0.061 1A-1 (1/1250 ## 6 A6 0.122 1A-1 (1/6250 ### Separate the information table into sample ID and dilution columns tidy_elisa_data &lt;- separate(elisa_data, col = &quot;information&quot;, into = c(&quot;sample_id&quot;, &quot;dilution&quot;), sep = &quot;\\\\(&quot;) ## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 2 rows [1, 2]. head(tidy_elisa_data) ## # A tibble: 6 × 4 ## well_id od_450nm sample_id dilution ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 A1 0.052 &quot;blank&quot; &lt;NA&gt; ## 2 A2 0.05 &quot;secondary&quot; &lt;NA&gt; ## 3 A3 0.069 &quot;naïve &quot; 1/250) ## 4 A4 0.063 &quot;1A-1 &quot; 1/250 ## 5 A5 0.061 &quot;1A-1 &quot; 1/1250 ## 6 A6 0.122 &quot;1A-1 &quot; 1/6250 tidy_elisa_data &lt;- tidy_elisa_data %&gt;% mutate(dilution = str_extract(dilution, &quot;(/)[0-9]+&quot;), dilution = str_replace(dilution, &quot;/&quot;, &quot;&quot;), dilution = as.numeric(dilution)) tidy_elisa_data &lt;- tidy_elisa_data %&gt;% select(well_id, sample_id, dilution, od_450nm) head(tidy_elisa_data) ## # A tibble: 6 × 4 ## well_id sample_id dilution od_450nm ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A1 &quot;blank&quot; NA 0.052 ## 2 A2 &quot;secondary&quot; NA 0.05 ## 3 A3 &quot;naïve &quot; 250 0.069 ## 4 A4 &quot;1A-1 &quot; 250 0.063 ## 5 A5 &quot;1A-1 &quot; 1250 0.061 ## 6 A6 &quot;1A-1 &quot; 6250 0.122 References "],["flow-cytometry.html", "Chapter 6 Flow cytometry 6.1 Loading packages 6.2 panel information 6.3 Loading data 6.4 Making the data tidy for plotting 6.5 boxplot", " Chapter 6 Flow cytometry Flow cytometry data can be quantified in many different ways and with different techniques. For the purpose of these data analyses, manual gating has been achieved in FlowJo and cell frequencies and populations exported as a .csv file. This .csv file is the primary input for this R pipeline which aims to output box plots for each gated cell population. This example data set is from an innate response study whcih investigated the immune response in the lungs during the first 28 days of infection. 6.1 Loading packages library(readxl) library(ggplot2) library(RColorBrewer) library(dplyr) library(tidyverse) library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor library(stringr) library(tidyr) library(knitr) library(forcats) library(broom) library(ggfortify) library(stats) library(ggpubr) library(grDevices) library(rstatix) ## ## Attaching package: &#39;rstatix&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter library(writexl) 6.2 panel information # antibody_panel &lt;- read_excel 6.3 Loading data Df &lt;- read_excel(&quot;DATA/innate_normalizedto45.xlsx&quot;, sheet = &quot;CD3CD11b No Day 14&quot;) marker_legend &lt;- read_excel(&quot;DATA/marker legend.xlsx&quot;) # Remove Freq of Parent columns Df1 &lt;- Df %&gt;% select(-matches(&quot;Parent&quot;)) # Remove &quot;Leukocytes/LIVE/Single Cells/&quot; from col names names(Df1) &lt;- str_remove(names(Df1), &quot;Leukocytes/LIVE/Single Cells/&quot;) Df1 &lt;- Df1 %&gt;% rename_all(funs(str_replace(., &quot;\\\\|.+&quot;, &quot;&quot;)))# Remove &quot;|Freq of...&quot; from col names ## Warning: `funs()` was deprecated in dplyr 0.8.0. ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. Df1 &lt;- Df1 %&gt;% rename_all(funs(str_replace_all(., &quot;\\\\/Q[:digit:]+\\\\:&quot;, &quot;&quot;))) %&gt;% rename_all(funs(str_replace(., &quot;\\\\/&quot;, &quot; &quot;))) %&gt;% rename_all(funs(str_replace(., &quot;\\\\,&quot;, &quot; &quot;))) %&gt;% rename_all(funs(str_replace(., &quot;\\\\ \\\\,&quot;, &quot; &quot;))) # str_extract_all(names(Df1), &quot;[:alpha:]+[:digit:]+[\\\\+\\\\-]&quot;) # # # # # # # marker_select &lt;- function(col_title) { # marker_df &lt;- str_detect(names(DATA1), &quot;[\\\\+\\\\-]&quot;) # return(marker_df) # } 6.4 Making the data tidy for plotting tidy_Df1 &lt;- pivot_longer(data = Df1, cols = starts_with(&quot;CD45+&quot;), names_to = &quot;cell_types&quot;, values_to = &quot;percentage_of_CD45&quot;) tidy_Df1 &lt;- tidy_Df1 %&gt;% separate(col = &quot;SAMPLE&quot;, into = c(&quot;day&quot;, &quot;replicate&quot;)) tidy_Df1 %&gt;% select(cell_types) %&gt;% unique() ## # A tibble: 128 × 1 ## cell_types ## &lt;chr&gt; ## 1 &quot;CD45+ &quot; ## 2 &quot;CD45+ CD3- CD11b+ &quot; ## 3 &quot;CD45+ CD3- CD11b+ CD25+ &quot; ## 4 &quot;CD45+ CD3- CD11b+ CD103+ &quot; ## 5 &quot;CD45+ CD3- CD11b+ gamma_delta &quot; ## 6 &quot;CD45+ CD3- CD11b+ NKp46+ &quot; ## 7 &quot;CD45+ CD3- CD11b+ CD11c+ CD64- &quot; ## 8 &quot;CD45+ CD3- CD11b+ CD11c- CD64- &quot; ## 9 &quot;CD45+ CD3- CD11b+ CD86- CD64+ &quot; ## 10 &quot;CD45+ CD3- CD11b+ CD86+ CD64+ &quot; ## # … with 118 more rows tidy_Df1 &lt;- tidy_Df1 %&gt;% filter(percentage_of_CD45 &gt; 0.005) head(tidy_Df1, n=10) ## # A tibble: 10 × 4 ## day replicate cell_types percentage_of_CD45 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 CNT 1 &quot;CD45+ &quot; 82.9 ## 2 CNT 1 &quot;CD45+ CD3- CD11b+ &quot; 29.3 ## 3 CNT 1 &quot;CD45+ CD3- CD11b+ CD25+ &quot; 0.88 ## 4 CNT 1 &quot;CD45+ CD3- CD11b+ CD103+ &quot; 0.75 ## 5 CNT 1 &quot;CD45+ CD3- CD11b+ gamma_delta &quot; 4.77 ## 6 CNT 1 &quot;CD45+ CD3- CD11b+ NKp46+ &quot; 7.3 ## 7 CNT 1 &quot;CD45+ CD3- CD11b+ CD11c+ CD64- &quot; 3.65 ## 8 CNT 1 &quot;CD45+ CD3- CD11b+ CD11c- CD64- &quot; 24.3 ## 9 CNT 1 &quot;CD45+ CD3- CD11b+ CD86- CD64+ &quot; 0.43 ## 10 CNT 1 &quot;CD45+ CD3- CD11b+ CD86+ CD64+ &quot; 0.85 # Select CD3 &amp; CD11b populations and create new data frames CD3pos_CD11bneg &lt;- tidy_Df1 %&gt;% filter(str_detect(cell_types, &quot;CD3\\\\+ + CD11b\\\\-&quot;)) CD3neg_CD11bpos &lt;- tidy_Df1 %&gt;% filter(str_detect(cell_types, &quot;CD3\\\\- + CD11b\\\\+&quot;)) CD3neg_CD11bneg &lt;- tidy_Df1 %&gt;% filter(str_detect(cell_types, &quot;CD3\\\\- + CD11b\\\\-&quot;)) 6.5 boxplot CD3pos_CD11bneg_bar_plot &lt;- CD3pos_CD11bneg %&gt;% mutate(day = fct_relevel(day, &quot;CNT&quot;, &quot;D3&quot;, &quot;D7&quot;, &quot;D28&quot;)) %&gt;% ggplot(aes(x = day, y = percentage_of_CD45, fill= day)) + stat_boxplot( aes(day, percentage_of_CD45), geom=&#39;errorbar&#39;, linetype=1, width=0.5)+ geom_boxplot(aes(day, percentage_of_CD45)) + facet_wrap(~cell_types, scale = &quot;free_y&quot;, labeller = label_wrap_gen(width=15), ncol = 5, nrow = 20) + theme_bw() + theme(axis.text.x = element_blank(), axis.text.y = element_text(size = 20), axis.title.x = element_text(size = 20, face = &quot;bold&quot;), axis.title.y = element_text(size = 20, face = &quot;bold&quot;), legend.text = element_text(size = 20), legend.title = element_text(size = 20), plot.title = element_text(color=&quot;black&quot;, size=30, face=&quot;bold&quot;)) + labs (y=&quot;Percentage of CD45&quot;, x = &quot;Day&quot;) + theme(strip.text = element_text(size=12, face = &quot;bold&quot;)) + theme(legend.position=&quot;bottom&quot;) + ggtitle(&quot;Changes in immune cell populations (lung) CD3+ CD11b-&quot;) + stat_compare_means(label = &quot;p.signif&quot;, method = &quot;t.test&quot;, ref.group = &quot;CNT&quot;) CD3neg_CD11bpos_bar_plot &lt;- CD3neg_CD11bpos %&gt;% mutate(day = fct_relevel(day, &quot;CNT&quot;, &quot;D3&quot;, &quot;D7&quot;, &quot;D28&quot;)) %&gt;% ggplot(aes(x = day, y = percentage_of_CD45, fill= day)) + stat_boxplot( aes(day, percentage_of_CD45), geom=&#39;errorbar&#39;, linetype=1, width=0.5)+ geom_boxplot( aes(day, percentage_of_CD45)) + facet_wrap(~cell_types, scale = &quot;free_y&quot;, labeller = label_wrap_gen(width=15), ncol = 5, nrow = 20) + theme_bw() + theme(axis.text.x = element_blank(), axis.text.y = element_text(size = 20), axis.title.x = element_text(size = 20, face = &quot;bold&quot;), axis.title.y = element_text(size = 20, face = &quot;bold&quot;), legend.text = element_text(size = 20), legend.title = element_text(size = 20), plot.title = element_text(color=&quot;black&quot;, size=30, face=&quot;bold&quot;)) + labs (y=&quot;Percentage of CD45&quot;, x = &quot;Day&quot;) + theme(strip.text = element_text(size=12, face = &quot;bold&quot;)) + theme(legend.position=&quot;bottom&quot;) + ggtitle(&quot;Changes in immune cell populations (lung) CD3- CD11b+&quot;) + stat_compare_means(label = &quot;p.signif&quot;, method = &quot;t.test&quot;, ref.group = &quot;CNT&quot;) CD3neg_CD11bneg_bar_plot &lt;- CD3neg_CD11bneg %&gt;% mutate(day = fct_relevel(day, &quot;CNT&quot;, &quot;D3&quot;, &quot;D7&quot;, &quot;D28&quot;)) %&gt;% ggplot(aes(x = day, y = percentage_of_CD45, fill= day)) + stat_boxplot( aes(day, percentage_of_CD45), geom=&#39;errorbar&#39;, linetype=1, width=0.5)+ geom_boxplot( aes(day, percentage_of_CD45)) + facet_wrap(~cell_types, scale = &quot;free_y&quot;, labeller = label_wrap_gen(width=15), ncol = 5, nrow = 20) + theme_bw() + theme(axis.text.x = element_blank(), axis.text.y = element_text(size = 20), axis.title.x = element_text(size = 20, face = &quot;bold&quot;), axis.title.y = element_text(size = 20, face = &quot;bold&quot;), legend.text = element_text(size = 20), legend.title = element_text(size = 20), plot.title = element_text(color=&quot;black&quot;, size=30, face=&quot;bold&quot;)) + labs (y=&quot;Percentage of CD45&quot;, x = &quot;Day&quot;) + theme(strip.text = element_text(size=12, face = &quot;bold&quot;)) + theme(legend.position=&quot;bottom&quot;) + ggtitle(&quot;Changes in immune cell populations (lung) CD3- CD11b-&quot;) + stat_compare_means(label = &quot;p.signif&quot;, method = &quot;t.test&quot;, ref.group = &quot;CNT&quot;) CD3pos_CD11bneg_bar_plot # CD3neg_CD11bpos_bar_plot # CD3neg_CD11bneg_bar_plot "],["pathology.html", "Chapter 7 Pathology", " Chapter 7 Pathology "],["proteomics.html", "Chapter 8 Proteomics", " Chapter 8 Proteomics For proteomics data, we will be getting data that have already been collected and pre-processed by another part of the team. The following shows an example of the type of data we will get as an input: library(tidyverse) prot_a &lt;- read_csv(&quot;DATA/Transition Results_CCTSI_A.csv&quot;) ## Rows: 3393 Columns: 18 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (7): Peptide, Protein, Replicate, Fragment Ion, Ratio Dot Product, Tota... ## dbl (11): Precursor Mz, Precursor Charge, Product Mz, Product Charge, Retent... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. prot_a ## # A tibble: 3,393 × 18 ## Peptide Protein Replicate `Precursor Mz` `Precursor Char…` `Product Mz` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 QELDEISTNIR Cfp10 091322_LT1 659. 2 1061. ## 2 QELDEISTNIR Cfp10 091322_LT2 659. 2 1061. ## 3 QELDEISTNIR Cfp10 091322_LT3 659. 2 1061. ## 4 QELDEISTNIR Cfp10 091322_LT4 659. 2 1061. ## 5 QELDEISTNIR Cfp10 091322_LT5 659. 2 1061. ## 6 QELDEISTNIR Cfp10 091322_LT6 659. 2 1061. ## 7 QELDEISTNIR Cfp10 091322_LT7 659. 2 1061. ## 8 QELDEISTNIR Cfp10 091322_LT8 659. 2 1061. ## 9 QELDEISTNIR Cfp10 091322_LT10 659. 2 1061. ## 10 QELDEISTNIR Cfp10 091322_LT11 659. 2 1061. ## # … with 3,383 more rows, and 12 more variables: `Product Charge` &lt;dbl&gt;, ## # `Fragment Ion` &lt;chr&gt;, `Retention Time` &lt;dbl&gt;, Area &lt;dbl&gt;, Background &lt;dbl&gt;, ## # `Peak Rank` &lt;dbl&gt;, `Ratio Dot Product` &lt;chr&gt;, ## # `Total Area Normalized` &lt;chr&gt;, `Total Area Ratio` &lt;chr&gt;, ## # `Library Dot Product` &lt;dbl&gt;, RatioLightToHeavy &lt;dbl&gt;, ## # DotProductLightToHeavy &lt;dbl&gt; These data include the following columns: Peptide: A short string of peptides that are being measured Protein: The protein that those peptides come from Replicate: An identifier for the sample that the measurement was taken on Precursor Mz, Precursor Charge, Product Mz, Product Charge, Fragment Ion, Retention Time: Measurements that help in identifying the peptide that is being measured (?) Area: Background: Peak Rank: Ratio Dot Product: Total Area Normalized: Total Area Ratio Library Dot Product: RatioLightToHeavy: DotProductLightToHeavy: [More about how these data were pre-processed. Softwarei: Skyline] Here are all the unique replicates in this file: prot_a %&gt;% pull(Replicate) %&gt;% unique() ## [1] &quot;091322_LT1&quot; &quot;091322_LT2&quot; &quot;091322_LT3&quot; &quot;091322_LT4&quot; &quot;091322_LT5&quot; ## [6] &quot;091322_LT6&quot; &quot;091322_LT7&quot; &quot;091322_LT8&quot; &quot;091322_LT10&quot; &quot;091322_LT11&quot; ## [11] &quot;091322_LT12&quot; &quot;091322_LT13&quot; &quot;091322_LT14&quot; &quot;091322_H1&quot; &quot;091322_H2&quot; ## [16] &quot;091322_H3&quot; &quot;091322_H4&quot; &quot;091322_H5&quot; &quot;091322_H6&quot; &quot;091322_H7&quot; ## [21] &quot;091322_H8&quot; &quot;091322_H9&quot; &quot;091322_H10&quot; &quot;091322_H11&quot; &quot;091322_H12&quot; ## [26] &quot;091322_H13&quot; &quot;091322_H14&quot; &quot;091322_TB1&quot; &quot;091322_TB2&quot; &quot;091322_TB3&quot; ## [31] &quot;091322_TB4&quot; &quot;091322_TB5&quot; &quot;091322_TB6&quot; &quot;091322_TB7&quot; &quot;091322_TB8&quot; ## [36] &quot;091322_TB9&quot; &quot;091322_TB10&quot; &quot;091322_TB11&quot; &quot;091322_TB12&quot; The three groups in this data are labeled with “LT,” “H,” and “TB” somewhere in the identifier. We can create a new column in the dataset that pulls out this treatment group information: prot_a &lt;- prot_a %&gt;% mutate(treatment_group = str_extract(Replicate, &quot;[A-Z]+&quot;)) prot_a %&gt;% filter(Peptide == first(Peptide)) %&gt;% group_by(treatment_group) %&gt;% count() ## # A tibble: 3 × 2 ## # Groups: treatment_group [3] ## treatment_group n ## &lt;chr&gt; &lt;int&gt; ## 1 H 140 ## 2 LT 130 ## 3 TB 120 prot_a %&gt;% filter(Peptide == first(Peptide) &amp; Replicate == first(Replicate)) ## # A tibble: 10 × 19 ## Peptide Protein Replicate `Precursor Mz` `Precursor Charge` `Product Mz` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 QELDEISTNIR Cfp10 091322_LT1 659. 2 1061. ## 2 QELDEISTNIR Cfp10 091322_LT1 659. 2 832. ## 3 QELDEISTNIR Cfp10 091322_LT1 659. 2 703. ## 4 QELDEISTNIR Cfp10 091322_LT1 659. 2 590. ## 5 QELDEISTNIR Cfp10 091322_LT1 659. 2 503. ## 6 QELDEISTNIR Cfp10 091322_LT1 664. 2 1071. ## 7 QELDEISTNIR Cfp10 091322_LT1 664. 2 842. ## 8 QELDEISTNIR Cfp10 091322_LT1 664. 2 713. ## 9 QELDEISTNIR Cfp10 091322_LT1 664. 2 600. ## 10 QELDEISTNIR Cfp10 091322_LT1 664. 2 513. ## # … with 13 more variables: `Product Charge` &lt;dbl&gt;, `Fragment Ion` &lt;chr&gt;, ## # `Retention Time` &lt;dbl&gt;, Area &lt;dbl&gt;, Background &lt;dbl&gt;, `Peak Rank` &lt;dbl&gt;, ## # `Ratio Dot Product` &lt;chr&gt;, `Total Area Normalized` &lt;chr&gt;, ## # `Total Area Ratio` &lt;chr&gt;, `Library Dot Product` &lt;dbl&gt;, ## # RatioLightToHeavy &lt;dbl&gt;, DotProductLightToHeavy &lt;dbl&gt;, ## # treatment_group &lt;chr&gt; prot_a %&gt;% pull(Protein) %&gt;% unique() ## [1] &quot;Cfp10&quot; &quot;acpM&quot; &quot;Ag85A&quot; ## [4] &quot;MtbH37Rv|Rv3841|BfrB&quot; &quot;MtbH37Rv|Rv1837c|GlcB&quot; &quot;MtbH37Rv|Rv3418c|GroES&quot; ## [7] &quot;MtbH37Rv|Rv3248c|SahH&quot; &quot;MtbH37Rv|Rv2031c|hspX&quot; Cfp10 acpM Ag85A MtbH37Rv|Rv3841|BfrB MtbH37Rv|Rv1837c|GlcB MtbH37Rv|Rv3418c|GroES MtbH37Rv|Rv3248c|SahH MtbH37Rv|Rv2031c|hspX "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
