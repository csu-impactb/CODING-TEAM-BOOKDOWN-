[["index.html", "Connect R to Microsoft Teams Chapter 1 Overview 1.1 About the project 1.2 About this book", " Connect R to Microsoft Teams Taru Dutt 2022-10-19 Chapter 1 Overview 1.1 About the project The objective of the Immune Mechanisms of Protection against Mycobacterium tuberculosis (IMPAc-TB) program is to get a thorough understanding of the immune responses necessary to avoid initial infection with Mycobacterium tuberculosis (Mtb), formation of latent infection, and progression to active TB illness. To achieve these goals, the National Institute of Allergy and Infectious Diseases awarded substantial funding and established multidisciplinary research teams that will analyze immune responses against Mtb in animal models (mice, guinea pigs, and non-human primates) and humans, as well as immune responses elicited by promising vaccine candidates. The contract awards establish and give up to seven years of assistance for IMPAc-TB Centers to explain the immune responses required for Mtb infection protection. The seven centers that are part of the study are (in alphabetical order): Colorado State University Harvard T.H. Chan School of Public Health Seattle Children Hospital [more] Colorado State University Team and role of each member: Dr. Marcela Henao-Tamayo: Principal Investigator Dr. Brendan Podell: Principal Investigator Dr. Andres Obregon-Henao: Research Scientist-III Dr. Taru S. Dutt: Research Scientist-I [more] 1.2 About this book The aim of this book is to provide data protocols and data collection templates for key types of data that are collected over the course of this project. By using standard templates to record data, as well as starting from defined pipelines to process and analyze the data, we aim to standardize the collection and processing of data across this project. Here, we have built a comprehensive guide to wet lab data collection, sample processing, and computational tool creation for robust and efficient data analysis and dissemination. "],["experimental-metadata.html", "Chapter 2 Experimental metadata", " Chapter 2 Experimental metadata Metadata for an experiment: species start_date end_date experimental_groups "],["initial-mouse-characteristics.html", "Chapter 3 Initial mouse characteristics", " Chapter 3 Initial mouse characteristics At the start of each experiment with a mouse model, we record several measurements or characteristics of each mouse. We record these measurements along with an identifier for each mouse (for example, based on tags or ear notches), so that we can later link the initial characteristics of each mouse with later measurements on the same mouse. The values that we initially record for each mouse include: group: An identifier for the experimental group to which the mouse is assigned (e.g., “Control,” “Group 1”) group_detail: A longer description of the mouse’s treatment group (e.g., “Vaccinated with vaccine candidate A at 4 and 8 weeks”) notch_id: The ear notch pattern of the mouse (e.g., “0” for no notch, “1R” for one notch in the right ear) mouse_number: A number that corresponds with the mouse’s ear notch patter; together with the mouse’s group number, this provides a unique identifier for each mouse in the experiment cage_number: The number of the cage to which the mouse is first assigned. This may change over the course of the experiment, as mice might be removed from a cage due to fighting, etc. Any of these later changes of cage will be recorded [where] sex: Whether the mouse is male (“m”) or female (“f”) age: Age strain: The strain of the mouse (e.g., “C57BL/6J” for Black 6, “C3HeB/FeJ” for Kramnik) We have created a spreadsheet template that can be used to record these data, which you can download by clicking here. This template currently includes example data (colored in blue to help you remember that it’s only there as an example). To Use this template, take a look at the example data, then delete it and replace with the real data for your experiment. Here is an example of how the first rows of this template might look once it’s filled out: This template should be used at the initial time when mice are brought into the experiment. The file format is an Excel file, so you can use it by saving it to your computer and then opening and recording data with Excel. Later code in this chapter will read in a file in this template format to provide basic summaries of the data. Later code will read in these files to record the data in a project-wide database, which will allow us to integrate it with other data collected over the course of the experiment. [Rules for naming the file. Include experiment name / study ID?] "],["animal-initial-conditions-and-weekly-weights.html", "Chapter 4 Animal initial conditions and weekly weights", " Chapter 4 Animal initial conditions and weekly weights 4.0.1 Overview We use the template in this section to record information about each animal used in the experiment. This includes the species, sex, and experimental group. It also includes some information to identify the animal, which in the case of mice includes a code describing the pattern of notches put in the mouse’s ear and the cage that the animal is assigned to at the beginning of the experiment. These are all values that can be determined at the start of the experiment, when the mice are first assigned to groups. This template is also used to record some data over the course of the experiment. This includes adverse events and cases where an animal is moved from one cage to another during the experiment. In addition, in our experiments, we are measuring the mice every week to record their weight over the course of the experiment. This weight measuring begins before the first vaccination and continues through until the last mouse is sacrificed. We have used ear notches to identify each mouse, and between the ear notch and the mouse’s cage number, we can uniquely track each mouse in the study. There are a few reasons that we are measuring these mouse weights. The first is to help us manage the mice, particularly in terms of animal welfare. If there are mice that are losing a lot of weight, that can be an indication that they may need to be euthanized. For example, some animal care standards consider that an adult animal that has lost 20% or more of its weight compared to its baseline weight is indicating a clear sign of morbidity or suffering. A second reason is that the weight measure might provide a record of each mouse’s general health over the course of the study. In the study, mice are weighed in grams weekly to monitor clinical status, as one potential sign of tuberculosis infection and severity is weight loss. In humans, tuberculosis patients frequently display weight loss as a clinical symptom associated with disease progression. In particular, extreme weight loss and loss of muscle mass, also known as cachexia, can present as a result of chronic inflammatory illnesses like tuberculosis (Baazim, Antonio-Herrera, and Bergthaler 2022). This cachexia is part of a systemic response to inflammation, and in humans has been linked to upregulation of pro-inflammatory cytokines including tumor necrosis factor, interleukin-6, and interferon-gamma (Baazim, Antonio-Herrera, and Bergthaler 2022). Additionally, studies support a role in cachexia of key immune cell populations such as cytotoxic T-cells which, when depleted, counteract muscle and fat deterioration (Baazim et al. 2019), suggest that thsi type of T-cells may metabolically reprogram adipose tissue. Given these relationships between weight loss, diseases, and immune processes, it is possible that mouse weight might provide a regularly measurable insight into the severity of disease in each animal. While many of data points are collected to measure the final disease state of each animal, fewer are available before the animal is sacrificed. We are hoping that mouse weights will provide one measure that, while it may not perfectly capture disease severity, may provide some information throughout the experiment that is correlated to disease severity at regular time intervals. Other studies that use a mouse model of tuberculosis have collected mouse weights, as well (Smith et al. 2022; Segueni et al. 2016). We plan to investigate these data to visualize the trajectory of weight gain / loss in each mouse both before and after they are challenged with tuberculosis. We also plan to test whether each mouse’s weight change after challenge is correlated with other metrics of the severity of disease and immune response. We will do this by testing the correlation between the percent change in weight between challenge and sacrifice with CFUs at sacrifice as well as expression of cytokines and other biological markers (Smith et al. 2022). 4.0.2 Template description Both the animals’ initial conditions and their weekly measures (adverse events, cage changes, and weights) should be recorded in an excel worksheet. You can download a copy of the template here. The worksheet is divided into sheets. The first sheet is recorded at the first time point when the mice are measured and is used to record information about the mice that will remain unchanged over the course of the study, like species and sex. Here is what the first sheet of the template looks like: The second and later sheets are used to record the weight at each measured timepoint. The second sheet will record the weights on the first date they are measured, so it should be recorded at the same time as the first sheet—with initial mouse information—is completed. Here is what the first sheet of the template looks like: As you continue to measure at new timepoints, you should add a sheet at each timepoint, with each new sheet following the format of the second sheet in the template. The second and later sheets should be labeled with the date when those weights were measured (e.g., “5.26.22” for weights measured on May 26, 2022). When you download the template, it will have example values filled out in blue. Use these to get an idea for how to record your own data. When you are ready to record your own data, delete these example values and replace them with data collected from your own experiment. Column titles are as follows. First, in the first sheet, you will record: notch_id: Record the ear notch pattern in the mouse. Make sure that you record consistently across all timepoints, so that each mouse can be tracked across dates. If you are doing single notches, for example, this might be “0” for no notches, “1R” for one notch in the right ear, “1L” for one notch in the left ear, and “1R1L” for one notch in each ear. starting_cage_number: Record the number of the cage that the mouse is put into at the start of the experiment. In combination with the mouse’s notch_id, this will provide a unique identifier for each mouse at the start of the experiment. dob: Record the date the mouse was born. species: Record the species of the mouse (e.g., “C57BL/6” for C57 black 6 mice or “CBA” for CBA mice). sex: Record as “m” for male or “f” for female group: Provide the experimental group of the mouse. Be sure that you use the same abbreviation or notation across each timepoint. Examples of group designations might be: bcg, saline, bcg+id93, saline+id93, saline+noMtb For the second and later sheets, you will record: who_collected: Record the first name of the person who actually handled the mouse from the scale. date_collected: Record the date using quotation marks, with the month, then day, then year. For example, “May 31, 2022.” weight: Record as a number, without a unit in this column. The next column will be used for the units. unit: Provide the units that were used to take the weight (e.g., “g” for grams). Be consistent across all animals and timepoints in the abbreviation that you use (e.g., always use “g” for grams, not “g” sometimes and “grams” sometimes) existing_cage_number: Provide the cage number that the mouse is in when you start weighing at that time point. If the mouse is moved to another cage on this day, you will specify that in the next column. If the animal was moved from one cage to another between the last weighing and the date of the timepoint you are measuring, put in this column the cage number that the animal was in the last time it was weighed. new_cage_number: If the animal is moved to a new cage on the date of the timepoint you are measuring, then use this column to record the number of the cage you move it too. Similarly, if the animal moved cages between the last measured timepoint and this one, use this column to record the cage it was moved to. Otherwise, if the animal stays in the same cage that it was at the last measured time point, leave this column empty. group: Provide the experimental group of the mouse. Be sure that you use the same abbreviation or notation across each timepoint. Examples of group designations might be: bcg, saline, bcg+id93, saline+id93, saline+noMtb notes: Record information regarding clinical observations (e.g., “back is balding,” “barbering,” “excessive grooming,” “euthanized”). 4.0.3 Processing collected data Once data are collected, the file can be run through an R workflow. This workflow will convert the data into a format that is easier to work with for data analysis and visualization. It will also produce a report on the data in the spreadsheet, and ultimately it will also write relevant results into a global database for the experiment. This section provides the details of that pipeline. It aims to explain the code that processes the data and generates visualizations. You do not need to run this code step-by-step, but instead can access a script with the full code by clicking [here]. The first step in the workflow is to read in the data from the spreadsheet. As long as the data are collected following the template that was described earlier, this code should be able to read it in correctly and create a master dataset with the data from all sheets of the spreadsheet. First, the workflow loads some additional R libraries. You may need to install these on your local R session if you do not already have them installed. library(readxl) library(tidyverse) Next, it uses a custom function to read in data from the spreadsheet. This function (read_mouse_weights) creates a list of all of the sheets in the spreadsheet, so it can be sure to read in all collected data. It creates a list with each of the sheets with weekly weight measures, so it can read in each sheet in that list. Next, it reads in both the first sheet (with the initial mouse measures), as well as all the weight sheets. read_mouse_weights &lt;- function(filepath) { # getting info about all excel sheets mouse_weights_sheets &lt;- readxl::excel_sheets(filepath)[-1] # First sheet is initial data, not mouse weights mouse_weights &lt;- purrr::map(mouse_weights_sheets, ~ readxl::read_excel(filepath, sheet = .x, col_types = c(&quot;text&quot;, # who_collected &quot;text&quot;, # date_collected &quot;text&quot;, # notch_id &quot;numeric&quot;, # weight &quot;text&quot;, # unit &quot;text&quot;, # existing_cage_number &quot;text&quot;, # new_cage_number &quot;text&quot;, # group &quot;text&quot; # notes ))) %&gt;% dplyr::bind_rows() %&gt;% mutate(date_collected = lubridate::mdy(date_collected)) return(mouse_weights) } # Use function to read in mouse weights from the sheet # (we&#39;ll read in the initial information later) our_mouse_weights &lt;- read_mouse_weights(filepath = &quot;DATA/body_weights_measurement.xlsx&quot;) The next part of the processing is needed to track each mouse throughout the study. Some mice are moved from one cage to another. Often, this might be because the mice are fighting or otherwise show signs that they’d do better if separated. The researchers in the wet lab record the mouse information based on the cage the mouse is in at the start of the day that the measurement is taken, and they also record if the mouse is moved to a new cage, specifically that new cage number. This code is using that information to create a unique ID for each mouse as it moves to different cages over the course of the study. # Add a unique mouse ID. For right now, add it only for the first time point. our_mouse_weights &lt;- our_mouse_weights %&gt;% mutate(mouse_id = 1:n(), mouse_id = ifelse(date_collected == first(date_collected), mouse_id, NA)) # Function to get the next cage number based on the # existing cage number and notch ID. If the mouse does not # switch cages again, the output is a vector of length 0 get_next_cage &lt;- function(existing_cage_number, notch_id, df = our_mouse_weights){ next_cage &lt;- df %&gt;% filter(.data$existing_cage_number == {{existing_cage_number}} &amp; .data$notch_id == {{notch_id}} &amp; !is.na(.data$new_cage_number)) %&gt;% pull(new_cage_number) return(next_cage) } # Function to get the full list of cages for each individual # mouse, over the course of all data collected to date get_mouse_cages &lt;- function(mouse_starting_cage, mouse_notch_id, df = our_mouse_weights){ mouse_cage_list &lt;- mouse_starting_cage i &lt;- 1 while(TRUE){ next_cage &lt;- get_next_cage(existing_cage_number = mouse_cage_list[i], notch_id = mouse_notch_id, df = df) if(length(next_cage) == 0) { break } i &lt;- i + 1 mouse_cage_list[i] &lt;- next_cage } return(mouse_cage_list) } # Create a dataframe that lists all mice at the first time point, # as well as a list of all the cages they have been in over the # experiment mice_cage_lists &lt;- our_mouse_weights %&gt;% filter(date_collected == first(date_collected)) %&gt;% select(notch_id, existing_cage_number, mouse_id) %&gt;% mutate(cage_list = map2(.x = existing_cage_number, .y = notch_id, .f = ~ get_mouse_cages(.x, .y, df = our_mouse_weights))) # Add a column with the latest cage to the weight dataframe our_mouse_weights$latest_cage &lt;- NA # Loop through all the individual mice, based on mice with a # measurement at the first time point. Add the unique ID for # each mouse, which will apply throughout the experiment. Also # add the most recent cage ID, so the mouse can be identified # by lab members based on it&#39;s current location for(i in 1:nrow(mice_cage_lists)){ this_notch_id &lt;- mice_cage_lists[i, ]$notch_id this_cage_list &lt;- mice_cage_lists[i, ]$cage_list[[1]] this_unique_id &lt;- mice_cage_lists[i, ]$mouse_id latest_cage &lt;- this_cage_list[length(this_cage_list)] our_mouse_weights$mouse_id[our_mouse_weights$notch_id == this_notch_id &amp; our_mouse_weights$existing_cage_number %in% this_cage_list] &lt;- this_unique_id our_mouse_weights$latest_cage[our_mouse_weights$notch_id == this_notch_id &amp; our_mouse_weights$existing_cage_number %in% this_cage_list] &lt;- latest_cage } # Add a label for each mouse based on its notch_id and latest cage our_mouse_weights &lt;- our_mouse_weights %&gt;% mutate(mouse_label = paste(&quot;Cage:&quot;, latest_cage, &quot;Notch:&quot;, notch_id)) our_mouse_weights ## # A tibble: 980 × 12 ## who_collected date_collected notch_id weight unit existing_cage_number ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Taru 2022-05-26 0 18.4 g 22003 ## 2 Taru 2022-05-26 1R 17.2 g 22003 ## 3 Taru 2022-05-26 1L 17 g 22003 ## 4 Taru 2022-05-26 1R1L 18.8 g 22003 ## 5 Taru 2022-05-26 0 18.4 g 22004 ## 6 Taru 2022-05-26 1R 17.7 g 22004 ## 7 Taru 2022-05-26 1L 20.2 g 22004 ## 8 Taru 2022-05-26 1R1L 17.1 g 22004 ## 9 Taru 2022-05-26 0 17.6 g 22005 ## 10 Taru 2022-05-26 1R 20 g 22005 ## # … with 970 more rows, and 6 more variables: new_cage_number &lt;chr&gt;, ## # group &lt;chr&gt;, notes &lt;chr&gt;, mouse_id &lt;int&gt;, latest_cage &lt;chr&gt;, ## # mouse_label &lt;chr&gt; Now that the weekly weights are processed, the pipeline will read in and add the information that was calculated in the first sheet of the spreadsheet. This information is constant information for each mouse, like their sex, species, and experimental group. mouse_initial &lt;- readxl::read_excel(&quot;DATA/body_weights_measurement.xlsx&quot;, sheet = 1, col_types = c(&quot;text&quot;, # notch_id &quot;text&quot;, # starting_cage_number &quot;text&quot;, # dob &quot;text&quot;, # species &quot;text&quot;, # sex &quot;text&quot; # group )) %&gt;% mutate(dob = lubridate::mdy(dob), sex = forcats::as_factor(sex)) mouse_ids &lt;- our_mouse_weights %&gt;% filter(date_collected == first(date_collected)) %&gt;% select(notch_id, existing_cage_number, mouse_id) %&gt;% rename(starting_cage_number = existing_cage_number) mouse_initial &lt;- mouse_initial %&gt;% left_join(mouse_ids, by = c(&quot;notch_id&quot;, &quot;starting_cage_number&quot;)) our_mouse_weights &lt;- our_mouse_weights %&gt;% left_join(mouse_initial, by = c(&quot;mouse_id&quot;, &quot;notch_id&quot;, &quot;group&quot;)) # Explore this data a bit our_mouse_weights %&gt;% ggplot(aes(x = date_collected, y = weight, group = mouse_id, color = sex)) + geom_line() + facet_wrap(~ group) our_mouse_weights %&gt;% ggplot(aes(x = date_collected, y = weight, color = who_collected)) + geom_point() library(ggbeeswarm) our_mouse_weights %&gt;% filter(date_collected == last(date_collected)) %&gt;% ggplot(aes(x = group, y = weight)) + geom_beeswarm(aes(color = sex)) + geom_boxplot(fill = NA, color = &quot;dodgerblue&quot;) write_csv(our_mouse_weights, &quot;DATA/example_mouse_output.csv&quot;) References "],["colony-forming-units-to-determine-bacterial-counts.html", "Chapter 5 Colony forming units to determine bacterial counts 5.1 Data description 5.2 Read in data 5.3 Example one 5.4 Exploratory analysis and quality checks 5.5 Exploratory analysis 5.6 Identify a good dilution for each sample 5.7 Calculate CFUs from best dilution/Estimate bacterial load for each sample based on good dilution 5.8 Create initial report information for these data 5.9 Sample ANOVA 5.10 Save processed data to database 5.11 Example two", " Chapter 5 Colony forming units to determine bacterial counts 5.1 Data description The data are collected in a spreadsheet with multiple sheets. The first sheet (named “metadata”) is used to record some metadata for the experiment, while the following sheets are used to record CFUs counts from the plates used for samples from each organ, with one sheet per organ. For example, if you plated data from both the lung and spleen, there would be three sheets in the file: one with the metadata, one with the plate counts for the lung, and one with the plate counts for the spleen. The metadata sheet is used to record information about the overall process of plating the data. Values from this sheet will be used in calculating the bacterial load in the original sample based on the CFU counts. This spreadsheet includes the following columns: organ: Include one row for each organ that was plated in the experiment. You should name the organ all in lowercase (e.g., “lung,” “spleen”). You should use the same name to also name the sheet that records data for that organ for example, if you have rows in the metadata sheet for “lung” and “spleen,” then you should have two other sheets in the file, one sheet named “lung” and one named “spleen,” which you’ll use to store the plate counts for each of those organs. prop_resuspended: In this column, give the proportion of that organ that was plated. For example, if you plated half the lung, then in the “lung” row of this spread sheet, you should put 0.5 in the prop_resuspended column. total_resuspended_uL: This column contains an original volume of tissue homogenate. For example, raw lung tissue is homogenized in 500 uL of PBS in a tube containing metal beads. og_aliquot_uL: 100 uL of th total_resuspended slurry would be considered an original aliquot and is used to peform serial dilutions. dilution_factor: Amount of the original stock solution that is present in the total solution, after dilution(s) plated_uL: Amount of suspension + diluent plated on section of solid agar 5.2 Read in data library(readxl) library(dplyr) library(purrr) library(tidyr) library(stringr) library(tidyverse) library(gridExtra) library(ggplot2) library(ggpubr) #Replace w/ path to CFU sheet path &lt;- c(&quot;DATA/Copy of baa_cfu_sheet.xlsx&quot;) sheet_names &lt;- excel_sheets(path) sheet_names &lt;- sheet_names[!sheet_names %in% c(&quot;metadata&quot;)] merged_data &lt;- list() for(i in 1:length(sheet_names)){ data &lt;- read_excel(path, sheet = sheet_names[i]) %&gt;% mutate(organ = paste0(sheet_names[i])) data &lt;- data %&gt;% #mutate(missing_col = NA) %&gt;% mutate_if(is.double, as.numeric) %&gt;% mutate_if(is.numeric, as.character) %&gt;% pivot_longer(starts_with(&quot;dil_&quot;), names_to = &quot;dilution&quot;, values_to = &quot;CFUs&quot;) %&gt;% mutate(dilution = str_extract(dilution, &quot;[0-9]+&quot;), dilution = as.numeric(dilution)) merged_data[[i]] &lt;- data } all_data &lt;- bind_rows(merged_data, .id = &quot;column_label&quot;) %&gt;% select(-column_label) head(merged_data) ## [[1]] ## # A tibble: 342 × 8 ## count_date who_plated who_counted groups mouse organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &quot;\\&quot;February 21 2022… BK BK group… A lung 0 TNTC ## 2 &quot;\\&quot;February 21 2022… BK BK group… A lung 1 TNTC ## 3 &quot;\\&quot;February 21 2022… BK BK group… A lung 2 TNTC ## 4 &quot;\\&quot;February 21 2022… BK BK group… A lung 3 53 ## 5 &quot;\\&quot;February 21 2022… BK BK group… A lung 4 9 ## 6 &quot;\\&quot;February 21 2022… BK BK group… A lung 5 4 ## 7 &quot;\\&quot;February 21 2022… BK BK group… A lung 6 2 ## 8 &quot;\\&quot;February 21 2022… BK BK group… A lung 7 1 ## 9 &quot;\\&quot;February 21 2022… BK BK group… A lung 8 0 ## 10 &quot;\\&quot;February 21 2022… BK BK group… B lung 0 TNTC ## # … with 332 more rows ## ## [[2]] ## # A tibble: 112 × 8 ## count_date who_plated who_counted groups mouse organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 0 TNTC ## 2 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 1 TNTC ## 3 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 2 53 ## 4 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 3 9 ## 5 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 4 4 ## 6 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 5 2 ## 7 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 6 1 ## 8 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 A sple… 7 0 ## 9 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 B sple… 0 TNTC ## 10 &quot;\\&quot;April 25 2022\\&quot;&quot; JR JR group_1 B sple… 1 TNTC ## # … with 102 more rows head(all_data) ## # A tibble: 6 × 8 ## count_date who_plated who_counted groups mouse organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 0 TNTC ## 2 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 1 TNTC ## 3 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 2 TNTC ## 4 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 3 53 ## 5 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 4 9 ## 6 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 5 4 5.3 Example one 5.4 Exploratory analysis and quality checks 5.5 Exploratory analysis Dimensions of input data: Based on the input data, data were collected for the following organ or organs: The following number of mice were included for each: The following number of replicates were recorded at each count date for each experimental group: The following number of dilutions and dilution level were recorded for each organ: People who plated and collected the data. Date or dates of counting: Based on the input data, the plates included in these data were counted by the following person or persons: Based on the input data, the plates included in these data were counted on the following date or dates: all_data %&gt;% select(organ, who_plated, who_counted, count_date) %&gt;% distinct() ## # A tibble: 3 × 4 ## organ who_plated who_counted count_date ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 lung BK BK &quot;\\&quot;February 21 2022\\&quot;&quot; ## 2 lung BK BK &quot;\\&quot;April 18 2022\\&quot;&quot; ## 3 spleen JR JR &quot;\\&quot;April 25 2022\\&quot;&quot; head(all_data) ## # A tibble: 6 × 8 ## count_date who_plated who_counted groups mouse organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 0 TNTC ## 2 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 1 TNTC ## 3 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 2 TNTC ## 4 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 3 53 ## 5 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 4 9 ## 6 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 5 4 Distribution of CFUs at each dilution: Here’s a plot that shows how many plates were too numerous to count at each dilution level: Here is a plot that shows how the CFU counts were distributed by dilution level in the data: 5.6 Identify a good dilution for each sample # Make all_data into tidy data and filter for CFUs between 10-75 tidy_cfu_data &lt;- all_data %&gt;% mutate(dilution = str_extract(dilution, &quot;[0-9]+&quot;), dilution = as.numeric(dilution)) %&gt;% filter((CFUs &gt;= 5 &amp; CFUs &lt;= 95) | groups == &quot;control&quot;) %&gt;% mutate(CFUs = as.numeric(CFUs)) head(tidy_cfu_data) ## # A tibble: 6 × 8 ## count_date who_plated who_counted groups mouse organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 3 53 ## 2 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 4 9 ## 3 &quot;\\&quot;February 21 2022\\… BK BK group… C lung 5 8 ## 4 &quot;\\&quot;February 21 2022\\… BK BK group… D lung 3 53 ## 5 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 2 92 ## 6 &quot;\\&quot;February 21 2022\\… BK BK group… A lung 4 7 5.7 Calculate CFUs from best dilution/Estimate bacterial load for each sample based on good dilution # Calculating CFU/ml for every qualifying replicate between 10-75 CFUs. Column binding by organ name to the metadata sheet via inner_join(). meta &lt;- read_excel(path, sheet = &quot;metadata&quot;) tidy_cfu_meta_joined &lt;- inner_join(meta, tidy_cfu_data) %&gt;% group_by(groups) %&gt;% mutate(CFUs_per_ml = (CFUs * (dilution_factor^dilution) * (total_resuspension_mL/volume_plated_ul) * 1000)) %&gt;% select(organ, count_date, who_plated, who_counted, groups, mouse, dilution, CFUs, CFUs_per_ml) %&gt;% ungroup() ## Joining, by = &quot;organ&quot; head(tidy_cfu_meta_joined) ## # A tibble: 6 × 9 ## organ count_date who_plated who_counted groups mouse dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lung &quot;\\&quot;February 21 2022\\… BK BK group… A 3 53 ## 2 lung &quot;\\&quot;February 21 2022\\… BK BK group… A 4 9 ## 3 lung &quot;\\&quot;February 21 2022\\… BK BK group… C 5 8 ## 4 lung &quot;\\&quot;February 21 2022\\… BK BK group… D 3 53 ## 5 lung &quot;\\&quot;February 21 2022\\… BK BK group… A 2 92 ## 6 lung &quot;\\&quot;February 21 2022\\… BK BK group… A 4 7 ## # … with 1 more variable: CFUs_per_ml &lt;dbl&gt; 5.8 Create initial report information for these data tidy_lung_cfu_plot &lt;- tidy_cfu_meta_joined %&gt;% filter(organ == &quot;lung&quot;) %&gt;% mutate(group = fct_relevel(groups, &quot;group_1&quot;, &quot;group_2&quot;, &quot;group_3&quot;, &quot;group_4&quot;)) %&gt;% ggplot(aes(x = groups, y = log10(CFUs_per_ml), fill = groups))+ stat_boxplot( aes(x = groups, y = log10(CFUs_per_ml)), geom=&#39;errorbar&#39;, linetype=1, width=0.5)+ geom_boxplot(aes(group = groups), fill = NA, show.legend = FALSE, color = &quot;lightgrey&quot;)+ geom_point(show.legend = FALSE)+ labs(title = paste0(&quot;CFUs in early infected mouse lung&quot;), x = &quot;Group&quot;, y = &quot;log10(CFU/mL)&quot;, color = &quot;Group&quot;)+ guides(shape = &quot;none&quot;)+ theme_minimal()+ stat_compare_means(label = &quot;p.signif&quot;, method = &quot;t.test&quot;, ref.group = &quot;group_1&quot;) + scale_y_continuous(expand = c(0, 0), limits = c(0, 8)) tidy_lung_cfu_plot 5.9 Sample ANOVA cfu_stats &lt;- tidy_cfu_meta_joined %&gt;% group_by(organ) %&gt;% nest() %&gt;% mutate(aov_result = map(data, ~aov(CFUs_per_ml ~ groups, data = .x)), tukey_result = map(aov_result, TukeyHSD), tidy_tukey = map(tukey_result, broom::tidy)) %&gt;% unnest(tidy_tukey, .drop = TRUE) %&gt;% separate(contrast, into = c(&quot;contrast1&quot;, &quot;contrast2&quot;), sep = &quot;-&quot;) %&gt;% select(-data, -aov_result, -tukey_result, -term, -null.value)# %&gt;% ## Warning: The `.drop` argument of `unnest()` is deprecated as of tidyr 1.0.0. ## All list-columns are now preserved. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. # filter(adj.p.value &lt;= 0.05) cfu_stats ## # A tibble: 9 × 7 ## # Groups: organ [2] ## organ contrast1 contrast2 estimate conf.low conf.high adj.p.value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lung group_2 group_1 -60953. -138742. 16836. 0.171 ## 2 lung group_3 group_1 -63903. -135699. 7893. 0.0963 ## 3 lung group_4 group_1 -26214. -102416. 49987. 0.793 ## 4 lung group_3 group_2 -2950. -69900. 64000. 0.999 ## 5 lung group_4 group_2 34739. -36915. 106393. 0.569 ## 6 lung group_4 group_3 37689. -27410. 102787. 0.417 ## 7 spleen group_2 group_1 -6565 -13529. 399. 0.0656 ## 8 spleen group_3 group_1 -7310 -13341. -1279. 0.0178 ## 9 spleen group_3 group_2 -745. -6776. 5286. 0.943 5.10 Save processed data to database 5.11 Example two "],["enzyme-linked-immunosorbest-assay-elisa.html", "Chapter 6 Enzyme-linked immunosorbest assay (ELISA) 6.1 Importance of ELISA 6.2 ELISA data analysis 6.3 1. Curve fitting model: 6.4 2. Endpoint titer method 6.5 Apply the fitting sigmoid model and endpoint titer function in our dataset 6.6 Create function of Fitted model and endpoint titer, where the output of the fitted model data will be the input of the endpoint titer 6.7 ELISA data processing", " Chapter 6 Enzyme-linked immunosorbest assay (ELISA) ELISA is a standard molecular biology assay for detecting and quantifying a variety of compounds, including peptides, proteins, and antibodies in a sample. The sample could be serum, plasma, or bronchoalveolar lavage fluid (BALF). 6.1 Importance of ELISA An antigen-specific reaction in the host results in the production of antibodies, which are proteins found in the blood. In the event of an infectious disease, it aids in the detection of antibodies in the body. ELISA is distinguishable from other antibody-assays in that it produces quantifiable findings and separates non-specific from specific interactions by serial binding to solid surfaces, which is often a polystyrene multi-well plate. In IMPAc-TB project, it is crucial to evaluate the if the vaccine is eliciting humoral immunity and generating antibodies against vaccine antigen. ELISA will be used to determine the presence of Immunoglobulin (Ig) IgG, IgA, and IgM in the serum different time points post-vaccination. 6.1.1 Principle of ELISA ELISA is based on the principle of antigen-antibody interaction. An antigen must be immobilized on a solid surface and then complexed with an enzyme-linked antibody in an ELISA. The conjugated enzyme’s activity is evaluated by incubating it with a substrate to yield a quantifiable result, which enables detection. There are four basic steps of ELISA: 1. Coating multiwell plate with antigen/antibody: This step depends on what we want to detect the sample. If we need to evaluate the the presence of antibody, the plate will be coated with the antigen, and vice versa. To coat the plate, a fixed concentration of antigen (protein) is added to a 96 well high-binding plate (charged plate). Plate is incubated over night with the antigen at 4 degree celsius (as proteins are temperature sensitive) so that antigens are completely bound to the well. 2. Blocking: It is possible that not each and every site of the well is coated with the targeted antigen, and there could be uncovered areas. It is important to block those empty spaces so that primary antibody (which we will add to the next step) binds to these spaces and give us false positive results. For this, microplate well surface-binding sites are blocked with an unrelated protein or other substance.Most common blocking agents are bovine serum albumin, skim milk, and casein. One of the best blocking agents is to use the serum from the organism in which your secondary (detection antibody) is raised. For example, if the secondary antibody is raised in goat, then we can use goat serum as a blocking agent. 3. Probing: Probing is the step where we add sample containing antibodies that we want to detect. This will be the primary antibody. If the antibodies against the antigen (which we have coated) are present in the sample, it will bind to the antigen with high affinity. 4. Washing: After the incubation of sample containing primary antibody, the wells are washed so that any unbound antibody is washed away. Washing solution contains phosphate buffer saline + 0.05% tween-20 (a mild detergent). 0.05% tween-20 washes away all the non-specific interactions as those are not strong, but keeps all the specific interaction as those are strong and cannot be detached with mild detergent. 5. Detection: To detect the presence of antibody-antigen complex, a secondary antibody labelled with an enzyme (usually horseradish peroxidase) is added to the wells, incubated and washed. 6. Signal Measurement: Finally to detect “if” and “how much” of the antibody is present, a chromogenic substrate (like 3,3’,5,5’-Tetramethylbenzidine) is added to the wells, which can be cleaved the the enzyme that is tagged to the secondary antibody. The color compund is formed after the addition of the substrate, which is directly proportional to the amount of antibody present in the sample. The plate is read on a plate reader, where color is converted to numbers. Figure 6.1: A caption 6.1.2 Loading libraries library(readxl) library(tidyverse) library(minpack.lm) library(broom) library(purrr) library(ggbeeswarm) 6.2 ELISA data analysis Analysis of ELISA data is the most important part of the ELISA experiment. ELISA data can be analyzed in different ways based on how the data is acquired. There are a a few examples of the type of ELISA data : 1. With standard curve: ELISA can be used to determine the concentrations of the antigen and antibody. This type of ELISA data usually have a standard curve with different concentrations of the known analyte and the concentration in the sample is determined by extrapolating the unknown values in the curve. This type of assay is straightforward, easy to interpret and are more robust. 2. Without standard curve: Usually vaccine studies involve investigating the presence of high-affinity (and novel) antibodies against the vaccine antigens. Therefore, plotting a standard curve is not feasible as there is no previous information available for antibody concentration or type of antibody. Also, because antibody response to a vaccine will differ depending on the individual, it is not practical to generate a calibration curve from which absolute concentrations can be extrapolated. For this type of ELISA, quantification of the antibody titers is performed using serial dilutions of the test samples, and analysis can be performed using the following three methods (Hartman et al. 2018): Fitting sigmoid model Endpoint titer method 3: Absorbance summation method Let’s have a look at these methods, how we can apply these methods in our data, and R-based packages that we can utilize to perform this analysis. 6.3 1. Curve fitting model: The curve in ELISA data represents a plot of known concentrations versus their corresponding signal responses. The typical range of these calibration curves is one to two orders of magnitude on the response axis and two or more orders of magnitude on the concentration axis. The real curve of each assay could be easily identified if an infinite number of concentration dilutions with an infinite number of repetitions could be tested. The correct curve must be approximated from a relatively small number of noisy points, though, because there are a finite number of dilutions that may be performed. To estimate the dose-response relationship between standard dilutions, a method of interpolating between standards is required because there cannot be a standard at every concentration. This process is typically performed using a mathematical function or regression to approximate the true shape of the curve. A curve model is the name given to this approximating function, which commonly uses two or more parameters to describe a family of curves, and are then adjusted in order to find the curve from the family of curves that best fits the assay data. Three qualities should be included in a good curve fitting model. 1. The true curve’s shape must be accurately approximated by the curve model. If the curve model does not accomplish this, there is no way to adjust for this component of the total error that results from a lack of fit. 2. In order to get concentration estimates with minimal inaccuracy, a decent curve model must be able to average away as much of the random variation as is practical. 3. A successful curve model must be capable of accurately predicting concentration values for points between the anchor points of the standard dilutions. 6.3.1 How do we perform curve fitting model There are two major steps in performing curve fitting model for non-linear data like ELISA: 1. Finding the initial starting estimates of the parameters 2. locating the optimal solution in a region of the initial estimates We have presented an example below where we have performed a 8-10 point serial dilution of our sample and fitted a 4 parameter curve model. 6.3.2 An example of the curve fitting model 6.3.2.1 Read in the data This information comes from the 2018 study conducted by Hartman et al. Hartman et al. analyzed the ELISA data in their study utilizing fitted sigmoid analysis, end point titer, and absorbance summation. We utilized this information to determine whether our formulas and calculations provide the same outcomes and values as theirs. elisa_example_data &lt;- read_excel(&quot;DATA/example_elisa_data.xlsx&quot;) 6.3.2.2 Tidying the data We next performed tidying the data and make it in a format so that we can plot a sigmoid curve with that. # Divide dilution column into two seoparate columns elisa_example_data &lt;- separate(elisa_example_data, col = &quot;dilution&quot;, into = c(&quot;numerator&quot;, &quot;denominator&quot;), sep = &quot;\\\\/&quot;) # Convert the tabke from character to numeric elisa_example_data &lt;- elisa_example_data %&gt;% mutate_if(is.character, as.numeric) elisa_example_data$dilution &lt;- elisa_example_data$numerator/elisa_example_data$denominator elisa_example_data &lt;- elisa_example_data %&gt;% mutate(log_dilution = log(dilution, base = 3)) head(elisa_example_data) ## # A tibble: 6 × 5 ## numerator denominator absorbance dilution log_dilution ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 30 4 0.0333 -3.10 ## 2 1 90 3.73 0.0111 -4.10 ## 3 1 270 2.34 0.00370 -5.10 ## 4 1 810 1.1 0.00123 -6.10 ## 5 1 2430 0.51 0.000412 -7.10 ## 6 1 7290 0.22 0.000137 -8.10 6.3.2.3 Create function for curve fitting model We next created the curve fitting model function by using nlsLM function from “minpack.lm” package. The purpose of nlslm is to minimize the sum square of the vector returned by the function fn, by a modification of the Levenberg-Marquardt algorithm. In the early 1960s, the Levenberg-Marquardt algorithm was developed to address nonlinear least squares problems. Through a series of well-chosen updates to model parameter values, Levenberg-Marquardt algorithm lower the sum of the squares of the errors between the model function and the data points. mod_1 &lt;- nlsLM(absorbance ~ ((a-d)/(1+(log_dilution/c)^b)) + d, data = elisa_example_data, start = list (a = 4, d= 0, c = -5, b = 1)) # a = maximum absorbance # d = minimum absobance # c = point of maximum growth # b = slope at c mod_1 ## Nonlinear regression model ## model: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## data: elisa_example_data ## a d c b ## 4.12406 0.04532 -5.31056 7.62972 ## residual sum-of-squares: 0.02221 ## ## Number of iterations to convergence: 9 ## Achieved convergence tolerance: 1.49e-08 summary(mod_1) ## ## Formula: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## a 4.12406 0.05820 70.860 1.75e-12 *** ## d 0.04532 0.02268 1.998 0.0808 . ## c -5.31056 0.03933 -135.037 1.01e-14 *** ## b 7.62972 0.35854 21.280 2.50e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05269 on 8 degrees of freedom ## ## Number of iterations to convergence: 9 ## Achieved convergence tolerance: 1.49e-08 6.3.2.4 Apply the function to the data tidy_params &lt;- mod_1 %&gt;% tidy() a &lt;- tidy_params$estimate[tidy_params$term == &quot;a&quot;] b &lt;- tidy_params$estimate[tidy_params$term == &quot;b&quot;] c &lt;- tidy_params$estimate[tidy_params$term == &quot;c&quot;] d &lt;- tidy_params$estimate[tidy_params$term == &quot;d&quot;] elisa_example_data &lt;- elisa_example_data %&gt;% mutate(fitted = predict(mod_1)) elisa_example_data &lt;- elisa_example_data %&gt;% mutate(fitted = predict(mod_1)) 6.3.2.5 Plot the sigmoid curve with fitted sigmoid model elisa_example_data %&gt;% ggplot(aes(x = log_dilution, y = absorbance)) + geom_point() + geom_line(aes(y=fitted), color = &quot;dodgerblue&quot;) 6.4 2. Endpoint titer method The endpoint titer approach chooses an absorbance value just above the background noise (or the lower asymptotic level). The highest dilution with an absorbance greater than this predetermined value is the endpoint titer. This method is based on the assumption that a sample with a higher protein concentration will require a higher dilution factor to achieve an absorbance just above the level of background noise. 6.4.1 Create an endpoint titer function and apply it to the output of the fitted sigmoid model values. endpoint_titer &lt;- c * (((a - d) / (0.2 - d)) - 1) ^ (1 / b) summary(endpoint_titer) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -8.113 -8.113 -8.113 -8.113 -8.113 -8.113 endpoint_titer ## [1] -8.113285 6.4.2 Other methods to analyze ELISA data 6.4.2.1 Absorption summation 6.4.2.2 Area under the curve In this model of data analysis, we sum all the absorbance values from each sample to obtain one value. This value is termed as absorption summation (AS). Using the above data, the AS will be calculated as below: AS = 0.04 + 0.04 + 0.05 + 0.05 + 0.06 + 0.1 + 0.22 + 0.51 + 1.1 + 2.34 + 3.73 + 4.0 AS ## [1] 12.24 6.5 Apply the fitting sigmoid model and endpoint titer function in our dataset The presented data is from a mouse study. In this data, presence of IgG antibody has been evaluated against receptor binding domain (RBD) of SARS-CoV-2 virus in two different groups of mice. We need to elucidate which group has higher concentration of the antibodies. 6.5.0.1 Read in the data elisa_data &lt;- read_excel(&quot;DATA/elisa_data_serial_dilution.xlsx&quot;) 6.5.0.2 Tidy the data elisa_data &lt;- pivot_longer(data = elisa_data, cols = &quot;Mouse_1&quot;:&quot;Mouse_5&quot;, names_to = &quot;mouse_id&quot;, values_to = &quot;absorbance&quot;) head(elisa_data) ## # A tibble: 6 × 4 ## Groups Dilution mouse_id absorbance ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Group 1 1/50 Mouse_1 4.1 ## 2 Group 1 1/50 Mouse_2 3.9 ## 3 Group 1 1/50 Mouse_3 4.3 ## 4 Group 1 1/50 Mouse_4 4.2 ## 5 Group 1 1/50 Mouse_5 4 ## 6 Group 1 1/100 Mouse_1 3.9 # separate dilution column and convert it to log2 elisa_data &lt;- separate(elisa_data, col = &quot;Dilution&quot;, into = c(&quot;numerator&quot;, &quot;denomenator&quot;), sep = &quot;\\\\/&quot;) elisa_data &lt;- elisa_data %&gt;% transform(numerator = as.numeric(numerator), denomenator = as.numeric(denomenator)) elisa_data &lt;- elisa_data %&gt;% mutate(dilution = elisa_data$numerator/elisa_data$denomenator) elisa_data &lt;- elisa_data %&gt;% mutate(log_dilution = log2(dilution)) head(elisa_data) ## Groups numerator denomenator mouse_id absorbance dilution log_dilution ## 1 Group 1 1 50 Mouse_1 4.1 0.02 -5.643856 ## 2 Group 1 1 50 Mouse_2 3.9 0.02 -5.643856 ## 3 Group 1 1 50 Mouse_3 4.3 0.02 -5.643856 ## 4 Group 1 1 50 Mouse_4 4.2 0.02 -5.643856 ## 5 Group 1 1 50 Mouse_5 4.0 0.02 -5.643856 ## 6 Group 1 1 100 Mouse_1 3.9 0.01 -6.643856 6.5.0.2.1 converting data into dataframe elisa_data_df &lt;- elisa_data %&gt;% group_by(Groups, mouse_id) %&gt;% summarize(log_dilution = log_dilution, absorbance = absorbance) ## `summarise()` has grouped output by &#39;Groups&#39;, &#39;mouse_id&#39;. You can override using ## the `.groups` argument. elisa_data_nested &lt;- elisa_data %&gt;% group_by(Groups, mouse_id) %&gt;% nest() head(elisa_data_nested) ## # A tibble: 6 × 3 ## # Groups: Groups, mouse_id [6] ## Groups mouse_id data ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 Group 1 Mouse_1 &lt;tibble [10 × 5]&gt; ## 2 Group 1 Mouse_2 &lt;tibble [10 × 5]&gt; ## 3 Group 1 Mouse_3 &lt;tibble [10 × 5]&gt; ## 4 Group 1 Mouse_4 &lt;tibble [10 × 5]&gt; ## 5 Group 1 Mouse_5 &lt;tibble [10 × 5]&gt; ## 6 Group 2 Mouse_1 &lt;tibble [10 × 5]&gt; 6.5.0.2.2 plot the curves to evaluate the a, d, c, and b elisa_data %&gt;% ggplot(aes(x = log_dilution, y = absorbance)) + geom_point() + geom_line() + facet_wrap(Groups ~ mouse_id) Based on the curve, the values are: a = 4, d = 0 c = 2 b = 1 6.5.1 Creating a function for fitting model fitted_model_elisa &lt;- function(df_elisa, start_a, start_d, start_c, start_b) { mod_1 &lt;- nlsLM(absorbance ~ ((a-d)/(1+(log_dilution/c)^b)) + d, data = df_elisa, start = list(a = start_a, d = start_d, c = start_c, b = start_b)) return(mod_1) } 6.5.1.1 Fitting the model into the dataset fitted_model_elisa(elisa_data_nested$data[[1]], start_a = 4, start_d = 0, start_c = -8, start_b = 1) ## Nonlinear regression model ## model: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## data: df_elisa ## a d c b ## 4.3070 -0.6009 -10.2577 5.2893 ## residual sum-of-squares: 0.1199 ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 1.49e-08 6.5.1.2 Apply the fitted model function to the whole dataframe elisa_fitted_data &lt;- elisa_data_nested %&gt;% mutate(fitted_data = purrr::map(data, ~ fitted_model_elisa(.x,start_a = 4, start_d = 0, start_c = -8, start_b = 1))) head(elisa_fitted_data) ## # A tibble: 6 × 4 ## # Groups: Groups, mouse_id [6] ## Groups mouse_id data fitted_data ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 Group 1 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 2 Group 1 Mouse_2 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 3 Group 1 Mouse_3 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 4 Group 1 Mouse_4 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 5 Group 1 Mouse_5 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 6 Group 2 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; 6.5.1.3 Take out the summary of the data elisa_fitted_data_summary &lt;- elisa_fitted_data %&gt;% mutate(elisa_fitted_data_summary = purrr::map(fitted_data, broom::glance)) unnested &lt;- elisa_fitted_data_summary %&gt;% unnest(elisa_fitted_data_summary) %&gt;% ungroup() %&gt;% dplyr::select(Groups, mouse_id, fitted_data) unnested$fitted_data[[1]] ## Nonlinear regression model ## model: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## data: df_elisa ## a d c b ## 4.3070 -0.6009 -10.2577 5.2893 ## residual sum-of-squares: 0.1199 ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 1.49e-08 6.6 Create function of Fitted model and endpoint titer, where the output of the fitted model data will be the input of the endpoint titer # Fitted model function fitted_model_elisa &lt;- function(df_elisa, start_a, start_d, start_c, start_b) { mod_1 &lt;- nlsLM(absorbance ~ ((a-d)/(1+(log_dilution/c)^b)) + d, data = df_elisa, start = list(a = start_a, d = start_d, c = start_c, b = start_b)) return(mod_1) } # Endpoint titer function endpoint_titer_elisa &lt;- function(fitted_data, back_value) { tidy_fitted &lt;- broom::tidy(fitted_data) est_a &lt;- tidy_fitted$estimate[tidy_fitted$term == &quot;a&quot;] est_b &lt;- tidy_fitted$estimate[tidy_fitted$term == &quot;b&quot;] est_c &lt;- tidy_fitted$estimate[tidy_fitted$term == &quot;c&quot;] est_d &lt;- tidy_fitted$estimate[tidy_fitted$term == &quot;d&quot;] endpoint_titer &lt;- est_c * (((est_a - est_d) / (back_value - est_d)) - 1) ^ (1 / est_b) return(endpoint_titer) } 6.6.0.1 Apply the fitted model fuction into the nested data and use the output of the fitted data as the input for endpoint titer value evaluation 6.6.0.1.1 Run fitted model on the data elisa_data_with_fit_model &lt;- elisa_data_nested %&gt;% mutate(fitted_data = purrr::map(data, ~ fitted_model_elisa(.x, start_a = 4, start_d = 0, start_c = -8, start_b = 1))) head(elisa_data_with_fit_model) ## # A tibble: 6 × 4 ## # Groups: Groups, mouse_id [6] ## Groups mouse_id data fitted_data ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 Group 1 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 2 Group 1 Mouse_2 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 3 Group 1 Mouse_3 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 4 Group 1 Mouse_4 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 5 Group 1 Mouse_5 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 6 Group 2 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; 6.6.0.1.2 Taking output of the fitted model function and into endpoint titer function elisa_data_with_endpoint_titer &lt;- elisa_data_with_fit_model %&gt;% mutate(endpoint_data = purrr::map(fitted_data, ~ endpoint_titer_elisa(.x, back_value = 0.2))) 6.6.0.2 Plot the endpoint titer data for the two groups elisa_data_with_endpoint_titer$endpoint_data= as.numeric(elisa_data_with_endpoint_titer$endpoint_data) elisa_data_with_endpoint_titer %&gt;% ggplot(aes(x = Groups, y = endpoint_data, color = Groups)) + geom_beeswarm(cex = 3) 6.6.0.3 Perform statistical analysis on the data elisa_data_stats &lt;- t.test(endpoint_data ~ Groups, data = elisa_data_with_endpoint_titer) elisa_data_stats %&gt;% tidy() ## # A tibble: 1 × 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.800 -14.0 -13.2 -18.8 0.00000268 5.63 -0.906 -0.695 ## # … with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; 6.6.0.4 Statistical data analysis for more than two groups 6.7 ELISA data processing We read ELISA plate in a 96 well plate using a plate reader. The plate reader generates the data in form of number in an excel sheet. We have created this pipeline/worksheet to bring out the information from the excl sheet to a tidy format in which the above created fitted model and endpoint titer functions can be applied. 6.7.0.1 Read in the first dataset Below is the example ELISA data that has came straight out of the plate reader. This data is arranged in a 96-well plate format and contains Optical Density (OD) values. elisa_raw_data &lt;- read_excel(&quot;DATA/elisa_s1_07-25-20.xlsx&quot;, sheet = &quot;S1&quot;, col_names = FALSE, range = &quot;B2:M9&quot;) ## New names: ## * `` -&gt; ...1 ## * `` -&gt; ...2 ## * `` -&gt; ...3 ## * `` -&gt; ...4 ## * `` -&gt; ...5 ## * ... head(elisa_raw_data) ## # A tibble: 6 × 12 ## ...1 ...2 ...3 ...4 ...5 ...6 ...7 ...8 ...9 ...10 ...11 ...12 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.199999999… 0.05 0.069 6.3E… 0.061 0.122 0.16… 0.145 0.135 6.80… 0.053 0.05 ## 2 7.900000000… 0.098 0.069 6.80… 0.115 0.202 5.89… 0.134 0.069 0.106 0.05 0.075 ## 3 8.899999999… 0.133 0.119 OVRF… 3.87 2.32 OVRF… 3.85 2.12 OVRF… 3.21 1.02 ## 4 OVRFLW 3.46 1.16 OVRF… 3.80 2.36 OVRF… 3.70 1.49 OVRF… 3.68 1.63 ## 5 3.815999999… 1.82 0.446 3.89… 3.42 1.13 OVRF… 2.33 0.608 OVRF… 3.41 1.10 ## 6 OVRFLW 3.69 1.43 OVRF… 3.66 1.27 3.839 1.74 0.444 2.49… 0.637 0.704 6.7.0.2 Tidy dataset 1 It is important to clean the data and arrange it in a format on which we can apply formulas and functions. # Convert all columns to numeric elisa_raw_data_numeric &lt;- elisa_raw_data %&gt;% mutate_if(is.character, as.numeric) ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion # pivot longer the data elisa_raw_data_tidy &lt;- pivot_longer(data = elisa_raw_data_numeric, cols = &quot;...1&quot;:&quot;...12&quot;, names_to = &quot;well_id&quot;, values_to = &quot;od_450nm&quot;) # remove &quot;...&quot; from the first column elisa_raw_data_tidy$well_id &lt;- str_replace(elisa_raw_data_tidy$well_id, &quot;...&quot;, &quot;&quot;) # Add new column to the data_frame elisa_raw_data_tidy_new &lt;- elisa_raw_data_tidy %&gt;% mutate(name = rep(LETTERS[1:8], each = 12)) elisa_raw_data_tidy_new &lt;- elisa_raw_data_tidy_new %&gt;% mutate(well_id = paste0(name, well_id)) %&gt;% select(-name) head(elisa_raw_data_tidy_new) ## # A tibble: 6 × 2 ## well_id od_450nm ## &lt;chr&gt; &lt;dbl&gt; ## 1 A1 0.052 ## 2 A2 0.05 ## 3 A3 0.069 ## 4 A4 0.063 ## 5 A5 0.061 ## 6 A6 0.122 6.7.0.3 Read in the second data set The second dataset contains the information such as groups, mouse id, and dilutions for the respective wells of the 96 well plate for the dataset-1. elisa_label_data &lt;- read_excel(&quot;DATA/elisa_s1_07-25-20.xlsx&quot;, sheet = &quot;S1&quot;, col_names = FALSE, range = &quot;Q2:AB9&quot;) ## New names: ## * `` -&gt; ...1 ## * `` -&gt; ...2 ## * `` -&gt; ...3 ## * `` -&gt; ...4 ## * `` -&gt; ...5 ## * ... head(elisa_label_data) ## # A tibble: 6 × 12 ## ...1 ...2 ...3 ...4 ...5 ...6 ...7 ...8 ...9 ...10 ...11 ...12 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 blank secon… naïv… 1A-1… 1A-1… 1A-1… 1A-2… 1A-2… 1A-2… 1A-3… 1A-3… 1A-3… ## 2 1A-4 (1/250 1A-4 … 1A-4… 1B-1… 1B-1… 1B-1… 1B-2… 1B-2… 1B-2… 1B-3… 1B-3… 1B-3… ## 3 1B-4 (1/250 1B-4 … 1B-4… 2A-1… 2A-1… 2A-1… 2A-2… 2A-2… 2A-2… 2A-3… 2A-3… 2A-3… ## 4 2B-1 (1/250 2B-1 … 2B-1… 2B-2… 2B-2… 2B-2… 2B-3… 2B-3… 2B-3… 2B-4… 2B-4… 2B-4… ## 5 3A-1 (1/250 3A-1 … 3A-1… 3A-2… 3A-2… 3A-2… 3A-3… 3A-3… 3A-3… 3A-4… 3A-4… 3A-4… ## 6 3B-1 (1/250 3B-1 … 3B-1… 3B-2… 3B-2… 3B-2… 3B-3… 3B-3… 3B-3… 3B-4… 3B-4… 3B-4… 6.7.0.4 Tidy dataset-2 # pivot longer the data elisa_label_data_tidy &lt;- pivot_longer(data = elisa_label_data, cols = &quot;...1&quot;:&quot;...12&quot;, names_to = &quot;well_id&quot;, values_to = &quot;information&quot;) # remove &quot;...&quot; from the first column elisa_label_data_tidy$well_id &lt;- str_replace(elisa_label_data_tidy$well_id, &quot;...&quot;, &quot;&quot;) # Add new column to the data_frame elisa_label_data_tidy_new &lt;- elisa_label_data_tidy %&gt;% mutate(name = rep(LETTERS[1:8], each = 12)) elisa_label_data_tidy_new &lt;- elisa_label_data_tidy_new %&gt;% mutate(well_id = paste0(name, well_id)) %&gt;% select(-name) head(elisa_label_data_tidy_new) ## # A tibble: 6 × 2 ## well_id information ## &lt;chr&gt; &lt;chr&gt; ## 1 A1 blank ## 2 A2 secondary ## 3 A3 naïve (1/250) ## 4 A4 1A-1 (1/250 ## 5 A5 1A-1 (1/1250 ## 6 A6 1A-1 (1/6250 6.7.0.5 Merge dataset-1 (with OD information) with dataset-2 (with respective data information) To create a complete full dataset with Groups, mouse-id, dilutions, and OD, we merged the dataset-1 and dataset-2 together. We also cleaned the data set so that mouse-ID and dilution columns are separate and have their own columns. #Merge the two datasets elisa_data = elisa_raw_data_tidy_new %&gt;% inner_join(elisa_label_data_tidy_new, by=&quot;well_id&quot;) head(elisa_data) ## # A tibble: 6 × 3 ## well_id od_450nm information ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 A1 0.052 blank ## 2 A2 0.05 secondary ## 3 A3 0.069 naïve (1/250) ## 4 A4 0.063 1A-1 (1/250 ## 5 A5 0.061 1A-1 (1/1250 ## 6 A6 0.122 1A-1 (1/6250 ### Separate the information table into sample ID and dilution columns tidy_elisa_data &lt;- separate(elisa_data, col = &quot;information&quot;, into = c(&quot;sample_id&quot;, &quot;dilution&quot;), sep = &quot;\\\\(&quot;) ## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 2 rows [1, 2]. head(tidy_elisa_data) ## # A tibble: 6 × 4 ## well_id od_450nm sample_id dilution ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 A1 0.052 &quot;blank&quot; &lt;NA&gt; ## 2 A2 0.05 &quot;secondary&quot; &lt;NA&gt; ## 3 A3 0.069 &quot;naïve &quot; 1/250) ## 4 A4 0.063 &quot;1A-1 &quot; 1/250 ## 5 A5 0.061 &quot;1A-1 &quot; 1/1250 ## 6 A6 0.122 &quot;1A-1 &quot; 1/6250 tidy_elisa_data &lt;- tidy_elisa_data %&gt;% mutate(dilution = str_extract(dilution, &quot;(/)[0-9]+&quot;), dilution = str_replace(dilution, &quot;/&quot;, &quot;&quot;), dilution = as.numeric(dilution)) tidy_elisa_data &lt;- tidy_elisa_data %&gt;% select(well_id, sample_id, dilution, od_450nm) head(tidy_elisa_data) ## # A tibble: 6 × 4 ## well_id sample_id dilution od_450nm ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A1 &quot;blank&quot; NA 0.052 ## 2 A2 &quot;secondary&quot; NA 0.05 ## 3 A3 &quot;naïve &quot; 250 0.069 ## 4 A4 &quot;1A-1 &quot; 250 0.063 ## 5 A5 &quot;1A-1 &quot; 1250 0.061 ## 6 A6 &quot;1A-1 &quot; 6250 0.122 References "],["flow-cytometry.html", "Chapter 7 Flow cytometry 7.1 Loading packages 7.2 panel information 7.3 Loading data 7.4 Making the data tidy for plotting 7.5 boxplot", " Chapter 7 Flow cytometry Flow cytometry data can be quantified in many different ways and with different techniques. For the purpose of these data analyses, manual gating has been achieved in FlowJo and cell frequencies and populations exported as a .csv file. This .csv file is the primary input for this R pipeline which aims to output box plots for each gated cell population. This example data set is from an innate response study whcih investigated the immune response in the lungs during the first 28 days of infection. 7.1 Loading packages library(readxl) library(ggplot2) library(RColorBrewer) library(dplyr) library(tidyverse) library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor library(stringr) library(tidyr) library(knitr) library(forcats) library(broom) library(ggfortify) library(stats) library(ggpubr) library(grDevices) library(rstatix) ## ## Attaching package: &#39;rstatix&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter library(writexl) 7.2 panel information # antibody_panel &lt;- read_excel 7.3 Loading data Df &lt;- read_excel(&quot;DATA/innate_normalizedto45.xlsx&quot;, sheet = &quot;CD3CD11b No Day 14&quot;) marker_legend &lt;- read_excel(&quot;DATA/marker legend.xlsx&quot;) # Remove Freq of Parent columns Df1 &lt;- Df %&gt;% select(-matches(&quot;Parent&quot;)) # Remove &quot;Leukocytes/LIVE/Single Cells/&quot; from col names names(Df1) &lt;- str_remove(names(Df1), &quot;Leukocytes/LIVE/Single Cells/&quot;) Df1 &lt;- Df1 %&gt;% rename_all(funs(str_replace(., &quot;\\\\|.+&quot;, &quot;&quot;)))# Remove &quot;|Freq of...&quot; from col names ## Warning: `funs()` was deprecated in dplyr 0.8.0. ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. Df1 &lt;- Df1 %&gt;% rename_all(funs(str_replace_all(., &quot;\\\\/Q[:digit:]+\\\\:&quot;, &quot;&quot;))) %&gt;% rename_all(funs(str_replace(., &quot;\\\\/&quot;, &quot; &quot;))) %&gt;% rename_all(funs(str_replace(., &quot;\\\\,&quot;, &quot; &quot;))) %&gt;% rename_all(funs(str_replace(., &quot;\\\\ \\\\,&quot;, &quot; &quot;))) # str_extract_all(names(Df1), &quot;[:alpha:]+[:digit:]+[\\\\+\\\\-]&quot;) # # # # # # # marker_select &lt;- function(col_title) { # marker_df &lt;- str_detect(names(DATA1), &quot;[\\\\+\\\\-]&quot;) # return(marker_df) # } 7.4 Making the data tidy for plotting tidy_Df1 &lt;- pivot_longer(data = Df1, cols = starts_with(&quot;CD45+&quot;), names_to = &quot;cell_types&quot;, values_to = &quot;percentage_of_CD45&quot;) tidy_Df1 &lt;- tidy_Df1 %&gt;% separate(col = &quot;SAMPLE&quot;, into = c(&quot;day&quot;, &quot;replicate&quot;)) tidy_Df1 %&gt;% select(cell_types) %&gt;% unique() ## # A tibble: 128 × 1 ## cell_types ## &lt;chr&gt; ## 1 &quot;CD45+ &quot; ## 2 &quot;CD45+ CD3- CD11b+ &quot; ## 3 &quot;CD45+ CD3- CD11b+ CD25+ &quot; ## 4 &quot;CD45+ CD3- CD11b+ CD103+ &quot; ## 5 &quot;CD45+ CD3- CD11b+ gamma_delta &quot; ## 6 &quot;CD45+ CD3- CD11b+ NKp46+ &quot; ## 7 &quot;CD45+ CD3- CD11b+ CD11c+ CD64- &quot; ## 8 &quot;CD45+ CD3- CD11b+ CD11c- CD64- &quot; ## 9 &quot;CD45+ CD3- CD11b+ CD86- CD64+ &quot; ## 10 &quot;CD45+ CD3- CD11b+ CD86+ CD64+ &quot; ## # … with 118 more rows tidy_Df1 &lt;- tidy_Df1 %&gt;% filter(percentage_of_CD45 &gt; 0.005) head(tidy_Df1, n=10) ## # A tibble: 10 × 4 ## day replicate cell_types percentage_of_CD45 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 CNT 1 &quot;CD45+ &quot; 82.9 ## 2 CNT 1 &quot;CD45+ CD3- CD11b+ &quot; 29.3 ## 3 CNT 1 &quot;CD45+ CD3- CD11b+ CD25+ &quot; 0.88 ## 4 CNT 1 &quot;CD45+ CD3- CD11b+ CD103+ &quot; 0.75 ## 5 CNT 1 &quot;CD45+ CD3- CD11b+ gamma_delta &quot; 4.77 ## 6 CNT 1 &quot;CD45+ CD3- CD11b+ NKp46+ &quot; 7.3 ## 7 CNT 1 &quot;CD45+ CD3- CD11b+ CD11c+ CD64- &quot; 3.65 ## 8 CNT 1 &quot;CD45+ CD3- CD11b+ CD11c- CD64- &quot; 24.3 ## 9 CNT 1 &quot;CD45+ CD3- CD11b+ CD86- CD64+ &quot; 0.43 ## 10 CNT 1 &quot;CD45+ CD3- CD11b+ CD86+ CD64+ &quot; 0.85 # Select CD3 &amp; CD11b populations and create new data frames CD3pos_CD11bneg &lt;- tidy_Df1 %&gt;% filter(str_detect(cell_types, &quot;CD3\\\\+ + CD11b\\\\-&quot;)) CD3neg_CD11bpos &lt;- tidy_Df1 %&gt;% filter(str_detect(cell_types, &quot;CD3\\\\- + CD11b\\\\+&quot;)) CD3neg_CD11bneg &lt;- tidy_Df1 %&gt;% filter(str_detect(cell_types, &quot;CD3\\\\- + CD11b\\\\-&quot;)) 7.5 boxplot CD3pos_CD11bneg_bar_plot &lt;- CD3pos_CD11bneg %&gt;% mutate(day = fct_relevel(day, &quot;CNT&quot;, &quot;D3&quot;, &quot;D7&quot;, &quot;D28&quot;)) %&gt;% ggplot(aes(x = day, y = percentage_of_CD45, fill= day)) + stat_boxplot( aes(day, percentage_of_CD45), geom=&#39;errorbar&#39;, linetype=1, width=0.5)+ geom_boxplot(aes(day, percentage_of_CD45)) + facet_wrap(~cell_types, scale = &quot;free_y&quot;, labeller = label_wrap_gen(width=15), ncol = 5, nrow = 20) + theme_bw() + theme(axis.text.x = element_blank(), axis.text.y = element_text(size = 20), axis.title.x = element_text(size = 20, face = &quot;bold&quot;), axis.title.y = element_text(size = 20, face = &quot;bold&quot;), legend.text = element_text(size = 20), legend.title = element_text(size = 20), plot.title = element_text(color=&quot;black&quot;, size=30, face=&quot;bold&quot;)) + labs (y=&quot;Percentage of CD45&quot;, x = &quot;Day&quot;) + theme(strip.text = element_text(size=12, face = &quot;bold&quot;)) + theme(legend.position=&quot;bottom&quot;) + ggtitle(&quot;Changes in immune cell populations (lung) CD3+ CD11b-&quot;) + stat_compare_means(label = &quot;p.signif&quot;, method = &quot;t.test&quot;, ref.group = &quot;CNT&quot;) CD3neg_CD11bpos_bar_plot &lt;- CD3neg_CD11bpos %&gt;% mutate(day = fct_relevel(day, &quot;CNT&quot;, &quot;D3&quot;, &quot;D7&quot;, &quot;D28&quot;)) %&gt;% ggplot(aes(x = day, y = percentage_of_CD45, fill= day)) + stat_boxplot( aes(day, percentage_of_CD45), geom=&#39;errorbar&#39;, linetype=1, width=0.5)+ geom_boxplot( aes(day, percentage_of_CD45)) + facet_wrap(~cell_types, scale = &quot;free_y&quot;, labeller = label_wrap_gen(width=15), ncol = 5, nrow = 20) + theme_bw() + theme(axis.text.x = element_blank(), axis.text.y = element_text(size = 20), axis.title.x = element_text(size = 20, face = &quot;bold&quot;), axis.title.y = element_text(size = 20, face = &quot;bold&quot;), legend.text = element_text(size = 20), legend.title = element_text(size = 20), plot.title = element_text(color=&quot;black&quot;, size=30, face=&quot;bold&quot;)) + labs (y=&quot;Percentage of CD45&quot;, x = &quot;Day&quot;) + theme(strip.text = element_text(size=12, face = &quot;bold&quot;)) + theme(legend.position=&quot;bottom&quot;) + ggtitle(&quot;Changes in immune cell populations (lung) CD3- CD11b+&quot;) + stat_compare_means(label = &quot;p.signif&quot;, method = &quot;t.test&quot;, ref.group = &quot;CNT&quot;) CD3neg_CD11bneg_bar_plot &lt;- CD3neg_CD11bneg %&gt;% mutate(day = fct_relevel(day, &quot;CNT&quot;, &quot;D3&quot;, &quot;D7&quot;, &quot;D28&quot;)) %&gt;% ggplot(aes(x = day, y = percentage_of_CD45, fill= day)) + stat_boxplot( aes(day, percentage_of_CD45), geom=&#39;errorbar&#39;, linetype=1, width=0.5)+ geom_boxplot( aes(day, percentage_of_CD45)) + facet_wrap(~cell_types, scale = &quot;free_y&quot;, labeller = label_wrap_gen(width=15), ncol = 5, nrow = 20) + theme_bw() + theme(axis.text.x = element_blank(), axis.text.y = element_text(size = 20), axis.title.x = element_text(size = 20, face = &quot;bold&quot;), axis.title.y = element_text(size = 20, face = &quot;bold&quot;), legend.text = element_text(size = 20), legend.title = element_text(size = 20), plot.title = element_text(color=&quot;black&quot;, size=30, face=&quot;bold&quot;)) + labs (y=&quot;Percentage of CD45&quot;, x = &quot;Day&quot;) + theme(strip.text = element_text(size=12, face = &quot;bold&quot;)) + theme(legend.position=&quot;bottom&quot;) + ggtitle(&quot;Changes in immune cell populations (lung) CD3- CD11b-&quot;) + stat_compare_means(label = &quot;p.signif&quot;, method = &quot;t.test&quot;, ref.group = &quot;CNT&quot;) CD3pos_CD11bneg_bar_plot # CD3neg_CD11bpos_bar_plot # CD3neg_CD11bneg_bar_plot "],["pathology.html", "Chapter 8 Pathology", " Chapter 8 Pathology "],["proteomics.html", "Chapter 9 Proteomics", " Chapter 9 Proteomics For proteomics data, we will be getting data that have already been collected and pre-processed by another part of the team. The following shows an example of the type of data we will get as an input: library(tidyverse) prot_a &lt;- read_csv(&quot;DATA/Transition Results_CCTSI_A.csv&quot;) ## Rows: 3393 Columns: 18 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (7): Peptide, Protein, Replicate, Fragment Ion, Ratio Dot Product, Tota... ## dbl (11): Precursor Mz, Precursor Charge, Product Mz, Product Charge, Retent... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. prot_a ## # A tibble: 3,393 × 18 ## Peptide Protein Replicate `Precursor Mz` `Precursor Char…` `Product Mz` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 QELDEISTNIR Cfp10 091322_LT1 659. 2 1061. ## 2 QELDEISTNIR Cfp10 091322_LT2 659. 2 1061. ## 3 QELDEISTNIR Cfp10 091322_LT3 659. 2 1061. ## 4 QELDEISTNIR Cfp10 091322_LT4 659. 2 1061. ## 5 QELDEISTNIR Cfp10 091322_LT5 659. 2 1061. ## 6 QELDEISTNIR Cfp10 091322_LT6 659. 2 1061. ## 7 QELDEISTNIR Cfp10 091322_LT7 659. 2 1061. ## 8 QELDEISTNIR Cfp10 091322_LT8 659. 2 1061. ## 9 QELDEISTNIR Cfp10 091322_LT10 659. 2 1061. ## 10 QELDEISTNIR Cfp10 091322_LT11 659. 2 1061. ## # … with 3,383 more rows, and 12 more variables: `Product Charge` &lt;dbl&gt;, ## # `Fragment Ion` &lt;chr&gt;, `Retention Time` &lt;dbl&gt;, Area &lt;dbl&gt;, Background &lt;dbl&gt;, ## # `Peak Rank` &lt;dbl&gt;, `Ratio Dot Product` &lt;chr&gt;, ## # `Total Area Normalized` &lt;chr&gt;, `Total Area Ratio` &lt;chr&gt;, ## # `Library Dot Product` &lt;dbl&gt;, RatioLightToHeavy &lt;dbl&gt;, ## # DotProductLightToHeavy &lt;dbl&gt; These data include the following columns: Peptide: A short string of peptides that are being measured Protein: The protein that those peptides come from Replicate: An identifier for the sample that the measurement was taken on Precursor Mz, Precursor Charge, Product Mz, Product Charge, Fragment Ion, Retention Time: Measurements that help in identifying the peptide that is being measured (?) Area: Background: Peak Rank: Ratio Dot Product: Total Area Normalized: Total Area Ratio Library Dot Product: RatioLightToHeavy: DotProductLightToHeavy: [More about how these data were pre-processed. Softwarei: Skyline] Here are all the unique replicates in this file: prot_a %&gt;% pull(Replicate) %&gt;% unique() ## [1] &quot;091322_LT1&quot; &quot;091322_LT2&quot; &quot;091322_LT3&quot; &quot;091322_LT4&quot; &quot;091322_LT5&quot; ## [6] &quot;091322_LT6&quot; &quot;091322_LT7&quot; &quot;091322_LT8&quot; &quot;091322_LT10&quot; &quot;091322_LT11&quot; ## [11] &quot;091322_LT12&quot; &quot;091322_LT13&quot; &quot;091322_LT14&quot; &quot;091322_H1&quot; &quot;091322_H2&quot; ## [16] &quot;091322_H3&quot; &quot;091322_H4&quot; &quot;091322_H5&quot; &quot;091322_H6&quot; &quot;091322_H7&quot; ## [21] &quot;091322_H8&quot; &quot;091322_H9&quot; &quot;091322_H10&quot; &quot;091322_H11&quot; &quot;091322_H12&quot; ## [26] &quot;091322_H13&quot; &quot;091322_H14&quot; &quot;091322_TB1&quot; &quot;091322_TB2&quot; &quot;091322_TB3&quot; ## [31] &quot;091322_TB4&quot; &quot;091322_TB5&quot; &quot;091322_TB6&quot; &quot;091322_TB7&quot; &quot;091322_TB8&quot; ## [36] &quot;091322_TB9&quot; &quot;091322_TB10&quot; &quot;091322_TB11&quot; &quot;091322_TB12&quot; The three groups in this data are labeled with “LT,” “H,” and “TB” somewhere in the identifier. We can create a new column in the dataset that pulls out this treatment group information: prot_a &lt;- prot_a %&gt;% mutate(treatment_group = str_extract(Replicate, &quot;[A-Z]+&quot;)) prot_a %&gt;% filter(Peptide == first(Peptide)) %&gt;% group_by(treatment_group) %&gt;% count() ## # A tibble: 3 × 2 ## # Groups: treatment_group [3] ## treatment_group n ## &lt;chr&gt; &lt;int&gt; ## 1 H 140 ## 2 LT 130 ## 3 TB 120 prot_a %&gt;% filter(Peptide == first(Peptide) &amp; Replicate == first(Replicate)) ## # A tibble: 10 × 19 ## Peptide Protein Replicate `Precursor Mz` `Precursor Charge` `Product Mz` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 QELDEISTNIR Cfp10 091322_LT1 659. 2 1061. ## 2 QELDEISTNIR Cfp10 091322_LT1 659. 2 832. ## 3 QELDEISTNIR Cfp10 091322_LT1 659. 2 703. ## 4 QELDEISTNIR Cfp10 091322_LT1 659. 2 590. ## 5 QELDEISTNIR Cfp10 091322_LT1 659. 2 503. ## 6 QELDEISTNIR Cfp10 091322_LT1 664. 2 1071. ## 7 QELDEISTNIR Cfp10 091322_LT1 664. 2 842. ## 8 QELDEISTNIR Cfp10 091322_LT1 664. 2 713. ## 9 QELDEISTNIR Cfp10 091322_LT1 664. 2 600. ## 10 QELDEISTNIR Cfp10 091322_LT1 664. 2 513. ## # … with 13 more variables: `Product Charge` &lt;dbl&gt;, `Fragment Ion` &lt;chr&gt;, ## # `Retention Time` &lt;dbl&gt;, Area &lt;dbl&gt;, Background &lt;dbl&gt;, `Peak Rank` &lt;dbl&gt;, ## # `Ratio Dot Product` &lt;chr&gt;, `Total Area Normalized` &lt;chr&gt;, ## # `Total Area Ratio` &lt;chr&gt;, `Library Dot Product` &lt;dbl&gt;, ## # RatioLightToHeavy &lt;dbl&gt;, DotProductLightToHeavy &lt;dbl&gt;, ## # treatment_group &lt;chr&gt; prot_a %&gt;% pull(Protein) %&gt;% unique() ## [1] &quot;Cfp10&quot; &quot;acpM&quot; &quot;Ag85A&quot; ## [4] &quot;MtbH37Rv|Rv3841|BfrB&quot; &quot;MtbH37Rv|Rv1837c|GlcB&quot; &quot;MtbH37Rv|Rv3418c|GroES&quot; ## [7] &quot;MtbH37Rv|Rv3248c|SahH&quot; &quot;MtbH37Rv|Rv2031c|hspX&quot; Cfp10 acpM Ag85A MtbH37Rv|Rv3841|BfrB MtbH37Rv|Rv1837c|GlcB MtbH37Rv|Rv3418c|GroES MtbH37Rv|Rv3248c|SahH MtbH37Rv|Rv2031c|hspX "],["references.html", "References 9.1 Download Microsoft365R package 9.2 Load the package 9.3 Set up teams via R", " References 9.1 Download Microsoft365R package install.packages(&quot;Microsoft365R&quot;) install.packages(&quot;AzureGraph&quot;) install.packages(&quot;RODBC&quot;) 9.2 Load the package library(Microsoft365R) library(AzureGraph) library(AzureAuth) 9.3 Set up teams via R my_team &lt;- get_team(&quot;CVMBS-MIP IMPAc-TB&quot;, app = &quot;04b07795-8ddb-461a-bbee-02f9e1bf7b46&quot;) # get the channel channels &lt;- my_team$get_channel() # get the folder folder &lt;- channels$get_folder() # list files files &lt;- folder$list_files() # open weight file body_weight &lt;- folder$get_item(&quot;Mouse_labeling_and_weighing/Body Weights.xlsx&quot;) body_weight$open() body_weight$download(overwrite=TRUE) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
