[["index.html", "ImpactTB/BAA: Standard Operating Procedures for Data Analysis Chapter 1 Overview 1.1 About the project", " ImpactTB/BAA: Standard Operating Procedures for Data Analysis Colorado State University Coding Team 2024-12-05 Chapter 1 Overview 1.1 About the project The objective of the Immune Mechanisms of Protection against Mycobacterium tuberculosis (IMPAc-TB) program is to improve understanding of the immune responses necessary to avoid initial infection with Mycobacterium tuberculosis (Mtb), formation of latent infection, and progression to active TB illness. To achieve these goals, the National Institute of Allergy and Infectious Diseases awarded substantial funding and established multidisciplinary research teams that will analyze immune responses against Mtb in animal models (mice, guinea pigs, and non-human primates) and humans, as well as immune responses elicited by promising vaccine candidates. This online book provides data collection templates and pre-processing protocols for data types collected at the CSU branch of the Phoenix IMPAc-TB program. By using standard templates to record data, as well as starting from defined pipelines to process and analyze the data, we aim to standardize the collection and processing of data across this project. "],["animal-initial-conditions-and-weekly-weights.html", "Chapter 2 Animal initial conditions and weekly weights", " Chapter 2 Animal initial conditions and weekly weights 2.0.1 Downloads The downloads for this chapter are: Data collection template for collecting initial information about the experimental animals and regular weight measurements, cage changes, and adverse events throughout the experiment Report template to process data collected with the template (when you go to this link, go to the “File” bar in your browser’s menu bar, chose “Save As”, then save the file as “animal_weights.Rmd”) Example output from the report template 2.0.2 Overview We use the template in this section to record information about each animal used in the experiment. This includes the species, sex, and experimental group. It also includes some information to identify the animal, which in the case of mice includes a code describing the pattern of notches put in the mouse’s ear and the cage that the animal is assigned to at the beginning of the experiment. These are all values that can be determined at the start of the experiment, when the mice are first assigned to groups. This template is also used to record some data over the course of the experiment. This includes adverse events and cases where an animal is moved from one cage to another during the experiment. In addition, in our experiments, we are measuring the mice every week to record their weight over the course of the experiment. This weight measuring begins before the first vaccination and continues through until the last mouse is sacrificed. We have used ear notches to identify each mouse, and between the ear notch and the mouse’s cage number, we can uniquely track each mouse in the study. There are a few reasons that we are measuring these mouse weights. The first is to help us manage the mice, particularly in terms of animal welfare. If there are mice that are losing a lot of weight, that can be an indication that they may need to be euthanized. For example, some animal care standards consider that an adult animal that has lost 20% or more of its weight compared to its baseline weight is indicating a clear sign of morbidity or suffering. A second reason is that the weight measure might provide a record of each mouse’s general health over the course of the study. In the study, mice are weighed in grams weekly to monitor clinical status, as one potential sign of tuberculosis infection and severity is weight loss. In humans, tuberculosis patients frequently display weight loss as a clinical symptom associated with disease progression. In particular, extreme weight loss and loss of muscle mass, also known as cachexia, can present as a result of chronic inflammatory illnesses like tuberculosis (Baazim, Antonio-Herrera, and Bergthaler 2022). This cachexia is part of a systemic response to inflammation, and in humans has been linked to upregulation of pro-inflammatory cytokines including tumor necrosis factor, interleukin-6, and interferon-gamma (Baazim, Antonio-Herrera, and Bergthaler 2022). Additionally, studies support a role in cachexia of key immune cell populations such as cytotoxic T-cells which, when depleted, counteract muscle and fat deterioration (Baazim et al. 2019), suggest that thsi type of T-cells may metabolically reprogram adipose tissue. Given these relationships between weight loss, diseases, and immune processes, it is possible that mouse weight might provide a regularly measurable insight into the severity of disease in each animal. While many of data points are collected to measure the final disease state of each animal, fewer are available before the animal is sacrificed. We are hoping that mouse weights will provide one measure that, while it may not perfectly capture disease severity, may provide some information throughout the experiment that is correlated to disease severity at regular time intervals. Other studies that use a mouse model of tuberculosis have collected mouse weights, as well (Smith et al. 2022; Segueni et al. 2016). We plan to investigate these data to visualize the trajectory of weight gain / loss in each mouse both before and after they are challenged with tuberculosis. We also plan to test whether each mouse’s weight change after challenge is correlated with other metrics of the severity of disease and immune response. We will do this by testing the correlation between the percent change in weight between challenge and sacrifice with CFUs at sacrifice as well as expression of cytokines and other biological markers (Smith et al. 2022). 2.0.3 Template description Both the animals’ initial conditions and their weekly measures (adverse events, cage changes, and weights) should be recorded in an excel worksheet. You can download a copy of the template here. The worksheet is divided into sheets. The first sheet is recorded at the first time point when the mice are measured and is used to record information about the mice that will remain unchanged over the course of the study, like species and sex. Here is what the first sheet of the template looks like: The second and later sheets are used to record the weight at each measured timepoint. The second sheet will record the weights on the first date they are measured, so it should be recorded at the same time as the first sheet—with initial mouse information—is completed. Here is what the first sheet of the template looks like: As you continue to measure at new timepoints, you should add a sheet at each timepoint, with each new sheet following the format of the second sheet in the template. The second and later sheets should be labeled with the date when those weights were measured (e.g., “5.26.22” for weights measured on May 26, 2022). When you download the template, it will have example values filled out in blue. Use these to get an idea for how to record your own data. When you are ready to record your own data, delete these example values and replace them with data collected from your own experiment. Column titles are as follows. First, in the first sheet, you will record: notch_id: Record the ear notch pattern in the mouse. Make sure that you record consistently across all timepoints, so that each mouse can be tracked across dates. If you are doing single notches, for example, this might be “0” for no notches, “1R” for one notch in the right ear, “1L” for one notch in the left ear, and “1R1L” for one notch in each ear. starting_cage_number: Record the number of the cage that the mouse is put into at the start of the experiment. In combination with the mouse’s notch_id, this will provide a unique identifier for each mouse at the start of the experiment. dob: Record the date the mouse was born. species: Record the species of the mouse (e.g., “C57BL/6” for C57 black 6 mice or “CBA” for CBA mice). sex: Record as “m” for male or “f” for female group: Provide the experimental group of the mouse. Be sure that you use the same abbreviation or notation across each timepoint. Examples of group designations might be: bcg, saline, bcg+id93, saline+id93, saline+noMtb For the second and later sheets, you will record: who_collected: Record the first name of the person who actually handled the mouse from the scale. date_collected: Record the date using quotation marks, with the month, then day, then year. For example, “May 31, 2022”. weight: Record as a number, without a unit in this column. The next column will be used for the units. unit: Provide the units that were used to take the weight (e.g., “g” for grams). Be consistent across all animals and timepoints in the abbreviation that you use (e.g., always use “g” for grams, not “g” sometimes and “grams” sometimes) existing_cage_number: Provide the cage number that the mouse is in when you start weighing at that time point. If the mouse is moved to another cage on this day, you will specify that in the next column. If the animal was moved from one cage to another between the last weighing and the date of the timepoint you are measuring, put in this column the cage number that the animal was in the last time it was weighed. new_cage_number: If the animal is moved to a new cage on the date of the timepoint you are measuring, then use this column to record the number of the cage you move it too. Similarly, if the animal moved cages between the last measured timepoint and this one, use this column to record the cage it was moved to. Otherwise, if the animal stays in the same cage that it was at the last measured time point, leave this column empty. group: Provide the experimental group of the mouse. Be sure that you use the same abbreviation or notation across each timepoint. Examples of group designations might be: bcg, saline, bcg+id93, saline+id93, saline+noMtb notes: Record information regarding clinical observations (e.g., “back is balding”, “barbering”, “excessive grooming”, “euthanized”). 2.0.4 Processing collected data Once data are collected, the file can be run through an R workflow. This workflow will convert the data into a format that is easier to work with for data analysis and visualization. It will also produce a report on the data in the spreadsheet, and ultimately it will also write relevant results in a format that can be used to populate a global database for all experiments in the project. The next section provides the details of the pipeline. It aims to explain the code that processes the data and generates visualizations. You do not need to run this code step-by-step, but instead can access a script with the full code here. To use this reporting template, you need to download it to your computer and save it in the file directory where you saved the data you collected with the data collection template. You can then open RStudio and navigate so that you are working within this directory. You should also make sure that you have installed a few required packages on R on the computer you are using to run the report. These packages are: tidyverse, purrr, lubridate, readxl, knitr, and ggbeeswarm. Within RStudio, open the report template file. There is one spot where you will need to change the code in the template file, so it will read in the data from the version of the template that you saved, which you may have renamed. In the YAML of the report template file, change the file path beside “data:” so that it is the file name of your data file. Once you’ve made this change, you can use the “Knit” button in RStudio to create a report from the data file and the report template file. The report includes the following elements: Summary table of animals at the start of the experiment Time series plots of animal weights over the experiment, grouped by experimental group Boxplots of the distribution of animal weights within each experimental group at the last available time point Plot of measured weight, identified by the person who was handling the animal, to help determine if there are consistent differences by handler Table of all the animals in the experiment at the last measured time point, ordered by their weight change since the previous measurement. This table is meant to help in identifying animals that may need to be euthanized for animal welfare reasons. You can download an example of a report created using this template by clicking here. When you knit to create the report, it will create a Word file in the same file directory where you put your data file and report template. It will also create and output a version of the data that has been processed (in the case of the weights data, this mainly involves tracking mice as they change cages, to link all weights that are from a single animal). This output fill will be named “mouse_weights_output.csv” and, like the report file, will be saved in the same file directory as the data file and the report template. 2.0.5 Details of processing script This section goes through the code within the report template. It explains each part of the code in detail. You do not need to understand these details to use the report template. However, if you have questions about how the data are being processed, or how the outputs are created, all those details are available in this section. As a note, there are two places in the following code where there’s a small change compared to the report template. In the report, you incorporate the path to the data file using the data: section in the YAML at the top of the document. In the following code, we’ve instead used the path of some example data within this book’s file directory, so the code will run for this chapter as well. First, the workflow loads some additional R libraries. You may need to install these on your local R session if you do not already have them installed. library(readxl) library(tidyverse) library(ggbeeswarm) These packages bring in some useful functions that are not available in the base installation of R. They are all open source. To cite any of them, you can use the citation function. For example, to get the information you would need to cite the readxl package, in R you can run: citation(&quot;readxl&quot;) ## To cite package &#39;readxl&#39; in publications use: ## ## Wickham H, Bryan J (2023). _readxl: Read Excel Files_. R package ## version 1.4.3, &lt;https://CRAN.R-project.org/package=readxl&gt;. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {readxl: Read Excel Files}, ## author = {Hadley Wickham and Jennifer Bryan}, ## year = {2023}, ## note = {R package version 1.4.3}, ## url = {https://CRAN.R-project.org/package=readxl}, ## } Next, the code in the report template creates a few custom functions to help process the data from the data collection template. The first of these functions checks the data collection template to identify all the timepoints that were collected and then reads each in, ultimately joining data from all time points into one large dataset. The data collection template requires you to use a new sheet in the spreadsheet for each weight collection time point, with a first sheet that records initial information about the animals. If you only take weights at three time points, there would only be three time point sheets in the final file. Conversely, if you collect weight data at twenty time points, there would be twenty sheets in the final file. The first function, called ``, reads the data file, checks to find all the weight recording sheets, whether it’s three or twenty, and then reads the data in from all the sheets and binds them together into a single dataframe. ## Function to read in mouse weights. This takes a filepath to an Excel sheet ## that follows the template of the animal weight collection template. It ## identifies all the sheets in that file and reads in all the ones that ## measure weekly weights. It returns one large dataframe with all of the ## measured weights. read_mouse_weights &lt;- function(filepath) { # getting info about all excel sheets mouse_weights_sheets &lt;- readxl::excel_sheets(filepath)[-1] # First sheet is initial data, not mouse weights mouse_weights &lt;- purrr::map(mouse_weights_sheets, ~ readxl::read_excel(filepath, sheet = .x, col_types = c(&quot;text&quot;, # who_collected &quot;text&quot;, # date_collected &quot;text&quot;, # notch_id &quot;numeric&quot;, # weight &quot;text&quot;, # unit &quot;text&quot;, # existing_cage_number &quot;text&quot;, # new_cage_number &quot;text&quot;, # group &quot;text&quot; # notes ))) %&gt;% dplyr::bind_rows() %&gt;% mutate(date_collected = lubridate::mdy(date_collected)) return(mouse_weights) } The remaining functions are all functions to help track a mouse over the experiment even if it changes cages. In processing this data, the key challenge is to track a single mouse over the experiment. The mice are identified by a pattern of notches in their ears. However, there are a limited number of notches that can be distinguished, so the notch information does not distinctly identify every mouse in the study, just every mouse within a certain cage. By knowing both an ear notch ID and a cage number, you can distinctly identify each mouse in the study. However, mice are moved from one cage to another in some cases during a study. If mice within a cage are fighting, or if they are showing signs of excessive grooming, these can be reasons to move a mouse to a new cage once the experiment has started. The cage moves need to be resolved when processing the data so that each mouse can be tracked even as they move. In the data collection template, we have created a design that aims to include information about cage moves, but to do so in a way that is as simple as possible for the person who is recording the data. The weights are recorded for each time point in a separate sheet of the data collection template. On the sheet for a time point, there are also columns to give the mouse’s cage at the start of that data collection time point, as well as the cage the mouse was moved to, if it was moved. The report template code then uses this information to create a unique ID for each mouse (one that is constant across the experiment), and then attach it to the mouse’s measurements even as the mouse is moved from one cage to another. The following two functions both help with this process: # Function to get the next cage number based on the # existing cage number and notch ID. If the mouse does not # switch cages again, the output is a vector of length 0. # This takes the dataframe and existing identifiers (notch id and # existing cage number) as inputs. It returns the next cage # that the mouse was moved to. If the mouse has not moved # from the existing case, the output has length 0. get_next_cage &lt;- function(existing_cage_number, notch_id, df = our_mouse_weights){ next_cage &lt;- df %&gt;% filter(.data$existing_cage_number == {{existing_cage_number}} &amp; .data$notch_id == {{notch_id}} &amp; !is.na(.data$new_cage_number)) %&gt;% pull(new_cage_number) return(next_cage) } # Function to get the full list of cages for each individual # mouse, over the course of all data collected to date. This # inputs the starting identifiers of the mouse (starting cage ID # and notch ID). It then works through any cage changes to create # a list for that mouse of all cages it was put in over the # course of the experiment. get_mouse_cages &lt;- function(mouse_starting_cage, mouse_notch_id, df = our_mouse_weights){ mouse_cage_list &lt;- mouse_starting_cage i &lt;- 1 while(TRUE){ next_cage &lt;- get_next_cage(existing_cage_number = mouse_cage_list[i], notch_id = mouse_notch_id, df = df) if(length(next_cage) == 0) { break } i &lt;- i + 1 mouse_cage_list[i] &lt;- next_cage } return(mouse_cage_list) } Next, the report template code gets to the workflow itself, where it uses both these custom functions and other R code to process the data and then to provide summaries and visualizations of the data. The first step in the workflow is to read in the data from the spreadsheet. As long as the data are collected following the template that was described earlier, this code should be able to read it in correctly and create a master dataset with the data from all sheets of the spreadsheet. This step of the pipeline uses one of the custom functions that was defined at the start of the report template code: # Read in the mouse weights from the Excel template. This creates one large # dataframe with the weights from all the timepoints. our_mouse_weights &lt;- read_mouse_weights(filepath = &quot;DATA/body_weights_measurement.xlsx&quot;) Next, the code runs through a number of steps to create a unique ID for each mouse and then apply that ID to each time point, even if a mouse changes cages. # Add a unique mouse ID for the first time point. This will become each mouse&#39;s # unique ID across all measured timepoints. our_mouse_weights &lt;- our_mouse_weights %&gt;% mutate(mouse_id = 1:n(), mouse_id = ifelse(date_collected == first(date_collected), mouse_id, NA)) # Create a dataframe that lists all mice at the first time point, # as well as a list of all the cages they have been in over the # experiment mice_cage_lists &lt;- our_mouse_weights %&gt;% filter(date_collected == first(date_collected)) %&gt;% select(notch_id, existing_cage_number, mouse_id) %&gt;% mutate(cage_list = map2(.x = existing_cage_number, .y = notch_id, .f = ~ get_mouse_cages(.x, .y, df = our_mouse_weights))) # Add a column with the latest cage to the weight dataframe our_mouse_weights$latest_cage &lt;- NA # Loop through all the individual mice, based on mice with a # measurement at the first time point. Add the unique ID for # each mouse, which will apply throughout the experiment. Also # add the most recent cage ID, so the mouse can be identified # by lab members based on it&#39;s current location for(i in 1:nrow(mice_cage_lists)){ this_notch_id &lt;- mice_cage_lists[i, ]$notch_id this_cage_list &lt;- mice_cage_lists[i, ]$cage_list[[1]] this_unique_id &lt;- mice_cage_lists[i, ]$mouse_id latest_cage &lt;- this_cage_list[length(this_cage_list)] our_mouse_weights$mouse_id[our_mouse_weights$notch_id == this_notch_id &amp; our_mouse_weights$existing_cage_number %in% this_cage_list] &lt;- this_unique_id our_mouse_weights$latest_cage[our_mouse_weights$notch_id == this_notch_id &amp; our_mouse_weights$existing_cage_number %in% this_cage_list] &lt;- latest_cage } # Add a label for each mouse based on its notch_id and latest cage our_mouse_weights &lt;- our_mouse_weights %&gt;% mutate(mouse_label = paste(&quot;Cage:&quot;, latest_cage, &quot;Notch:&quot;, notch_id)) Ultimately, this creates both a unique ID for each mouse (in a column of the dataframe called mouse_id), as well as creates a unique label that can be used in plots and tables (given in the mouse_label column). The unique ID is set at the beginning of the study for each mouse and remains the same throughout the study. The label, on the other hand, is based on the mouse’s ear notch pattern and the most recent cage it was recorded to be in. We made this choice for a labeling identifier, because it will help the researchers to quickly identify a mouse in the study based on it’s current, rather than starting, cage. The next part of the code reads in the initial data that were recorded for each animal in the experiment. The code then pulls in information from the processed weights dataset to match these initial data with each animals unique ID. Ultimately, these starting data are incorporated into the large dataset of mouse weights, creating a single large dataset to work with (our_mouse_weights) that includes all the information that was recorded in the data collection template. # Read in the data from the original file with the initial animal # characteristics mouse_initial &lt;- readxl::read_excel(&quot;DATA/body_weights_measurement.xlsx&quot;, sheet = 1, col_types = c(&quot;text&quot;, # notch_id &quot;text&quot;, # starting_cage_number &quot;text&quot;, # dob &quot;text&quot;, # species &quot;text&quot;, # sex &quot;text&quot; # group )) %&gt;% mutate(dob = lubridate::mdy(dob), sex = forcats::as_factor(sex)) # Figure out the starting cage for each mouse, so they can be incorporated # with the initial data so we can get the mouse ID that was added for the # starting time point mouse_ids &lt;- our_mouse_weights %&gt;% filter(date_collected == first(date_collected)) %&gt;% select(notch_id, existing_cage_number, mouse_id) %&gt;% rename(starting_cage_number = existing_cage_number) # Merge in the mouse IDs with the dataframe of initial mouse characteristics mouse_initial &lt;- mouse_initial %&gt;% left_join(mouse_ids, by = c(&quot;notch_id&quot;, &quot;starting_cage_number&quot;)) # Join the initial data with the weekly weights data into one large dataset our_mouse_weights &lt;- our_mouse_weights %&gt;% left_join(mouse_initial, by = c(&quot;mouse_id&quot;, &quot;notch_id&quot;, &quot;group&quot;)) At this point, the first few rows of this large dataset look like this: our_mouse_weights %&gt;% slice(1:5) ## # A tibble: 5 × 16 ## who_collected date_collected notch_id weight unit existing_cage_number ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Taru 2022-05-26 0 18.4 g 22003 ## 2 Taru 2022-05-26 1R 17.2 g 22003 ## 3 Taru 2022-05-26 1L 17 g 22003 ## 4 Taru 2022-05-26 1R1L 18.8 g 22003 ## 5 Taru 2022-05-26 0 18.4 g 22004 ## # ℹ 10 more variables: new_cage_number &lt;chr&gt;, group &lt;chr&gt;, notes &lt;chr&gt;, ## # mouse_id &lt;int&gt;, latest_cage &lt;chr&gt;, mouse_label &lt;chr&gt;, ## # starting_cage_number &lt;chr&gt;, dob &lt;date&gt;, species &lt;chr&gt;, sex &lt;fct&gt; The rest of the code in the report template will create summaries and graphs of the data. First, there is some code that provides summaries of the research animals at the start of the experiment. It uses the mouse_initial dataset (which pulled in data from the first sheet of the data collection template). It uses a summarize call to summarize details from this sheet of data, including the species of the animal, the total number of animals, how many were males versus females, and which experimental groups were included. It uses some additional code to format the data so the resulting table will be clearer, and then uses the kable function to output the results as a nicely formatted table. # Create a table that summarizes the animals at the start of the experiment mouse_initial %&gt;% summarize(Species = paste(unique(species), collapse = &quot;, &quot;), `Total animals` = n(), `Sex distribution` = paste0(&quot;male: &quot;, sum(sex == &quot;m&quot;), &quot;, female: &quot;, sum(sex == &quot;f&quot;)), `Experimental groups` = paste(unique(group), collapse = &quot;, &quot;), `N. of starting cages` = length(unique(starting_cage_number))) %&gt;% mutate_all(as.character) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = paste0(name, &quot;:&quot;)) %&gt;% knitr::kable(col.names = c(&quot;&quot;, &quot;&quot;), caption = &quot;Summary of experimental animals at the start of the experiment&quot;, align = c(&quot;r&quot;, &quot;l&quot;)) Table 2.1: Summary of experimental animals at the start of the experiment Species: C57BL/6 Total animals: 140 Sex distribution: male: 60, female: 80 Experimental groups: bcg, bcg+id93, saline, saline+id93, saline+noMtb N. of starting cages: 34 The next piece of code creates a time series of mouse weights over time. The points for each mouse are connected to create a line, so it’s easy to see both variation across mice at a single time point and variation in a single mouse over the study. The lines are colored to distinguish male from female mouse (and there is a clear difference in average weights in the two groups). The plot is faceted so that the time series for mice in each experimental group are shown in different small “facets” of the plot, but with the same axis ranges used on each small plot to help comparisons across plots. # Create a plot of mouse weights over time our_mouse_weights %&gt;% ggplot(aes(x = date_collected, y = weight, group = mouse_id, color = sex)) + geom_line() + facet_wrap(~ group) + ggtitle(&quot;Animal weights over time by experiment group&quot;) + labs(x = &quot;Date collected&quot;, y = &quot;Weight (g)&quot;) Next, the code creates boxplots that focus on differences in weights at the latest available timepoint. One boxplot is created for each experimental group, and the points for individual mice are shown behind the boxplot, to provide a better idea of the pattern of variation in individual mice. These points are colored based on sex, to help explore patterns by sex. # Plot animal weight boxplots for the latest time point our_mouse_weights %&gt;% filter(date_collected == last(date_collected)) %&gt;% ggplot(aes(x = group, y = weight)) + geom_beeswarm(aes(color = sex)) + geom_boxplot(fill = NA, color = &quot;dodgerblue&quot;) + ggtitle(&quot;Animal weights at last collection by experimental group&quot;) + labs(x = &quot;Experimental group&quot;, y = &quot;Weight (g)&quot;) The next piece of code shows how mouse weights vary by the person who was handling the animals at a certain time point. Different handlers may have small differences in how they handle and weight the mice. If there are noticable differences in the measured weights, this is something that could be corrected through statisical modeling in later analysis, so we included it as a potential check. # Plot animal weights by animal handler our_mouse_weights %&gt;% ggplot(aes(x = date_collected, y = weight, color = who_collected)) + geom_point() + ggtitle(&quot;Animal weights by animal handler&quot;) + labs(x = &quot;Date collected&quot;, y = &quot;Weight (g)&quot;, color = &quot;Person who\\nhandled the\\nanimal&quot;) The next piece of code creates a table with each of the animals that was still tracked at the last time point (if animals were sacrificed prior to the last recorded time point, they would not be included here). This table focuses on the weight change since the previous measured time point. It is ordered by the change in weight, from the largest decrease to the largest increase. It is meant as an aide in identifying mice that are showing signs of suffering and may need to be considered for being euthanized. The animals are labeled in this table by their most recent cage location, so it will be easier to find them if necessary. For this example code, we’ve shown only a sample of 15 animals, but the report will show data for all animals. # Create table of animal weight changes since previous time point our_mouse_weights %&gt;% select(date_collected, weight, group, mouse_label, sex) %&gt;% group_by(mouse_label) %&gt;% mutate(weight_change = (weight - lag(weight)) / lag(weight)) %&gt;% ungroup() %&gt;% filter(date_collected == last(date_collected)) %&gt;% mutate(formatted_weight_change = paste0(formatC(weight_change * 100, digits = 1, format = &quot;f&quot;), &quot;%&quot;)) %&gt;% arrange(weight_change) %&gt;% select(mouse_label, group, sex, weight, formatted_weight_change) %&gt;% slice(1:15) %&gt;% # Only for the chapter--show a sample, not all knitr::kable(col.names = c(&quot;Mouse&quot;, &quot;Experimental group&quot;, &quot;Sex&quot;, &quot;Weight (g)&quot;, &quot;Weight change since last measure&quot;), caption = &quot;Individual data on weight changes in mice between current measurement and previous measurement.&quot;) Table 2.2: Individual data on weight changes in mice between current measurement and previous measurement. Mouse Experimental group Sex Weight (g) Weight change since last measure Cage: 22021 Notch: 1R bcg m 25.20 -10.4% Cage: 22017 Notch: 1R saline f 23.13 -4.1% Cage: 22015 Notch: 1L bcg f 23.18 -3.1% Cage: 22476 Notch: 1R1L saline+noMtb f 20.54 -2.8% Cage: 22014 Notch: 1L saline+id93 f 21.47 -2.8% Cage: 22014 Notch: 0 saline+id93 f 21.40 -2.7% Cage: 22006 Notch: 1R1L bcg+id93 f 21.02 -2.4% Cage: 22015 Notch: 0 bcg f 20.06 -1.0% Cage: 22004 Notch: 1R bcg f 21.42 -1.0% Cage: 22006 Notch: 0 bcg+id93 f 18.67 -0.7% Cage: 22476 Notch: 0 saline+noMtb f 19.42 -0.6% Cage: 22016 Notch: 1R1L bcg+id93 f 23.13 -0.5% Cage: 22012 Notch: 1R saline+id93 f 22.44 -0.4% Cage: 22015 Notch: 2L bcg f 20.56 -0.3% Cage: 22013B Notch: 1L saline+id93 f 20.46 -0.2% As a last step, the code in the template writes a CSV file with the processed data. This file will be an input into a script that will format the data to add to a database where we are collecting and integrating data from all the CSU experiments, and ultimately from there into project-wide storage. # Write out processed data into a CSV file write_csv(our_mouse_weights, &quot;mouse_weights_output.csv&quot;) References Baazim, Hatoon, Laura Antonio-Herrera, and Andreas Bergthaler. 2022. “The Interplay of Immunology and Cachexia in Infection and Cancer.” Nature Reviews Immunology 22 (5): 309–21. Baazim, Hatoon, Martina Schweiger, Michael Moschinger, Haifeng Xu, Thomas Scherer, Alexandra Popa, Suchira Gallage, et al. 2019. “CD8+ t Cells Induce Cachexia During Chronic Viral Infection.” Nature Immunology 20 (6): 701–10. Segueni, Noria, Elaine Tritto, Marie-Laure Bourigault, Stéphanie Rose, François Erard, Marc Le Bert, Muazzam Jacobs, et al. 2016. “Controlled Mycobacterium Tuberculosis Infection in Mice Under Treatment with Anti-IL-17A or IL-17F Antibodies, in Contrast to TNF-Alpha Neutralization.” Scientific Reports 6 (1): 1–17. Smith, Clare M, Richard E Baker, Megan K Proulx, Bibhuti B Mishra, Jarukit E Long, Sae Woong Park, Ha-Na Lee, et al. 2022. “Host-Pathogen Genetic Interactions Underlie Tuberculosis Susceptibility in Genetically Diverse Mice.” Elife 11: e74419. "],["colony-forming-units-to-determine-bacterial-counts.html", "Chapter 3 Colony forming units to determine bacterial counts", " Chapter 3 Colony forming units to determine bacterial counts 3.0.1 Downloads The downloads for this chapter are: Data collection template for recording colony forming units counted on each plate or section of plate in the laboratory Report template to process data collected with the data template (when you go to this link, go to the “File” bar in your browser’s menu bar, chose “Save As”, then save the file as “animal_weights.Rmd”) Example output from the report template 3.0.2 Overview TEST In the experiments, we will need to estimate the bacterial load of Mycobacterium tuberculosis in organs—including lungs and spleens—of animals from experiments. These measurements help us assess how well a vaccine has worked in comparison to controls. We will be estimating bacterial load in an animal organ using the plate count method with serial dilutions. Serial dilutions allow you to create a highly diluted sample without needing a massive amount of diluent: as you increase the dilution one step at a time, you can steadily bring the samples down to lower bacterial loads per volume. This method is common across laboratories that study tuberculosis drug efficacy as a method for estimating bacterial load in animal organs (Franzblau et al. 2012) and is a well-established method across microbiology in general, dating back to Koch in the late 1800s (Wilson 1922; Ben-David and Davidson 2014). With this method, we homogenize part of the organ, and then create several increasingly dilute samples. Each dilution is then spread on a plate with a medium in which Mycobacterium tuberculosis can grow and left to grow for several weeks at a temperature conducive to Mycobacterium tuberculosis growth. The idea is that individual bacteria from the original sample end up randomly spread across the surface of the plate, and any bacteria that are viable (able to reproduce) will form a new colony that, after a while, you’ll be able to see (Wilson 1922; Goldman and Green 2015). At the end of this incubation period, you can count the number of these colony-forming units (CFUs) on each plate. To count the number of CFUs, you need a “just right” dilution (and we often won’t know what this is until after plating) to have a countable plate. If you have too high of a dilution (i.e., one with very few viable bacteria), randomness will play a big role in the CFU count, and you’ll estimate the original with more variability, which isn’t ideal. If you have too low of a dilution (i.e., one with lots of viable bacteria), it will be difficult to identify separate colonies, and they may compete for resources. (The pattern you see when the dilution is too low (i.e., too concentrated with bacteria) is called a lawn—colonies merge together). Once you identify a good dilution for each sample, the CFU count from this dilution can be used to estimate the bacterial load in the animal’s organ. To translate from diluted concentration to original concentration, you do a back-calculation, incorporating both the number of colonies counted at that dilution and how dilute the sample was (Ben-David and Davidson 2014; Goldman and Green 2015). 3.0.3 Template description The data are collected in a spreadsheet with multiple sheets. The first sheet (named “metadata”) is used to record some metadata for the experiment, while the following sheets are used to record CFUs counts from the plates used for samples from each organ, with one sheet per organ. For example, if you plated data from both the lung and spleen, there would be three sheets in the file: one with the metadata, one with the plate counts for the lung, and one with the plate counts for the spleen. The first sheet, which is the metadata sheet, is shown below: This metadata sheet is used to record information about the overall process of plating the data. Values from this sheet will be used in calculating the bacterial load in the original sample based on the CFU counts. This spreadsheet includes the following columns: organ: Include one row for each organ that was plated in the experiment. You should name the organ all in lowercase (e.g., “lung”, “spleen”). You should use the same name to also name the sheet that records data for that organ for example, if you have rows in the metadata sheet for “lung” and “spleen”, then you should have two other sheets in the file, one sheet named “lung” and one named “spleen”, which you’ll use to store the plate counts for each of those organs. percentage_of_organ: In this column, give the proportion of that organ that was plated. For example, if you plated half the lung, then in the “lung” row of this spread sheet, you should put 0.5 in the prop_resuspended column. aliquot_ul: 100 uL of the total_resuspended slurry would be considered an original aliquot and is used to peform serial dilutions. dilution_factor: Amount of the original stock solution that is present in the total solution, after dilution(s) total_resuspended_mL: This column contains an original volume of tissue homogenate. For example, raw lung tissue is homogenized in 0.5 mL of PBS in a tube containing metal beads. volume_plated_ul: Amount of suspension + diluent plated on section of solid agar Following this first sheet in the file, you should have one sheet for each organ. The organs that you record in these sheets should match up with the rows on the first, metadata sheet of the template. Each of these organ-specific sheets should look like this: Each of these organ-specific sheets of the template include the following columns: count_date: The date that the CFUs were counted. In some cases, the same plates may be counted at multiple dates. who_plated: An identifier for the researcher who plated the sample who_counted: An identifier for the researcher who counted the plate on this specific date groups: The experimental group to which the mouse belonged to mouse: An identifier for the unique mouse within the group (note: as we collect data from the new experiment, this can be a unique ID by mouse, based on notch ID and cage number) dil_0, dil_1, dil_2, …: The count at each dilution. You can add additional columns if there were more dilutions that are in the template or take away dilution columns if there were fewer. However, all dilution columns should be named consistently, with “dil_” followed by the dilution number (e.g., “0”, “1”, “2”). If the CFUs were too numerous to count for a sample at a particular dilution, put “TNTC” in that cell of the spreadsheet. You can download the template here. When you download the template, it will have example values filled out in blue. Use these to get an idea for how to record your own data. When you are ready to record your own data, delete these example values and replace them with data collected from your own experiment. 3.0.4 Processing collected data Once data are collected, the file can be run through an R workflow. This workflow will convert the data into a format that is easier to work with for data analysis and visualization. It will also produce a report on the data in the spreadsheet, and ultimately it will also write relevant results in a format that can be used to populate a global database for all experiments in the project. The next section provides the details of the pipeline. It aims to explain the code that processes the data and generates visualizations. You do not need to run this code step-by-step, but instead can access a script with the full code here. To use this reporting template, you need to download it to your computer and save it in the file directory where you saved the data you collected with the data collection template. You can then open RStudio and navigate so that you are working within this directory. You should also make sure that you have installed a few required packages on R on the computer you are using to run the report. These packages are: tidyverse, readxl, ggbeeswarm, ggpubr, purrr, knitr, and broom. Within RStudio, open the report template file. There is one spot where you will need to change the code in the template file, so it will read in the data from the version of the template that you saved, which you may have renamed. In the YAML of the report template file, change the file path beside “data:” so that it is the file name of your data file. Once you’ve made this change, you can use the “Knit” button in RStudio to create a report from the data file and the report template file. The report includes the following elements: Organ-specific summaries of the experiment, including the number of mice, experimental groups, date counted, and dilutions used for each experiment Metadata on the CFU collection process (e.g., percent of organ plated for each organ, dilution factor) Plot showing the distribution of CFUs by group in each organ Table giving the results of an ANOVA analysis comparing log CFUs across groups within each organ You can download an example of a report created using this template by clicking here. When you knit to create the report, it will create a Word file in the same file directory where you put your data file and report template. It will also create and output a version of the data that has been processed (in the case of the weights data, this mainly involves tracking mice as they change cages, to link all weights that are from a single animal). This output fill will be named “cfu_output.csv” and, like the report file, will be saved in the same file directory as the data file and the report template. 3.0.5 Details of processing script This section goes through the code within the report template. It explains each part of the code in detail. You do not need to understand these details to use the report template. However, if you have questions about how the data are being processed, or how the outputs are created, all those details are available in this section. As a note, there are two places in the following code where there’s a small change compared to the report template. In the report, you incorporate the path to the data file using the data: section in the YAML at the top of the document. In the following code, we’ve instead used the path of some example data within this book’s file directory, so the code will run for this chapter as well. First, the workflow loads some additional R libraries. You may need to install these on your local R session if you do not already have them installed. Next, the pipeline reads in the organ-specific data. To do this, it creates a list of all of the sheets that are in the spreadsheet other than the metadata sheet. It then loops through each of these organ-specific sheets. It uses pivoting to convert all the dilution levels and values into two columns (a longer rather than wider format), so that the data from all the organs can be joined into a single large dataframe, even if a different number of dilutions were used for the different organs. ## [[1]] ## # A tibble: 880 × 10 ## count_date who_plated who_counted group mouse day sex organ dilution ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… lung 0 ## 2 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… lung 1 ## 3 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… lung 2 ## 4 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… lung 3 ## 5 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… lung 4 ## 6 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… lung 5 ## 7 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… lung 6 ## 8 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… lung 7 ## 9 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… lung 8 ## 10 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… lung 9 ## # ℹ 870 more rows ## # ℹ 1 more variable: CFUs &lt;chr&gt; ## ## [[2]] ## # A tibble: 870 × 10 ## count_date who_plated who_counted group mouse day sex organ dilution ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… sple… 0 ## 2 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… sple… 1 ## 3 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… sple… 2 ## 4 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… sple… 3 ## 5 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… sple… 4 ## 6 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… sple… 5 ## 7 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… sple… 6 ## 8 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… sple… 7 ## 9 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… sple… 8 ## 10 &quot;\\&quot;Dec 28 2023… FL FL BCG 1 14 fema… sple… 9 ## # ℹ 860 more rows ## # ℹ 1 more variable: CFUs &lt;chr&gt; ## # A tibble: 6 × 10 ## count_date who_plated who_counted group mouse day sex organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 0 TNTC ## 2 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 1 TNTC ## 3 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 2 52 ## 4 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 3 10 ## 5 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 4 0 ## 6 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 5 0 At this stage… ## # A tibble: 6 × 4 ## organ who_plated who_counted count_date ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 lung FL FL &quot;\\&quot;Dec 28 2023\\&quot;&quot; ## 2 lung FL FL &quot;\\&quot;Feb 28 2023\\&quot;&quot; ## 3 lung FL FL &quot;\\&quot;Mar 22 24\\&quot;&quot; ## 4 spleen FL FL &quot;\\&quot;Dec 28 2023\\&quot;&quot; ## 5 spleen FL FL &quot;\\&quot;Feb 28 2023\\&quot;&quot; ## 6 spleen FL FL &quot;\\&quot;Mar 22 24\\&quot;&quot; ## # A tibble: 6 × 10 ## count_date who_plated who_counted group mouse day sex organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 2 52 ## 2 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 3 10 ## 3 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 4 0 ## 4 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 5 0 ## 5 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 6 0 ## 6 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 7 0 You can see that, rather than having separate columns for each dilution level on a single row for a sample, there are now multiple rows per sample, with the CFUs at different dilutions given in a CFUs column, with the dilution column identifying which dilution level for each. The next steps work through the data, identifying which dilution is an appropriate one to use to count CFUs for each sample. ## # A tibble: 171 × 10 ## count_date who_plated who_counted group mouse day sex organ dilution ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;\\&quot;Dec 28 2023… FL FL Sali… 1 14 fema… lung 2 ## 2 &quot;\\&quot;Feb 28 2023… FL FL Sali… 1 56 fema… lung 2 ## 3 &quot;\\&quot;Mar 22 24\\&quot;&quot; FL FL Sali… 1 90 fema… lung 3 ## 4 &quot;\\&quot;Dec 28 2023… FL FL Sali… 1 14 male lung 3 ## 5 &quot;\\&quot;Feb 28 2023… FL FL Sali… 1 56 male lung 2 ## 6 &quot;\\&quot;Mar 22 24\\&quot;&quot; FL FL Sali… 1 90 male lung 3 ## 7 &quot;\\&quot;Dec 28 2023… FL FL Sali… 1 14 fema… sple… 0 ## 8 &quot;\\&quot;Feb 28 2023… FL FL Sali… 1 56 fema… sple… 2 ## 9 &quot;\\&quot;Mar 22 24\\&quot;&quot; FL FL Sali… 1 90 fema… sple… 2 ## 10 &quot;\\&quot;Dec 28 2023… FL FL Sali… 1 14 male sple… 1 ## # ℹ 161 more rows ## # ℹ 1 more variable: CFUs &lt;dbl&gt; In the example data, this step has reduced the number observations to consider from over 1406 to 171. ## [1] 1406 ## [1] 171 If you look at the first few rows of the data before and after cleaning, you can see that in particular it has removed a lot of “TNTC” values (as well as a lot of 0 values, although that’s harder to see in this sample of the data): ## # A tibble: 5 × 10 ## count_date who_plated who_counted group mouse day sex organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 2 52 ## 2 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 3 10 ## 3 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 4 0 ## 4 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 5 0 ## 5 &quot;\\&quot;Dec 28… FL FL BCG 1 14 fema… lung 6 0 ## # A tibble: 5 × 10 ## count_date who_plated who_counted group mouse day sex organ dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;\\&quot;Dec 28… FL FL Sali… 1 14 fema… lung 2 61 ## 2 &quot;\\&quot;Feb 28… FL FL Sali… 1 56 fema… lung 2 55 ## 3 &quot;\\&quot;Mar 22… FL FL Sali… 1 90 fema… lung 3 4 ## 4 &quot;\\&quot;Dec 28… FL FL Sali… 1 14 male lung 3 41 ## 5 &quot;\\&quot;Feb 28… FL FL Sali… 1 56 male lung 2 85 Next, the code brings in the information from the metadata sheet, including data on what percent of each organ was resuspended, the dilution factor, and so on. It uses this information to take the CFU value at a given dilution and convert it to an estimate of CFUs per mL. ## # A tibble: 171 × 11 ## organ count_date day who_plated who_counted group sex mouse dilution ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 lung &quot;\\&quot;Dec 28 2023… 14 FL FL Sali… fema… 1 2 ## 2 lung &quot;\\&quot;Feb 28 2023… 56 FL FL Sali… fema… 1 2 ## 3 lung &quot;\\&quot;Mar 22 24\\&quot;&quot; 90 FL FL Sali… fema… 1 3 ## 4 lung &quot;\\&quot;Dec 28 2023… 14 FL FL Sali… male 1 3 ## 5 lung &quot;\\&quot;Feb 28 2023… 56 FL FL Sali… male 1 2 ## 6 lung &quot;\\&quot;Mar 22 24\\&quot;&quot; 90 FL FL Sali… male 1 3 ## 7 lung &quot;\\&quot;Dec 28 2023… 14 FL FL Sali… fema… 2 3 ## 8 lung &quot;\\&quot;Feb 28 2023… 56 FL FL Sali… fema… 2 2 ## 9 lung &quot;\\&quot;Mar 22 24\\&quot;&quot; 90 FL FL Sali… fema… 2 3 ## 10 lung &quot;\\&quot;Dec 28 2023… 14 FL FL Sali… male 2 3 ## # ℹ 161 more rows ## # ℹ 2 more variables: CFUs &lt;dbl&gt;, CFUs_whole &lt;dbl&gt; ## # A tibble: 171 × 11 ## organ count_date day who_plated who_counted group sex mouse dilution ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 lung &quot;\\&quot;Dec 28 2023… 14 FL FL Sali… fema… 1 2 ## 2 lung &quot;\\&quot;Feb 28 2023… 56 FL FL Sali… fema… 1 2 ## 3 lung &quot;\\&quot;Mar 22 24\\&quot;&quot; 90 FL FL Sali… fema… 1 3 ## 4 lung &quot;\\&quot;Dec 28 2023… 14 FL FL Sali… male 1 3 ## 5 lung &quot;\\&quot;Feb 28 2023… 56 FL FL Sali… male 1 2 ## 6 lung &quot;\\&quot;Mar 22 24\\&quot;&quot; 90 FL FL Sali… male 1 3 ## 7 lung &quot;\\&quot;Dec 28 2023… 14 FL FL Sali… fema… 2 3 ## 8 lung &quot;\\&quot;Feb 28 2023… 56 FL FL Sali… fema… 2 2 ## 9 lung &quot;\\&quot;Mar 22 24\\&quot;&quot; 90 FL FL Sali… fema… 2 3 ## 10 lung &quot;\\&quot;Dec 28 2023… 14 FL FL Sali… male 2 3 ## # ℹ 161 more rows ## # ℹ 2 more variables: CFUs &lt;dbl&gt;, CFUs_whole &lt;dbl&gt; ## # A tibble: 6 × 11 ## organ count_date day who_plated who_counted group sex mouse dilution CFUs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lung &quot;\\&quot;Dec 28… 14 FL FL Sali… fema… 1 2 61 ## 2 lung &quot;\\&quot;Feb 28… 56 FL FL Sali… fema… 1 2 55 ## 3 lung &quot;\\&quot;Mar 22… 90 FL FL Sali… fema… 1 3 4 ## 4 lung &quot;\\&quot;Dec 28… 14 FL FL Sali… male 1 3 41 ## 5 lung &quot;\\&quot;Feb 28… 56 FL FL Sali… male 1 2 85 ## 6 lung &quot;\\&quot;Mar 22… 90 FL FL Sali… male 1 3 9 ## # ℹ 1 more variable: CFUs_whole &lt;dbl&gt; The rest of the report code is used to provide summaries, visualizations, and analysis of these data. First, there is code to provide a summary of the number of mice, experimental groups, and some other details for each of the organs: Table 3.1: Organ-specific summary of the experiment organ name value lung Experimental groups: Saline, BCG, ID93-GLA-SE, BCG+ID93-GLA-SE lung Dates counted: “Dec 28 2023”, “Feb 28 2023”, “Mar 22 24” lung Total mice: 17 lung Dilutions considered: 2, 3, 4, 5 spleen Experimental groups: Saline, BCG, ID93-GLA-SE, BCG+ID93-GLA-SE spleen Dates counted: “Dec 28 2023”, “Feb 28 2023”, “Mar 22 24” spleen Total mice: 17 spleen Dilutions considered: 0, 1, 2, 3 Next, the pipeline provides a table with the conditions of the CFU collection, based on the collected metadata from the template: Table 3.2: Conditions of the CFU collection Organ Percent of Organ Plated Dilution Factor Total resuspension mL Volume Plated mL lung 33 10 1.5 0.1 spleen 50 10 1.5 0.1 Next, the pipeline creates a plot showing the distribution of CFUs by experimental group in each of the organs: Next, the pipeline runs an ANOVA analysis on the data. This is conducted after transforming the CFUs with a log-10 transform. Table 3.3: ANOVA results comparing CFUs in each organ across the experimental groups organ contrast1 contrast2 estimate conf.low conf.high adj.p.value lung BCG Saline -0.2368058 -0.5832132 0.1096016 0.2844430 lung ID93 GLA SE Saline 0.0407358 -0.2933692 0.3748408 0.9886291 lung BCG+ID93 GLA SE Saline -0.2381384 -0.5760187 0.0997419 0.2587702 lung ID93 GLA SE BCG 0.2775416 -0.0688658 0.6239490 0.1614924 lung BCG+ID93 GLA SE BCG -0.0013326 -0.3513826 0.3487175 0.9999996 lung BCG+ID93 GLA SE ID93 GLA SE -0.2788742 -0.6167545 0.0590061 0.1420190 spleen BCG Saline -0.2738479 -1.0919602 0.5442645 0.8159594 spleen ID93 GLA SE Saline -0.0292823 -0.8276782 0.7691136 0.9996778 spleen BCG+ID93 GLA SE Saline -0.4036624 -1.2329754 0.4256505 0.5798709 spleen ID93 GLA SE BCG 0.2445656 -0.5735467 1.0626780 0.8612381 spleen BCG+ID93 GLA SE BCG -0.1298145 -0.9781256 0.7184965 0.9779295 spleen BCG+ID93 GLA SE ID93 GLA SE -0.3743802 -1.2036931 0.4549328 0.6381738 ## [1] 0.6506753 ## [1] 0.8720107 ## [1] 0.3555729 ## [1] 0.01680341 ## [1] 0.2140449 ## [1] 0.0267285 As a last step, the code in the template writes a CSV file with the processed data. This file will be an input into a script that will format the data to add to a database where we are collecting and integrating data from all the CSU experiments, and ultimately from there into project-wide storage. References Ben-David, Avishai, and Charles E Davidson. 2014. “Estimation Method for Serial Dilution Experiments.” Journal of Microbiological Methods 107: 214–21. Franzblau, Scott G, Mary Ann DeGroote, Sang Hyun Cho, Koen Andries, Eric Nuermberger, Ian M Orme, Khisimuzi Mdluli, et al. 2012. “Comprehensive Analysis of Methods Used for the Evaluation of Compounds Against Mycobacterium Tuberculosis.” Tuberculosis 92 (6): 453–88. Goldman, Emanuel, and Lorrence H Green. 2015. Practical Handbook of Microbiology. CRC Press. Wilson, GS. 1922. “The Proportion of Viable Bacteria in Young Cultures with Especial Reference to the Technique Employed in Counting.” Journal of Bacteriology 7 (4): 405. "],["enzyme-linked-immunosorbest-assay-elisa.html", "Chapter 4 Enzyme-linked immunosorbest assay (ELISA) 4.1 Importance of ELISA 4.2 ELISA data analysis 4.3 1. Curve fitting model: 4.4 2. Endpoint titer method 4.5 Apply the fitting sigmoid model and endpoint titer function in our dataset 4.6 Create function of Fitted model and endpoint titer, where the output of the fitted model data will be the input of the endpoint titer 4.7 ELISA data processing", " Chapter 4 Enzyme-linked immunosorbest assay (ELISA) ELISA is a standard molecular biology assay for detecting and quantifying a variety of compounds, including peptides, proteins, and antibodies in a sample. The sample could be serum, plasma, or bronchoalveolar lavage fluid (BALF). 4.1 Importance of ELISA An antigen-specific reaction in the host results in the production of antibodies, which are proteins found in the blood. In the event of an infectious disease, it aids in the detection of antibodies in the body. ELISA is distinguishable from other antibody-assays in that it produces quantifiable findings and separates non-specific from specific interactions by serial binding to solid surfaces, which is often a polystyrene multi-well plate. In IMPAc-TB project, it is crucial to evaluate the if the vaccine is eliciting humoral immunity and generating antibodies against vaccine antigen. ELISA will be used to determine the presence of Immunoglobulin (Ig) IgG, IgA, and IgM in the serum different time points post-vaccination. 4.1.1 Principle of ELISA ELISA is based on the principle of antigen-antibody interaction. An antigen must be immobilized on a solid surface and then complexed with an enzyme-linked antibody in an ELISA. The conjugated enzyme’s activity is evaluated by incubating it with a substrate to yield a quantifiable result, which enables detection. There are four basic steps of ELISA: 1. Coating multiwell plate with antigen/antibody: This step depends on what we want to detect the sample. If we need to evaluate the the presence of antibody, the plate will be coated with the antigen, and vice versa. To coat the plate, a fixed concentration of antigen (protein) is added to a 96 well high-binding plate (charged plate). Plate is incubated over night with the antigen at 4 degree celsius (as proteins are temperature sensitive) so that antigens are completely bound to the well. 2. Blocking: It is possible that not each and every site of the well is coated with the targeted antigen, and there could be uncovered areas. It is important to block those empty spaces so that primary antibody (which we will add to the next step) binds to these spaces and give us false positive results. For this, microplate well surface-binding sites are blocked with an unrelated protein or other substance.Most common blocking agents are bovine serum albumin, skim milk, and casein. One of the best blocking agents is to use the serum from the organism in which your secondary (detection antibody) is raised. For example, if the secondary antibody is raised in goat, then we can use goat serum as a blocking agent. 3. Probing: Probing is the step where we add sample containing antibodies that we want to detect. This will be the primary antibody. If the antibodies against the antigen (which we have coated) are present in the sample, it will bind to the antigen with high affinity. 4. Washing: After the incubation of sample containing primary antibody, the wells are washed so that any unbound antibody is washed away. Washing solution contains phosphate buffer saline + 0.05% tween-20 (a mild detergent). 0.05% tween-20 washes away all the non-specific interactions as those are not strong, but keeps all the specific interaction as those are strong and cannot be detached with mild detergent. 5. Detection: To detect the presence of antibody-antigen complex, a secondary antibody labelled with an enzyme (usually horseradish peroxidase) is added to the wells, incubated and washed. 6. Signal Measurement: Finally to detect “if” and “how much” of the antibody is present, a chromogenic substrate (like 3,3’,5,5’-Tetramethylbenzidine) is added to the wells, which can be cleaved the the enzyme that is tagged to the secondary antibody. The color compund is formed after the addition of the substrate, which is directly proportional to the amount of antibody present in the sample. The plate is read on a plate reader, where color is converted to numbers. Figure 4.1: A caption 4.1.2 Loading libraries 4.2 ELISA data analysis Analysis of ELISA data is the most important part of the ELISA experiment. ELISA data can be analyzed in different ways based on how the data is acquired. There are a a few examples of the type of ELISA data : 1. With standard curve: ELISA can be used to determine the concentrations of the antigen and antibody. This type of ELISA data usually have a standard curve with different concentrations of the known analyte and the concentration in the sample is determined by extrapolating the unknown values in the curve. This type of assay is straightforward, easy to interpret and are more robust. 2. Without standard curve: Usually vaccine studies involve investigating the presence of high-affinity (and novel) antibodies against the vaccine antigens. Therefore, plotting a standard curve is not feasible as there is no previous information available for antibody concentration or type of antibody. Also, because antibody response to a vaccine will differ depending on the individual, it is not practical to generate a calibration curve from which absolute concentrations can be extrapolated. For this type of ELISA, quantification of the antibody titers is performed using serial dilutions of the test samples, and analysis can be performed using the following three methods (Hartman et al. 2018): Fitting sigmoid model Endpoint titer method 3: Absorbance summation method Let’s have a look at these methods, how we can apply these methods in our data, and R-based packages that we can utilize to perform this analysis. 4.3 1. Curve fitting model: The curve in ELISA data represents a plot of known concentrations versus their corresponding signal responses. The typical range of these calibration curves is one to two orders of magnitude on the response axis and two or more orders of magnitude on the concentration axis. The real curve of each assay could be easily identified if an infinite number of concentration dilutions with an infinite number of repetitions could be tested. The correct curve must be approximated from a relatively small number of noisy points, though, because there are a finite number of dilutions that may be performed. To estimate the dose-response relationship between standard dilutions, a method of interpolating between standards is required because there cannot be a standard at every concentration. This process is typically performed using a mathematical function or regression to approximate the true shape of the curve. A curve model is the name given to this approximating function, which commonly uses two or more parameters to describe a family of curves, and are then adjusted in order to find the curve from the family of curves that best fits the assay data. Three qualities should be included in a good curve fitting model. 1. The true curve’s shape must be accurately approximated by the curve model. If the curve model does not accomplish this, there is no way to adjust for this component of the total error that results from a lack of fit. 2. In order to get concentration estimates with minimal inaccuracy, a decent curve model must be able to average away as much of the random variation as is practical. 3. A successful curve model must be capable of accurately predicting concentration values for points between the anchor points of the standard dilutions. 4.3.1 How do we perform curve fitting model There are two major steps in performing curve fitting model for non-linear data like ELISA: 1. Finding the initial starting estimates of the parameters 2. locating the optimal solution in a region of the initial estimates We have presented an example below where we have performed a 8-10 point serial dilution of our sample and fitted a 4 parameter curve model. 4.3.2 An example of the curve fitting model 4.3.2.1 Read in the data This information comes from the 2018 study conducted by Hartman et al. Hartman et al. analyzed the ELISA data in their study utilizing fitted sigmoid analysis, end point titer, and absorbance summation. We utilized this information to determine whether our formulas and calculations provide the same outcomes and values as theirs. 4.3.2.2 Tidying the data We next performed tidying the data and make it in a format so that we can plot a sigmoid curve with that. ## # A tibble: 6 × 5 ## numerator denominator absorbance dilution log_dilution ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 30 4 0.0333 -3.10 ## 2 1 90 3.73 0.0111 -4.10 ## 3 1 270 2.34 0.00370 -5.10 ## 4 1 810 1.1 0.00123 -6.10 ## 5 1 2430 0.51 0.000412 -7.10 ## 6 1 7290 0.22 0.000137 -8.10 4.3.2.3 Create function for curve fitting model We next created the curve fitting model function by using nlsLM function from “minpack.lm” package. The purpose of nlslm is to minimize the sum square of the vector returned by the function fn, by a modification of the Levenberg-Marquardt algorithm. In the early 1960s, the Levenberg-Marquardt algorithm was developed to address nonlinear least squares problems. Through a series of well-chosen updates to model parameter values, Levenberg-Marquardt algorithm lower the sum of the squares of the errors between the model function and the data points. ## Nonlinear regression model ## model: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## data: elisa_example_data ## a d c b ## 4.12406 0.04532 -5.31056 7.62972 ## residual sum-of-squares: 0.02221 ## ## Number of iterations to convergence: 9 ## Achieved convergence tolerance: 1.49e-08 ## ## Formula: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## a 4.12406 0.05820 70.860 1.75e-12 *** ## d 0.04532 0.02268 1.998 0.0808 . ## c -5.31056 0.03933 -135.037 1.01e-14 *** ## b 7.62972 0.35854 21.280 2.50e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05269 on 8 degrees of freedom ## ## Number of iterations to convergence: 9 ## Achieved convergence tolerance: 1.49e-08 4.3.2.4 Apply the function to the data 4.3.2.5 Plot the sigmoid curve with fitted sigmoid model 4.4 2. Endpoint titer method The endpoint titer approach chooses an absorbance value just above the background noise (or the lower asymptotic level). The highest dilution with an absorbance greater than this predetermined value is the endpoint titer. This method is based on the assumption that a sample with a higher protein concentration will require a higher dilution factor to achieve an absorbance just above the level of background noise. 4.4.1 Create an endpoint titer function and apply it to the output of the fitted sigmoid model values. ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -8.113 -8.113 -8.113 -8.113 -8.113 -8.113 ## [1] -8.113285 4.4.2 Other methods to analyze ELISA data 4.4.2.1 Absorption summation 4.4.2.2 Area under the curve In this model of data analysis, we sum all the absorbance values from each sample to obtain one value. This value is termed as absorption summation (AS). Using the above data, the AS will be calculated as below: ## [1] 12.24 4.5 Apply the fitting sigmoid model and endpoint titer function in our dataset The presented data is from a mouse study. In this data, presence of IgG antibody has been evaluated against receptor binding domain (RBD) of SARS-CoV-2 virus in two different groups of mice. We need to elucidate which group has higher concentration of the antibodies. 4.5.0.1 Read in the data 4.5.0.2 Tidy the data ## # A tibble: 6 × 4 ## Groups Dilution mouse_id absorbance ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Group 1 1/50 Mouse_1 4.1 ## 2 Group 1 1/50 Mouse_2 3.9 ## 3 Group 1 1/50 Mouse_3 4.3 ## 4 Group 1 1/50 Mouse_4 4.2 ## 5 Group 1 1/50 Mouse_5 4 ## 6 Group 1 1/100 Mouse_1 3.9 ## Groups numerator denomenator mouse_id absorbance dilution log_dilution ## 1 Group 1 1 50 Mouse_1 4.1 0.02 -5.643856 ## 2 Group 1 1 50 Mouse_2 3.9 0.02 -5.643856 ## 3 Group 1 1 50 Mouse_3 4.3 0.02 -5.643856 ## 4 Group 1 1 50 Mouse_4 4.2 0.02 -5.643856 ## 5 Group 1 1 50 Mouse_5 4.0 0.02 -5.643856 ## 6 Group 1 1 100 Mouse_1 3.9 0.01 -6.643856 4.5.0.2.1 converting data into dataframe ## # A tibble: 6 × 3 ## # Groups: Groups, mouse_id [6] ## Groups mouse_id data ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 Group 1 Mouse_1 &lt;tibble [10 × 5]&gt; ## 2 Group 1 Mouse_2 &lt;tibble [10 × 5]&gt; ## 3 Group 1 Mouse_3 &lt;tibble [10 × 5]&gt; ## 4 Group 1 Mouse_4 &lt;tibble [10 × 5]&gt; ## 5 Group 1 Mouse_5 &lt;tibble [10 × 5]&gt; ## 6 Group 2 Mouse_1 &lt;tibble [10 × 5]&gt; 4.5.0.2.2 plot the curves to evaluate the a, d, c, and b Based on the curve, the values are: a = 4, d = 0 c = 2 b = 1 4.5.1 Creating a function for fitting model 4.5.1.1 Fitting the model into the dataset ## Nonlinear regression model ## model: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## data: df_elisa ## a d c b ## 4.3070 -0.6009 -10.2577 5.2893 ## residual sum-of-squares: 0.1199 ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 1.49e-08 4.5.1.2 Apply the fitted model function to the whole dataframe ## # A tibble: 6 × 4 ## # Groups: Groups, mouse_id [6] ## Groups mouse_id data fitted_data ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 Group 1 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 2 Group 1 Mouse_2 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 3 Group 1 Mouse_3 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 4 Group 1 Mouse_4 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 5 Group 1 Mouse_5 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 6 Group 2 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; 4.5.1.3 Take out the summary of the data ## Nonlinear regression model ## model: absorbance ~ ((a - d)/(1 + (log_dilution/c)^b)) + d ## data: df_elisa ## a d c b ## 4.3070 -0.6009 -10.2577 5.2893 ## residual sum-of-squares: 0.1199 ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 1.49e-08 4.6 Create function of Fitted model and endpoint titer, where the output of the fitted model data will be the input of the endpoint titer 4.6.0.1 Apply the fitted model fuction into the nested data and use the output of the fitted data as the input for endpoint titer value evaluation 4.6.0.1.1 Run fitted model on the data ## # A tibble: 6 × 4 ## # Groups: Groups, mouse_id [6] ## Groups mouse_id data fitted_data ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 Group 1 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 2 Group 1 Mouse_2 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 3 Group 1 Mouse_3 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 4 Group 1 Mouse_4 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 5 Group 1 Mouse_5 &lt;tibble [10 × 5]&gt; &lt;nls&gt; ## 6 Group 2 Mouse_1 &lt;tibble [10 × 5]&gt; &lt;nls&gt; 4.6.0.1.2 Taking output of the fitted model function and into endpoint titer function 4.6.0.2 Plot the endpoint titer data for the two groups 4.6.0.3 Perform statistical analysis on the data ## # A tibble: 1 × 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.800 -14.0 -13.2 -18.8 0.00000268 5.63 -0.906 -0.695 ## # ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; 4.6.0.4 Statistical data analysis for more than two groups 4.7 ELISA data processing We read ELISA plate in a 96 well plate using a plate reader. The plate reader generates the data in form of number in an excel sheet. We have created this pipeline/worksheet to bring out the information from the excl sheet to a tidy format in which the above created fitted model and endpoint titer functions can be applied. 4.7.0.1 Read in the first dataset Below is the example ELISA data that has came straight out of the plate reader. This data is arranged in a 96-well plate format and contains Optical Density (OD) values. ## # A tibble: 6 × 12 ## ...1 ...2 ...3 ...4 ...5 ...6 ...7 ...8 ...9 ...10 ...11 ...12 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.199999999… 0.05 0.069 6.3E… 0.061 0.122 0.16… 0.145 0.135 6.80… 0.053 0.05 ## 2 7.900000000… 0.098 0.069 6.80… 0.115 0.202 5.89… 0.134 0.069 0.106 0.05 0.075 ## 3 8.899999999… 0.133 0.119 OVRF… 3.87 2.32 OVRF… 3.85 2.12 OVRF… 3.21 1.02 ## 4 OVRFLW 3.46 1.16 OVRF… 3.80 2.36 OVRF… 3.70 1.49 OVRF… 3.68 1.63 ## 5 3.815999999… 1.82 0.446 3.89… 3.42 1.13 OVRF… 2.33 0.608 OVRF… 3.41 1.10 ## 6 OVRFLW 3.69 1.43 OVRF… 3.66 1.27 3.839 1.74 0.444 2.49… 0.637 0.704 4.7.0.2 Tidy dataset 1 It is important to clean the data and arrange it in a format on which we can apply formulas and functions. ## # A tibble: 6 × 2 ## well_id od_450nm ## &lt;chr&gt; &lt;dbl&gt; ## 1 A1 0.052 ## 2 A2 0.05 ## 3 A3 0.069 ## 4 A4 0.063 ## 5 A5 0.061 ## 6 A6 0.122 4.7.0.3 Read in the second data set The second dataset contains the information such as groups, mouse id, and dilutions for the respective wells of the 96 well plate for the dataset-1. ## # A tibble: 6 × 12 ## ...1 ...2 ...3 ...4 ...5 ...6 ...7 ...8 ...9 ...10 ...11 ...12 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 blank secon… naïv… 1A-1… 1A-1… 1A-1… 1A-2… 1A-2… 1A-2… 1A-3… 1A-3… 1A-3… ## 2 1A-4 (1/250 1A-4 … 1A-4… 1B-1… 1B-1… 1B-1… 1B-2… 1B-2… 1B-2… 1B-3… 1B-3… 1B-3… ## 3 1B-4 (1/250 1B-4 … 1B-4… 2A-1… 2A-1… 2A-1… 2A-2… 2A-2… 2A-2… 2A-3… 2A-3… 2A-3… ## 4 2B-1 (1/250 2B-1 … 2B-1… 2B-2… 2B-2… 2B-2… 2B-3… 2B-3… 2B-3… 2B-4… 2B-4… 2B-4… ## 5 3A-1 (1/250 3A-1 … 3A-1… 3A-2… 3A-2… 3A-2… 3A-3… 3A-3… 3A-3… 3A-4… 3A-4… 3A-4… ## 6 3B-1 (1/250 3B-1 … 3B-1… 3B-2… 3B-2… 3B-2… 3B-3… 3B-3… 3B-3… 3B-4… 3B-4… 3B-4… 4.7.0.4 Tidy dataset-2 ## # A tibble: 6 × 2 ## well_id information ## &lt;chr&gt; &lt;chr&gt; ## 1 A1 blank ## 2 A2 secondary ## 3 A3 naïve (1/250) ## 4 A4 1A-1 (1/250 ## 5 A5 1A-1 (1/1250 ## 6 A6 1A-1 (1/6250 4.7.0.5 Merge dataset-1 (with OD information) with dataset-2 (with respective data information) To create a complete full dataset with Groups, mouse-id, dilutions, and OD, we merged the dataset-1 and dataset-2 together. We also cleaned the data set so that mouse-ID and dilution columns are separate and have their own columns. ## # A tibble: 6 × 3 ## well_id od_450nm information ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 A1 0.052 blank ## 2 A2 0.05 secondary ## 3 A3 0.069 naïve (1/250) ## 4 A4 0.063 1A-1 (1/250 ## 5 A5 0.061 1A-1 (1/1250 ## 6 A6 0.122 1A-1 (1/6250 ## # A tibble: 6 × 4 ## well_id od_450nm sample_id dilution ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 A1 0.052 &quot;blank&quot; &lt;NA&gt; ## 2 A2 0.05 &quot;secondary&quot; &lt;NA&gt; ## 3 A3 0.069 &quot;naïve &quot; 1/250) ## 4 A4 0.063 &quot;1A-1 &quot; 1/250 ## 5 A5 0.061 &quot;1A-1 &quot; 1/1250 ## 6 A6 0.122 &quot;1A-1 &quot; 1/6250 ## # A tibble: 6 × 4 ## well_id sample_id dilution od_450nm ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A1 &quot;blank&quot; NA 0.052 ## 2 A2 &quot;secondary&quot; NA 0.05 ## 3 A3 &quot;naïve &quot; 250 0.069 ## 4 A4 &quot;1A-1 &quot; 250 0.063 ## 5 A5 &quot;1A-1 &quot; 1250 0.061 ## 6 A6 &quot;1A-1 &quot; 6250 0.122 References Hartman, Holly, Yuge Wang, Harry W Schroeder Jr, and Xiangqin Cui. 2018. “Absorbance Summation: A Novel Approach for Analyzing High-Throughput ELISA Data in the Absence of a Standard.” PloS One 13 (6): e0198528. "],["flow-cytometry.html", "Chapter 5 Flow cytometry 5.1 Loading packages 5.2 Loading data", " Chapter 5 Flow cytometry NOTICE.This chapter is in progress. Flow cytometry data can be quantified in many different ways and with different techniques. For the purpose of these data analyses, manual gating has been achieved in FlowJo and cell frequencies and populations exported as a .csv file. This .csv file is the primary input for this R pipeline which aims to output box plots for each gated cell population. This example data set is from an innate response study whcih investigated the immune response in the lungs during the first 28 days of infection. Immune cells are very diverse, and the make-up of immune cells within a sample can provide important insights on immune processes based on measures of this composition. Immune cells can be categorized into large groups (e.g., T cells, B cells, macrophages, dendritic cells). They can also be characterized into different populations within these large groups, based on things like activation and differentiation [?] within the group [?] (Maecker, McCoy, and Nussenblatt 2012) (e.g., T cells can be divided into naive T cells versus memory T cells, helper T cells versus cytotoxic T cells, [others] [?]). This process of characterizing the immune cells in a sample is called immunophenotyping. To make these classifications, flow cytometry uses a pretty clever mix of physics and biology. First, it starts by leveraging the biological knowledge that antibodies can have a very specific affinity for a certain protein. This means that you can find a set of antibodies that will target and stick specifically to certain proteins. Flow cytometry starts by creating a panel of up to [x] protein markers, focusing on proteins that can help in identifying a specific cell type. Typically, the panel will include several proteins that are “CD” proteins (the “CD” stands for cluster of differentiation or classification determinant). These are proteins that show up on the surface of immune cells, with specific CD proteins common to only certain types of cells, making their presence or absence helpful in classifying cells. Two of the most common CDs to include on a panel are CD3, CD4, and CD8. T cells have CD3 on their surface, so a marker for CD3 can be used to distinguish T cells from other types of white blood cells, including granulocytes, monocytes, and B cells. Among T cells, the helper T cells have the CD4 protein on their surface, while the cytotoxic T cells have the CD8 protein on their surface, so the CD4 and CD8 markers can help in refining a T cell into a more specific type. In flow cytometry, you can characterize immune cells into populations based on proteins on the cell surface and inside the cell, as well as cell size and granularity Barnett et al. (2008). For example, macrophages can be distinguished from T cells and B cells based on … [size? granularity?], which T cells and B cells can be distinguished from each other based on whether the cell has the [surface protein? CD3?], and helper T cells versus cytotoxic T cells can be distinguished from each other based on whether the cell has the [surface protein CD8? CD4?]. 5.1 Loading packages MULTI CSV SHEETS MALE FEMALE 5.2 Loading data ## # A tibble: 64 × 30 ## Sample sex day `CD3+ ` `CD3+ CD4+ ` `CD3+ CD4+ CD62L- CD44+ ` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot; Saline_1.fcs&quot; female 14 30 14.1 5.19 ## 2 &quot; Saline_2.fcs&quot; female 14 29.5 12.4 2.47 ## 3 &quot; Saline_3.fcs&quot; female 14 26.7 12.6 4.86 ## 4 &quot; Saline_4.fcs&quot; female 14 17.2 7.2 3 ## 5 &quot; BCG_1.fcs&quot; female 14 43.2 24.9 14.8 ## 6 &quot; BCG_2.fcs&quot; female 14 11.7 6.42 1.45 ## 7 &quot; BCG_3.fcs&quot; female 14 24.1 13.5 4.59 ## 8 &quot; BCG_4.fcs&quot; female 14 41.2 24 10.2 ## 9 &quot; ID93_1.fcs&quot; female 14 38.9 24.2 11.9 ## 10 &quot; ID93_2.fcs&quot; female 14 34.2 18.9 10.1 ## # ℹ 54 more rows ## # ℹ 24 more variables: `CD3+ CD4+ CD62L- CD44+ PD1- KLRG1+ ` &lt;dbl&gt;, ## # `CD3+ CD4+ CD62L- CD44+ PD1+ KLRG1+ ` &lt;dbl&gt;, ## # `CD3+ CD4+ CD62L- CD44+ PD1+ KLRG1- ` &lt;dbl&gt;, ## # `CD3+ CD4+ CD62L- CD44+ PD1- KLRG1- ` &lt;dbl&gt;, ## # `CD3+ CD4+ CD62L+ CD44+ ` &lt;dbl&gt;, `CD3+ CD4+ CD62L+ CD44- ` &lt;dbl&gt;, ## # `CD3+ CD4+ CD62L+ CD44- PD1- KLRG1+ ` &lt;dbl&gt;, … ## Sample sex ## 0 0 ## day CD3+ ## 0 0 ## CD3+ CD4+ CD3+ CD4+ CD62L- CD44+ ## 0 0 ## CD3+ CD4+ CD62L- CD44+ PD1- KLRG1+ CD3+ CD4+ CD62L- CD44+ PD1+ KLRG1+ ## 0 0 ## CD3+ CD4+ CD62L- CD44+ PD1+ KLRG1- CD3+ CD4+ CD62L- CD44+ PD1- KLRG1- ## 0 0 ## CD3+ CD4+ CD62L+ CD44+ CD3+ CD4+ CD62L+ CD44- ## 0 0 ## CD3+ CD4+ CD62L+ CD44- PD1- KLRG1+ CD3+ CD4+ CD62L+ CD44- PD1+ KLRG1+ ## 0 0 ## CD3+ CD4+ CD62L+ CD44- PD1+ KLRG1- CD3+ CD4+ CD62L+ CD44- PD1- KLRG1- ## 0 0 ## CD3+ CD4+ CD62L- CD44- CD3+ CD8+ ## 0 0 ## CD3+ CD8+ CD62L- CD44+ CD3+ CD8+ CD62L- CD44+ PD1- KLRG1+ ## 0 0 ## CD3+ CD8+ CD62L- CD44+ PD1+ KLRG1+ CD3+ CD8+ CD62L- CD44+ PD1+ KLRG1- ## 0 0 ## CD3+ CD8+ CD62L- CD44+ PD1- KLRG1- CD3+ CD8+ CD62L+ CD44+ ## 0 0 ## CD3+ CD8+ CD62L+ CD44- CD3+ CD8+ CD62L+ CD44- PD1- KLRG1+ ## 0 0 ## CD3+ CD8+ CD62L+ CD44- PD1+ KLRG1+ CD3+ CD8+ CD62L+ CD44- PD1+ KLRG1- ## 0 0 ## CD3+ CD8+ CD62L+ CD44- PD1- KLRG1- CD3+ CD8+ CD62L- CD44- ## 0 0 MULTIDAY SHEETS FOR EXCEL SINGLE SHEETS Loading data MAKING DATA TIDY FOR PLOTTING ## # A tibble: 1,728 × 6 ## group mouse_ID sex day cell_types percentage_of_LIVE ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Saline 1 female 14 CD3+ 30 ## 2 Saline 1 female 14 CD3+ CD4+ 14.1 ## 3 Saline 1 female 14 CD3+ CD4+ CD62L- CD44+ 5.19 ## 4 Saline 1 female 14 CD3+ CD4+ CD62L- CD44+ PD1- … 0.42 ## 5 Saline 1 female 14 CD3+ CD4+ CD62L- CD44+ PD1+ … 0.24 ## 6 Saline 1 female 14 CD3+ CD4+ CD62L- CD44+ PD1+ … 1.87 ## 7 Saline 1 female 14 CD3+ CD4+ CD62L- CD44+ PD1- … 2.66 ## 8 Saline 1 female 14 CD3+ CD4+ CD62L+ CD44+ 0.72 ## 9 Saline 1 female 14 CD3+ CD4+ CD62L+ CD44- 3.9 ## 10 Saline 1 female 14 CD3+ CD4+ CD62L+ CD44- PD1- … 0.00482 ## # ℹ 1,718 more rows ## tibble [1,728 × 6] (S3: tbl_df/tbl/data.frame) ## $ group : chr [1:1728] &quot;Saline&quot; &quot;Saline&quot; &quot;Saline&quot; &quot;Saline&quot; ... ## $ mouse_ID : chr [1:1728] &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ... ## $ sex : chr [1:1728] &quot;female&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... ## $ day : chr [1:1728] &quot;14&quot; &quot;14&quot; &quot;14&quot; &quot;14&quot; ... ## $ cell_types : chr [1:1728] &quot;CD3+&quot; &quot;CD3+ CD4+&quot; &quot;CD3+ CD4+ CD62L- CD44+&quot; &quot;CD3+ CD4+ CD62L- CD44+ PD1- KLRG1+&quot; ... ## $ percentage_of_LIVE: num [1:1728] 30 14.1 5.19 0.42 0.24 1.87 2.66 0.72 3.9 0.00482 ... CHECK FIRST ASSUMPTION IN STATS (Independent obervations) ## [1] &quot;CD3+&quot; &quot;CD3+ CD4+&quot; ## [3] &quot;CD3+ CD4+ CD62L- CD44+&quot; &quot;CD3+ CD4+ CD62L- CD44+ PD1- KLRG1+&quot; ## [5] &quot;CD3+ CD4+ CD62L- CD44+ PD1+ KLRG1+&quot; &quot;CD3+ CD4+ CD62L- CD44+ PD1+ KLRG1-&quot; ## [7] &quot;CD3+ CD4+ CD62L- CD44+ PD1- KLRG1-&quot; &quot;CD3+ CD4+ CD62L+ CD44+&quot; ## [9] &quot;CD3+ CD4+ CD62L+ CD44-&quot; &quot;CD3+ CD4+ CD62L+ CD44- PD1- KLRG1+&quot; ## [11] &quot;CD3+ CD4+ CD62L+ CD44- PD1+ KLRG1+&quot; &quot;CD3+ CD4+ CD62L+ CD44- PD1+ KLRG1-&quot; ## [13] &quot;CD3+ CD4+ CD62L+ CD44- PD1- KLRG1-&quot; &quot;CD3+ CD4+ CD62L- CD44-&quot; ## [15] &quot;CD3+ CD8+&quot; &quot;CD3+ CD8+ CD62L- CD44+&quot; ## [17] &quot;CD3+ CD8+ CD62L- CD44+ PD1- KLRG1+&quot; &quot;CD3+ CD8+ CD62L- CD44+ PD1+ KLRG1+&quot; ## [19] &quot;CD3+ CD8+ CD62L- CD44+ PD1+ KLRG1-&quot; &quot;CD3+ CD8+ CD62L- CD44+ PD1- KLRG1-&quot; ## [21] &quot;CD3+ CD8+ CD62L+ CD44+&quot; &quot;CD3+ CD8+ CD62L+ CD44-&quot; ## [23] &quot;CD3+ CD8+ CD62L+ CD44- PD1- KLRG1+&quot; &quot;CD3+ CD8+ CD62L+ CD44- PD1+ KLRG1+&quot; ## [25] &quot;CD3+ CD8+ CD62L+ CD44- PD1+ KLRG1-&quot; &quot;CD3+ CD8+ CD62L+ CD44- PD1- KLRG1-&quot; ## [27] &quot;CD3+ CD8+ CD62L- CD44-&quot; CHECKING EQUAL VARIANCES OF POPULATION OF INTEREST ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 7 8.3438 6.034e-07 *** ## 56 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Bartlett test of homogeneity of variances ## ## data: percentage_of_LIVE by interaction(group, day) ## Bartlett&#39;s K-squared = 36.214, df = 7, p-value = 6.605e-06 WORKING ON STATS June 1, 2023 EXPLORATORY ANALYSIS ## [1] &quot;Saline&quot; &quot;BCG&quot; &quot;ID93&quot; &quot;BCG+ID93&quot; PLOT POPULATIONS WORKING ON THIS Saving plots to PDF Looking at specific populations References Barnett, David, Brooke Walker, Alan Landay, and Thomas N Denny. 2008. “CD4 Immunophenotyping in HIV Infection.” Nature Reviews Microbiology 6 (11): S7–15. Maecker, Holden T, J Philip McCoy, and Robert Nussenblatt. 2012. “Standardizing Immunophenotyping for the Human Immunology Project.” Nature Reviews Immunology 12 (3): 191–200. "],["pathology.html", "Chapter 6 Pathology", " Chapter 6 Pathology NOTICE.This chapter is in progress. "],["references.html", "References", " References Baazim, Hatoon, Laura Antonio-Herrera, and Andreas Bergthaler. 2022. “The Interplay of Immunology and Cachexia in Infection and Cancer.” Nature Reviews Immunology 22 (5): 309–21. Baazim, Hatoon, Martina Schweiger, Michael Moschinger, Haifeng Xu, Thomas Scherer, Alexandra Popa, Suchira Gallage, et al. 2019. “CD8+ t Cells Induce Cachexia During Chronic Viral Infection.” Nature Immunology 20 (6): 701–10. Barnett, David, Brooke Walker, Alan Landay, and Thomas N Denny. 2008. “CD4 Immunophenotyping in HIV Infection.” Nature Reviews Microbiology 6 (11): S7–15. Ben-David, Avishai, and Charles E Davidson. 2014. “Estimation Method for Serial Dilution Experiments.” Journal of Microbiological Methods 107: 214–21. Franzblau, Scott G, Mary Ann DeGroote, Sang Hyun Cho, Koen Andries, Eric Nuermberger, Ian M Orme, Khisimuzi Mdluli, et al. 2012. “Comprehensive Analysis of Methods Used for the Evaluation of Compounds Against Mycobacterium Tuberculosis.” Tuberculosis 92 (6): 453–88. Goldman, Emanuel, and Lorrence H Green. 2015. Practical Handbook of Microbiology. CRC Press. Hartman, Holly, Yuge Wang, Harry W Schroeder Jr, and Xiangqin Cui. 2018. “Absorbance Summation: A Novel Approach for Analyzing High-Throughput ELISA Data in the Absence of a Standard.” PloS One 13 (6): e0198528. Maecker, Holden T, J Philip McCoy, and Robert Nussenblatt. 2012. “Standardizing Immunophenotyping for the Human Immunology Project.” Nature Reviews Immunology 12 (3): 191–200. Segueni, Noria, Elaine Tritto, Marie-Laure Bourigault, Stéphanie Rose, François Erard, Marc Le Bert, Muazzam Jacobs, et al. 2016. “Controlled Mycobacterium Tuberculosis Infection in Mice Under Treatment with Anti-IL-17A or IL-17F Antibodies, in Contrast to TNF-Alpha Neutralization.” Scientific Reports 6 (1): 1–17. Smith, Clare M, Richard E Baker, Megan K Proulx, Bibhuti B Mishra, Jarukit E Long, Sae Woong Park, Ha-Na Lee, et al. 2022. “Host-Pathogen Genetic Interactions Underlie Tuberculosis Susceptibility in Genetically Diverse Mice.” Elife 11: e74419. Wilson, GS. 1922. “The Proportion of Viable Bacteria in Young Cultures with Especial Reference to the Technique Employed in Counting.” Journal of Bacteriology 7 (4): 405. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
